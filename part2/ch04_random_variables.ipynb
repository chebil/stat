{
 "cells": [
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "# 4.1 Random Variables\n",
    "\n",
    "Quite commonly, we would like to deal with **numbers that are random**. We can do so by linking numbers to the outcome of an experiment. \n",
    "\n",
    "A **random variable** is a function that assigns a number to each outcome of a random experiment. This simple idea is incredibly powerful—it lets us work with numbers instead of abstract outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition 4.1: Discrete Random Variable\n",
    "\n",
    "Given a sample space $\\Omega$, a set of events $\\mathcal{F}$, a probability function $P$, and a **countable** set of real numbers $\\mathcal{D}$, a **discrete random variable** is a function with domain $\\Omega$ and range $\\mathcal{D}$.\n",
    "\n",
    "This means that for any outcome $\\omega$ there is a number $X(\\omega)$.\n",
    "\n",
    "$$X: \\Omega \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "For each outcome $\\omega \\in \\Omega$, $X(\\omega)$ is a real number.\n",
    "\n",
    "---\n",
    "\n",
    "## Examples of Random Variables\n",
    "\n",
    "### Example 4.1: Numbers from Coins\n",
    "\n",
    "We flip a coin:\n",
    "- Whenever the coin comes up **heads**, we report **1**\n",
    "- When it comes up **tails**, we report **0**\n",
    "\n",
    "This is a random variable.\n",
    "\n",
    "### Example 4.2: Numbers from Coins II\n",
    "\n",
    "We flip a coin **32 times**. We record a 1 when it comes up heads, and when it comes up tails, we record a 0. This produces a **32-bit random number**, which is a random variable.\n",
    "\n",
    "### Example 4.3: The Number of Pairs in a Poker Hand\n",
    "\n",
    "We draw a hand of five cards. The **number of pairs** in this hand is a random variable, which takes the values $\\{0, 1, 2\\}$ depending on which hand we draw.\n",
    "\n",
    "### Example 4.4: Parity of Coin Flips\n",
    "\n",
    "We flip a coin 32 times. We record a 1 when it comes up heads, and when it comes up tails, we record a 0. This produces a 32-bit random number, which is a random variable. The **parity** of this number is also a random variable.\n",
    "\n",
    "**Key Point**: A function that takes a discrete random variable to a set of numbers is **also** a discrete random variable.\n",
    "\n",
    "---\n",
    "\n",
    "## Probability Distribution of Random Variables\n",
    "\n",
    "Associated with any value $x$ of the random variable $X$ are a series of events. The most important is the set of outcomes $\\omega$ such that $X(\\omega) = x$, which we can write $\\{\\omega : X(\\omega) = x\\}$ (it is usual to simplify to $\\{X = x\\}$, and we will do so).\n",
    "\n",
    "The probability that a random variable $X$ takes the value $x$ is given by $P(\\{\\omega : X(\\omega) = x\\})$, which is more usually written $P(\\{X = x\\})$. This is sometimes written as $P(X = x)$, and rather often written as $P(x)$.\n",
    "\n",
    "### Definition 4.2: Probability Distribution\n",
    "\n",
    "The **probability distribution** of a discrete random variable is the set of numbers $P(\\{X = x\\})$ for each value $x$ that $X$ can take. The distribution takes the value 0 at all other numbers.\n",
    "\n",
    "Notice that the distribution is **non-negative**.\n",
    "\n",
    "The probability distribution is sometimes known as the **probability mass function** (PMF).\n",
    "\n",
    "### Definition 4.3: Cumulative Distribution\n",
    "\n",
    "The **cumulative distribution** of a discrete random variable is the set of numbers $P(\\{X \\leq x\\})$ for each value $x$ that $X$ can take.\n",
    "\n",
    "Notice that this is a **non-decreasing** function of $x$.\n",
    "\n",
    "---\n",
    "\n",
    "## Worked Example 4.1: Numbers from Coins III\n",
    "\n",
    "**Problem**: We flip a biased coin **2 times**. The flips are independent. The coin has $P(H) = p$, $P(T) = 1-p$. We record a 1 when it comes up heads, and when it comes up tails, we record a 0. This produces a **2-bit random number**, which is a random variable taking the values $\\{0, 1, 2, 3\\}$. What is the probability distribution and cumulative distribution of this random variable?\n",
    "\n",
    "**Solution**: \n",
    "\n",
    "**Probability distribution**:\n",
    "- $P(0) = (1-p)^2$\n",
    "- $P(1) = (1-p)p$\n",
    "- $P(2) = p(1-p)$\n",
    "- $P(3) = p^2$\n",
    "\n",
    "**Cumulative distribution**:\n",
    "- $F(0) = (1-p)^2$\n",
    "- $F(1) = (1-p)$\n",
    "- $F(2) = (1-p)^2 + p(1-p) = (1-p)(2-p)$\n",
    "- $F(3) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example with p = 0.6\n",
    "p = 0.6\n",
    "values = [0, 1, 2, 3]\n",
    "\n",
    "# PMF\n",
    "pmf = [(1-p)**2, (1-p)*p, p*(1-p), p**2]\n",
    "\n",
    "# CDF\n",
    "cdf = [pmf[0], \n",
    "       pmf[0] + pmf[1],\n",
    "       pmf[0] + pmf[1] + pmf[2],\n",
    "       1.0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot PMF\n",
    "ax1.bar(values, pmf, edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title(f'PMF: 2-bit random number (p={p})')\n",
    "ax1.set_xticks(values)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (v, prob) in enumerate(zip(values, pmf)):\n",
    "    ax1.text(v, prob, f'{prob:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot CDF\n",
    "ax2.step(values, cdf, where='post', linewidth=2, label='CDF')\n",
    "ax2.scatter(values, cdf, color='red', s=100, zorder=5)\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Cumulative Probability')\n",
    "ax2.set_title(f'CDF: 2-bit random number (p={p})')\n",
    "ax2.set_xticks(values)\n",
    "ax2.set_ylim([0, 1.1])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Probability Distribution:\")\n",
    "for v, prob in zip(values, pmf):\n",
    "    print(f\"  P(X = {v}) = {prob:.4f}\")\n",
    "\n",
    "print(\"\\nCumulative Distribution:\")\n",
    "for v, cum in zip(values, cdf):\n",
    "    print(f\"  F({v}) = P(X ≤ {v}) = {cum:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probability Distribution:\n",
    "  P(X = 0) = 0.1600\n",
    "  P(X = 1) = 0.2400\n",
    "  P(X = 2) = 0.2400\n",
    "  P(X = 3) = 0.3600\n",
    "\n",
    "Cumulative Distribution:\n",
    "  F(0) = P(X ≤ 0) = 0.1600\n",
    "  F(1) = P(X ≤ 1) = 0.4000\n",
    "  F(2) = P(X ≤ 2) = 0.6400\n",
    "  F(3) = P(X ≤ 3) = 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Worked Example 4.2: Betting on Coins\n",
    "\n",
    "**Problem**: One way to get a random variable is to think about the **reward for a bet**. We agree to play the following game:\n",
    "- I flip a coin with $P(H) = p$, $P(T) = 1-p$\n",
    "- If the coin comes up **heads**, you pay me $q$\n",
    "- If the coin comes up **tails**, I pay you $r$\n",
    "\n",
    "The number of dollars that change hands is a random variable. What is its probability distribution?\n",
    "\n",
    "**Solution**: We see this problem from **my perspective**:\n",
    "- If the coin comes up heads, I get $q$\n",
    "- If it comes up tails, I get $-r$\n",
    "\n",
    "So we have:\n",
    "$$P(X = q) = p \\text{ and } P(X = -r) = (1-p)$$\n",
    "\n",
    "and all other probabilities are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betting game\n",
    "p = 0.5  # Fair coin\n",
    "q = 2    # I win 2 if heads\n",
    "r = 1    # You win 1 if tails\n",
    "\n",
    "outcomes = [q, -r]\n",
    "probs = [p, 1-p]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(outcomes, probs, width=0.3, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('My winnings ($)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Betting Game: Win ${q} with prob {p}, Lose ${r} with prob {1-p}')\n",
    "plt.xticks(outcomes)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for outcome, prob in zip(outcomes, probs):\n",
    "    plt.text(outcome, prob, f'{prob:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Expected value\n",
    "expected = q*p + (-r)*(1-p)\n",
    "plt.axvline(x=expected, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Expected value = ${expected:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Expected value of my winnings = ${expected:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.1 Joint and Conditional Probability for Random Variables\n",
    "\n",
    "All the concepts of probability that we described for events carry over to random variables. This is as it should be, because random variables are really just a way of getting numbers out of events. However, **terminology and notation change a bit**.\n",
    "\n",
    "### Definition 4.4: Joint Probability Distribution\n",
    "\n",
    "Assume we have two random variables $X$ and $Y$. The probability that $X$ takes the value $x$ **and** $Y$ takes the value $y$ could be written as $P(\\{X = x\\} \\cap \\{Y = y\\})$. It is more usual to write it as:\n",
    "\n",
    "$$P(x, y)$$\n",
    "\n",
    "This is referred to as the **joint probability distribution** of the two random variables (or, quite commonly, the **joint**).\n",
    "\n",
    "You can think of this as a **table of probabilities**, one for each possible pair of $x$ and $y$ values.\n",
    "\n",
    "### Simplified Notation\n",
    "\n",
    "Usually, we are interested in random variables, rather than potentially arbitrary outcomes or sets of outcomes. We will write:\n",
    "- $P(X)$ to denote the probability distribution of a random variable\n",
    "- $P(x)$ or $P(X = x)$ to denote the probability that random variable takes a particular value\n",
    "\n",
    "This means that, for example, the rule we could write as:\n",
    "$$P(\\{X = x\\} | \\{Y = y\\})P(\\{Y = y\\}) = P(\\{X = x\\} \\cap \\{Y = y\\})$$\n",
    "\n",
    "will be written as:\n",
    "$$P(x|y)P(y) = P(x, y)$$\n",
    "\n",
    "### Definition 4.5: Bayes Rule for Random Variables\n",
    "\n",
    "Recall the rule from Section 3.4.1:\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "This rule can be rewritten in our notation for random variables:\n",
    "\n",
    "$$\\boxed{P(x|y) = \\frac{P(y|x)P(x)}{P(y)}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Important Properties\n",
    "\n",
    "Random variables have another useful property. If $x_0 \\neq x_1$, then the event $\\{X = x_0\\}$ must be **disjoint** from the event $\\{X = x_1\\}$. This means that:\n",
    "\n",
    "$$\\sum_x P(x) = 1$$\n",
    "\n",
    "and that, for any $y$:\n",
    "\n",
    "$$\\sum_x P(x|y) = 1$$\n",
    "\n",
    "---\n",
    "\n",
    "## Definition 4.6: Marginal Probability\n",
    "\n",
    "Write $P(x, y)$ for the joint probability distribution of two random variables $X$ and $Y$. Then:\n",
    "\n",
    "$$P(x) = \\sum_y P(x, y) = \\sum_y P(\\{X = x\\} \\cap \\{Y = y\\}) = P(\\{X = x\\})$$\n",
    "\n",
    "is referred to as the **marginal probability distribution** of $X$.\n",
    "\n",
    "**Intuition**: We \"marginalize out\" $Y$ by summing over all its possible values.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition 4.7: Independent Random Variables\n",
    "\n",
    "The random variables $X$ and $Y$ are **independent** if the events $\\{X = x\\}$ and $\\{Y = y\\}$ are independent for all values $x$ and $y$.\n",
    "\n",
    "This means that:\n",
    "$$P(\\{X = x\\} \\cap \\{Y = y\\}) = P(\\{X = x\\})P(\\{Y = y\\})$$\n",
    "\n",
    "which we can rewrite as:\n",
    "\n",
    "$$\\boxed{P(x, y) = P(x)P(y)}$$\n",
    "\n",
    "**Equivalently**: $P(x|y) = P(x)$ for all $x, y$ (knowing $Y$ tells us nothing about $X$).\n",
    "\n",
    "---\n",
    "\n",
    "## Worked Example 4.3: Sums and Differences of Dice\n",
    "\n",
    "**Problem**: You throw two dice. The number of spots on the first die is a random variable (call it $X$), so is the number of spots on the second die ($Y$). $X$ and $Y$ are independent. Now define:\n",
    "- $S = X + Y$ (sum)\n",
    "- $D = X - Y$ (difference)\n",
    "\n",
    "What is the probability distribution of $S$ and of $D$?\n",
    "\n",
    "**Solution**: \n",
    "\n",
    "**For $S$**: $S$ can have values in the range $\\{2, 3, \\ldots, 12\\}$.\n",
    "- There is only **one way** to get $S = 2$: $(1,1)$\n",
    "- There are **two ways** to get $S = 3$: $(1,2), (2,1)$\n",
    "- And so on...\n",
    "\n",
    "Using the methods of Chapter 3 for each case, the probabilities for $2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12$ are:\n",
    "$$\\frac{1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1}{36}$$\n",
    "\n",
    "**For $D$**: $D$ can have values in the range $\\{-5, -4, \\ldots, 4, 5\\}$.\n",
    "\n",
    "Again, using the methods of Chapter 3, the probabilities for $-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5$ are:\n",
    "$$\\frac{1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1}{36}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Two dice\n",
    "dice_outcomes = list(product(range(1, 7), repeat=2))\n",
    "\n",
    "# Compute S = X + Y\n",
    "sums = [x + y for x, y in dice_outcomes]\n",
    "sum_counts = {}\n",
    "for s in range(2, 13):\n",
    "    sum_counts[s] = sums.count(s)\n",
    "\n",
    "# Compute D = X - Y\n",
    "diffs = [x - y for x, y in dice_outcomes]\n",
    "diff_counts = {}\n",
    "for d in range(-5, 6):\n",
    "    diff_counts[d] = diffs.count(d)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot S distribution\n",
    "ax1.bar(sum_counts.keys(), [c/36 for c in sum_counts.values()], \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Sum (S = X + Y)')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title('Distribution of Sum of Two Dice')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot D distribution\n",
    "ax2.bar(diff_counts.keys(), [c/36 for c in diff_counts.values()],\n",
    "        edgecolor='black', alpha=0.7, color='orange')\n",
    "ax2.set_xlabel('Difference (D = X - Y)')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('Distribution of Difference of Two Dice')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution of S (sum):\")\n",
    "for s, count in sum_counts.items():\n",
    "    print(f\"  P(S = {s:2d}) = {count}/36 = {count/36:.4f}\")\n",
    "\n",
    "print(\"\\nDistribution of D (difference):\")\n",
    "for d, count in sorted(diff_counts.items()):\n",
    "    print(f\"  P(D = {d:2d}) = {count}/36 = {count/36:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Worked Example 4.4: Joint Distribution of S and D\n",
    "\n",
    "**Problem**: Using the terminology of Example 4.3, what is the **joint probability distribution** of $S$ and $D$?\n",
    "\n",
    "**Solution**: This is more interesting to display, because it's an $11 \\times 11$ table. Each entry of the table represents a pair of $(S, D)$ values.\n",
    "\n",
    "Many pairs **can't occur**:\n",
    "- For example, for $S = 2$, $D$ can only be zero\n",
    "- If $S$ is even, then $D$ must be even\n",
    "- And so on\n",
    "\n",
    "You can work out the table by checking each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build joint distribution table\n",
    "joint = {}\n",
    "for x in range(1, 7):\n",
    "    for y in range(1, 7):\n",
    "        s = x + y\n",
    "        d = x - y\n",
    "        joint[(s, d)] = joint.get((s, d), 0) + 1/36\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "S_values = range(2, 13)\n",
    "D_values = range(-5, 6)\n",
    "joint_matrix = np.zeros((len(S_values), len(D_values)))\n",
    "\n",
    "for i, s in enumerate(S_values):\n",
    "    for j, d in enumerate(D_values):\n",
    "        joint_matrix[i, j] = joint.get((s, d), 0)\n",
    "\n",
    "# Plot as heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(joint_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Probability')\n",
    "plt.xlabel('D (Difference)')\n",
    "plt.ylabel('S (Sum)')\n",
    "plt.title('Joint Probability Distribution P(S, D)')\n",
    "plt.xticks(range(len(D_values)), D_values)\n",
    "plt.yticks(range(len(S_values)), S_values)\n",
    "\n",
    "# Annotate cells with probabilities\n",
    "for i in range(len(S_values)):\n",
    "    for j in range(len(D_values)):\n",
    "        if joint_matrix[i, j] > 0:\n",
    "            plt.text(j, i, f'{joint_matrix[i, j]:.3f}', \n",
    "                    ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Joint Distribution Table (non-zero entries):\")\n",
    "for (s, d), prob in sorted(joint.items()):\n",
    "    if prob > 0:\n",
    "        print(f\"  P(S={s:2d}, D={d:2d}) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Worked Example 4.5: Are S and D Independent?\n",
    "\n",
    "**Problem**: Using the terminology of Example 4.3, are $X$ and $Y$ independent? Are $S$ and $D$ independent?\n",
    "\n",
    "**Solution**: \n",
    "\n",
    "$X$ and $Y$ are **clearly independent** (outcomes of two different dice).\n",
    "\n",
    "But $S$ and $D$ are **not independent**. There are several ways to see this:\n",
    "\n",
    "1. **Notice that**: If you know $S = 2$, then you know the value of $D$ precisely ($D = 0$). But if you know $S = 3$, $D$ could be either $-1$ or $1$. This means that $P(D|S)$ depends on $S$, so they're not independent.\n",
    "\n",
    "2. **Matrix rank**: The rank of the joint distribution table, as a matrix, is 6, which means that it **can't** be the outer product of two vectors (which would have rank 1).\n",
    "\n",
    "3. **Check formula**: For independence, we need $P(S, D) = P(S)P(D)$ for all $(S, D)$ pairs. Let's check one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check independence\n",
    "# Pick S=7, D=1\n",
    "s_val, d_val = 7, 1\n",
    "\n",
    "# P(S=7, D=1)\n",
    "p_s_and_d = joint.get((s_val, d_val), 0)\n",
    "\n",
    "# P(S=7)\n",
    "p_s = sum_counts[s_val] / 36\n",
    "\n",
    "# P(D=1)\n",
    "p_d = diff_counts[d_val] / 36\n",
    "\n",
    "# P(S) * P(D)\n",
    "p_s_times_p_d = p_s * p_d\n",
    "\n",
    "print(f\"Testing independence for S={s_val}, D={d_val}:\")\n",
    "print(f\"  P(S={s_val}, D={d_val}) = {p_s_and_d:.4f}\")\n",
    "print(f\"  P(S={s_val}) = {p_s:.4f}\")\n",
    "print(f\"  P(D={d_val}) = {p_d:.4f}\")\n",
    "print(f\"  P(S) × P(D) = {p_s_times_p_d:.4f}\")\n",
    "print(f\"\\n  Independent? {np.isclose(p_s_and_d, p_s_times_p_d)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing independence for S=7, D=1:\n",
    "  P(S=7, D=1) = 0.0833\n",
    "  P(S=7) = 0.1667\n",
    "  P(D=1) = 0.1389\n",
    "  P(S) × P(D) = 0.0231\n",
    "\n",
    "  Independent? False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Worked Example 4.6: Conditional Probabilities\n",
    "\n",
    "**Problem**: Using the terminology of Example 4.3, what is $P(S|D = 0)$? What is $P(D|S = 11)$?\n",
    "\n",
    "**Solution**: You could work out either of these from the table, or by first principles.\n",
    "\n",
    "**Part 1**: If $D = 0$, then $X = Y$. So $S$ can have values $\\{2, 4, 6, 8, 10, 12\\}$, and each value has conditional probability $\\frac{1}{6}$.\n",
    "\n",
    "**Part 2**: If $S = 11$, then either:\n",
    "- $X = 5, Y = 6$ (so $D = -1$), or\n",
    "- $X = 6, Y = 5$ (so $D = 1$)\n",
    "\n",
    "Each value has conditional probability $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(S | D=0)\n",
    "d_0_outcomes = [(x, y) for x, y in dice_outcomes if x - y == 0]\n",
    "print(f\"When D=0, possible outcomes: {d_0_outcomes}\")\n",
    "print(f\"Sums when D=0: {[x+y for x,y in d_0_outcomes]}\")\n",
    "print(f\"P(S | D=0): Each of {set([x+y for x,y in d_0_outcomes])} has probability 1/6\\n\")\n",
    "\n",
    "# P(D | S=11)\n",
    "s_11_outcomes = [(x, y) for x, y in dice_outcomes if x + y == 11]\n",
    "print(f\"When S=11, possible outcomes: {s_11_outcomes}\")\n",
    "print(f\"Differences when S=11: {[x-y for x,y in s_11_outcomes]}\")\n",
    "print(f\"P(D | S=11): Each of {set([x-y for x,y in s_11_outcomes])} has probability 1/2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.2 Just a Little Continuous Probability\n",
    "\n",
    "Our random variables take values from a **discrete** set of numbers $\\mathcal{D}$. This makes the underlying machinery somewhat simpler to describe, and is often, but not always, enough for model building.\n",
    "\n",
    "Some phenomena are more naturally modelled as being **continuous**, for example:\n",
    "- Human height\n",
    "- Human weight  \n",
    "- The mass of a distant star\n",
    "- And so on\n",
    "\n",
    "Giving a complete formal description of probability on a continuous space is surprisingly tricky. These issues are caused by two interrelated facts:\n",
    "1. Real numbers have **infinite precision**\n",
    "2. You **can't count** real numbers\n",
    "\n",
    "### Continuous Random Variables\n",
    "\n",
    "A **continuous random variable** is still a random variable, and comes with all the stuff that a random variable comes with. We will not speculate on what the underlying sample space is, nor on the underlying events.\n",
    "\n",
    "The most interesting thing for us is specifying the **probability distribution**. Rather than talk about the probability that a real number takes a particular value (which we can't really do satisfactorily most of the time), we will instead talk about the **probability that it lies in some interval**.\n",
    "\n",
    "---\n",
    "\n",
    "## Probability Density Functions (PDF)\n",
    "\n",
    "So we can specify a probability distribution for a continuous random variable by giving a set of very small intervals, and for each interval providing the probability that the random variable lies in this interval. The easiest way to do this is to supply a **probability density function**.\n",
    "\n",
    "Let $p(x)$ be a probability density function (often called a **pdf** or **density**) for a continuous random variable $X$. We interpret this function by thinking in terms of small intervals.\n",
    "\n",
    "Assume that $dx$ is an **infinitesimally small interval**. Then:\n",
    "\n",
    "$$p(x)dx = P(\\{\\text{event that } X \\text{ takes a value in the range } [x, x + dx]\\})$$\n",
    "\n",
    "---\n",
    "\n",
    "## Useful Facts 4.1: Properties of Probability Density Functions\n",
    "\n",
    "1. **Non-negative**: Probability density functions are non-negative. This follows from the definition—a negative value at some $u$ would imply that $P(\\{x \\in [u, u + du]\\})$ was negative, and this cannot occur.\n",
    "\n",
    "2. **Integration gives probability**: For $a < b$:\n",
    "   $$P(\\{X \\text{ takes a value in the range } [a, b]\\}) = \\int_a^b p(x)dx$$\n",
    "   which we obtain by summing $p(x)dx$ over all the infinitesimal intervals between $a$ and $b$.\n",
    "\n",
    "3. **Normalizes to 1**: We must have that:\n",
    "   $$\\int_{-\\infty}^{\\infty} p(x)dx = 1$$\n",
    "   This is because:\n",
    "   $$P(\\{X \\text{ takes a value in the range } [-\\infty, \\infty]\\}) = 1$$\n",
    "\n",
    "### Normalizing\n",
    "\n",
    "The property that $\\int_{-\\infty}^{\\infty} p(x)dx = 1$ is useful, because when we are trying to determine a probability density function, we can **ignore a constant factor**.\n",
    "\n",
    "So if $g(x)$ is a non-negative function that is **proportional** to the probability density function (often pdf) we are interested in, we can recover the pdf by computing:\n",
    "\n",
    "$$p(x) = \\frac{1}{\\int_{-\\infty}^{\\infty} g(x)dx} \\cdot g(x)$$\n",
    "\n",
    "This procedure is sometimes known as **normalizing**, and $\\int_{-\\infty}^{\\infty} g(x)dx$ is the **normalizing constant**.\n",
    "\n",
    "---\n",
    "\n",
    "## The Histogram Perspective\n",
    "\n",
    "One good way to think about pdfs is as **the limit of a histogram**. Imagine you:\n",
    "\n",
    "1. Collect an arbitrarily large dataset of data items, each of which is independent\n",
    "2. Build a histogram of that dataset, using arbitrarily narrow boxes\n",
    "3. Scale the histogram so that the sum of the box areas is one\n",
    "\n",
    "The result is a probability density function.\n",
    "\n",
    "The pdf doesn't represent the probability that a random variable takes a value. Instead, you should think of $p(x)$ as being the **limit of a ratio**:\n",
    "\n",
    "$$p(x) = \\lim_{\\text{small interval}} \\frac{\\text{probability that the random variable will lie in a small interval centered on } x}{\\text{length of the small interval centered on } x}$$\n",
    "\n",
    "(which is why it's called a **density**)\n",
    "\n",
    "Notice that, while a pdf has to be:\n",
    "- **Non-negative**\n",
    "- **Integrate to 1**\n",
    "\n",
    "it does **not** have to be smaller than one. A ratio like this could be a lot larger than one, as long as it isn't larger than one for too many $x$ (because the integral must be one).\n",
    "\n",
    "---\n",
    "\n",
    "## Worked Example 4.7: A PDF Larger than One\n",
    "\n",
    "**Problem**: Assume we have a physical system that can produce random numbers. It produces numbers in the range $[0, \\epsilon]$, where $0 < \\epsilon < 1$. Each number has the **same probability** of appearing. No number larger than $\\epsilon$ or smaller than $0$ can ever appear. What is the probability density function?\n",
    "\n",
    "**Solution**: Write $p(x)$ for the probability density function. We must have that:\n",
    "- $p(x) = 0$ for $x < 0$\n",
    "- $p(x) = 0$ for $x > \\epsilon$\n",
    "- $p(x)$ is constant between $0$ and $\\epsilon$\n",
    "- $\\int_{-\\infty}^{\\infty} p(x)dx = 1$\n",
    "\n",
    "So:\n",
    "$$p(x) = \\begin{cases}\n",
    "0 & \\text{if } x < 0 \\\\\n",
    "\\frac{1}{\\epsilon} & \\text{if } 0 \\leq x \\leq \\epsilon \\\\\n",
    "0 & \\text{if } x > \\epsilon\n",
    "\\end{cases}$$\n",
    "\n",
    "**Notice** that if $\\epsilon < 1$, we have that $p(x) > 1$ for all $x$ in $[0, \\epsilon]$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF larger than 1\n",
    "epsilons = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, eps in enumerate(epsilons):\n",
    "    x = np.linspace(-0.5, 3, 1000)\n",
    "    pdf = np.where((x >= 0) & (x <= eps), 1/eps, 0)\n",
    "    \n",
    "    axes[idx].plot(x, pdf, linewidth=2, label=f'p(x) = 1/{eps:.1f}')\n",
    "    axes[idx].fill_between(x, 0, pdf, alpha=0.3)\n",
    "    axes[idx].axhline(y=1, color='red', linestyle='--', linewidth=1, \n",
    "                      label='p(x) = 1')\n",
    "    axes[idx].set_xlabel('x')\n",
    "    axes[idx].set_ylabel('p(x)')\n",
    "    axes[idx].set_title(f'Uniform on [0, {eps}]: max p(x) = {1/eps:.1f}')\n",
    "    axes[idx].set_ylim([0, max(3, 1.2/eps)])\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].legend()\n",
    "    \n",
    "    # Verify integral = 1\n",
    "    integral = eps * (1/eps)\n",
    "    axes[idx].text(0.5, 0.95, f'∫p(x)dx = {integral:.1f}', \n",
    "                  transform=axes[idx].transAxes, \n",
    "                  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Important Note on Notation\n",
    "\n",
    "```{admonition} Remember This\n",
    ":class: warning\n",
    "Probability notation can be **quirky**:\n",
    "\n",
    "1. Usually, one uses a **big $P$** for actual probabilities, and a **small $p$** for probability densities\n",
    "\n",
    "2. The argument, or context, is supposed to tell you which probability distribution is meant (i.e., $P(X)$ likely refers to a different probability distribution than $P(Y)$, which should strike a computer scientist familiar with dummy variables as bizarre)\n",
    "\n",
    "3. Because the probability distribution for a discrete random variable is a collection of probabilities, following this convention requires that such a probability distribution be written with a big $P$. However, having different notation for discrete and continuous random variables can get quite clunky.\n",
    "\n",
    "4. In application areas it is usual to write a **small $p$** for a probability distribution, and whether a density or a distribution is intended depends on whether the random variable is continuous or discrete.\n",
    "\n",
    "5. If you want to emphasize that a probability is intended, you can write $P$.\n",
    "\n",
    "6. You may encounter:\n",
    "   - $p(x)$ with the meaning \"some probability distribution\"\n",
    "   - $p(x)$ meaning the value of the probability distribution $P(\\{X = x\\})$ at the point $x$\n",
    "   - $p(x)$ with the meaning the probability distribution $P(\\{X = x\\})$ as a function of $x$\n",
    "\n",
    "7. Cumulative distributions are often written with an $F$, so that an unexpected $F(x)$ might mean $P(\\{X \\leq x\\})$.\n",
    "\n",
    "You can usually figure out what is intended—as long as you don't think too closely about it (authors are often quite inconsistent; context may help disambiguate different intended meanings, too).\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Random variables** map outcomes to numbers: $X: \\Omega \\rightarrow \\mathbb{R}$\n",
    "2. **Discrete RVs** have probability mass functions (PMF): $P(X = x)$\n",
    "3. **Continuous RVs** have probability density functions (PDF): $p(x)$\n",
    "4. **Joint probability** describes multiple RVs: $P(x, y)$\n",
    "5. **Marginal probability**: $P(x) = \\sum_y P(x, y)$\n",
    "6. **Conditional probability**: $P(x|y) = \\frac{P(x,y)}{P(y)}$\n",
    "7. **Independence**: $P(x, y) = P(x)P(y)$\n",
    "8. For continuous RVs: $P(X = x) = 0$ for any specific $x$\n",
    "9. PDFs can be **greater than 1**!\n",
    "\n",
    "### Important Formulas\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|-------------|\n",
    "| $P(x) = \\sum_y P(x, y)$ | Marginalization |\n",
    "| $P(x|y) = \\frac{P(y|x)P(x)}{P(y)}$ | Bayes' rule |\n",
    "| $P(x, y) = P(x)P(y)$ | Independence |\n",
    "| $P(a \\leq X \\leq b) = \\int_a^b p(x)dx$ | Probability from PDF |\n",
    "| $\\int_{-\\infty}^{\\infty} p(x)dx = 1$ | PDF normalization |\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Problems\n",
    "\n",
    "1. **Discrete RV**: Define random variable $X$ = number of heads in 3 coin flips. Find the complete PMF $P(X = k)$ for all $k$.\n",
    "\n",
    "2. **Independence**: For two dice, define $X$ = first die, $Y$ = second die, $Z = X + Y$. Are $X$ and $Z$ independent? Prove your answer.\n",
    "\n",
    "3. **Uniform distribution**: If $X$ is uniform on $[0, 10]$, find:\n",
    "   - $P(3 \\leq X \\leq 7)$\n",
    "   - The PDF $p(x)$\n",
    "   - Verify that the PDF can be greater than 1 for some uniform distributions\n",
    "\n",
    "4. **Joint distribution**: Two coins are flipped. $X$ = number of heads on first coin, $Y$ = number of heads on second coin. Write out the complete joint distribution $P(X, Y)$.\n",
    "\n",
    "5. **Conditional**: You roll a die. $X$ = the outcome. Given that $X$ is even, what is $P(X = 2 | X \\text{ is even})$?\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that we understand random variables and their distributions, we can:\n",
    "\n",
    "1. Define **expected values** and explore properties of expectations\n",
    "2. Study important **discrete distributions** (Bernoulli, Binomial, Poisson)\n",
    "3. Study important **continuous distributions** (Uniform, Exponential, Normal)\n",
    "4. Apply the **weak law of large numbers**\n",
    "\n",
    "→ Continue to [4.2 Expectations and Expected Values](ch04_expectations.md)\n",
    "\n",
    "→ Return to [Chapter 4 Overview](chapter04.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}