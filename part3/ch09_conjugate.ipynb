{
 "cells": [
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "# 9.3 Conjugate Priors and MAP Estimation\n",
    "\n",
    "## Conjugate Priors\n",
    "\n",
    "### Definition\n",
    "\n",
    "A prior distribution is **conjugate** to a likelihood if the posterior has the **same distributional form** as the prior.\n",
    "\n",
    "$$\n",
    "\\text{Prior family} + \\text{Likelihood} \\Rightarrow \\text{Posterior in same family}\n",
    "$$\n",
    "\n",
    "**Why Conjugacy Matters**:\n",
    "- Closed-form posterior (no numerical integration needed)\n",
    "- Easy sequential updating\n",
    "- Interpretable hyperparameters\n",
    "- Computational efficiency\n",
    "\n",
    "## Common Conjugate Pairs\n",
    "\n",
    "### Table of Conjugate Priors\n",
    "\n",
    "| Likelihood | Parameter | Conjugate Prior | Posterior |\n",
    "|------------|-----------|-----------------|------------|\n",
    "| Bernoulli(p) | p | Beta(α, β) | Beta(α+k, β+n-k) |\n",
    "| Binomial(n, p) | p | Beta(α, β) | Beta(α+k, β+n-k) |\n",
    "| Poisson(λ) | λ | Gamma(α, β) | Gamma(α+Σx, β+n) |\n",
    "| Exponential(λ) | λ | Gamma(α, β) | Gamma(α+n, β+Σx) |\n",
    "| Normal(μ, σ²) | μ (σ² known) | Normal(μ₀, σ₀²) | Normal(...) |\n",
    "| Normal(μ, σ²) | σ² (μ known) | Inverse-Gamma | Inverse-Gamma |\n",
    "\n",
    "where:\n",
    "- k = number of successes\n",
    "- n = number of trials\n",
    "- Σx = sum of observations\n",
    "\n",
    "## Beta-Binomial (Detailed)\n",
    "\n",
    "### The Setup\n",
    "\n",
    "**Likelihood**: Binomial\n",
    "\n",
    "$$\n",
    "p(k \\mid p, n) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "**Prior**: Beta\n",
    "\n",
    "$$\n",
    "p(p) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "$$\n",
    "\n",
    "### Posterior Derivation\n",
    "\n",
    "$$\n",
    "p(p \\mid k, n) \\propto p(k \\mid p) \\cdot p(p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\propto p^k (1-p)^{n-k} \\cdot p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= p^{(\\alpha+k)-1}(1-p)^{(\\beta+n-k)-1}\n",
    "$$\n",
    "\n",
    "This is the kernel of:\n",
    "\n",
    "$$\n",
    "p(p \\mid k, n) = \\text{Beta}(\\alpha + k, \\beta + n - k)\n",
    "$$\n",
    "\n",
    "### Hyperparameter Interpretation\n",
    "\n",
    "**Prior hyperparameters**:\n",
    "- \\(\\alpha\\) = \"pseudo-counts\" of successes\n",
    "- \\(\\beta\\) = \"pseudo-counts\" of failures\n",
    "- \\(\\alpha + \\beta\\) = \"effective sample size\" of prior\n",
    "\n",
    "**Prior mean**: \\(\\frac{\\alpha}{\\alpha + \\beta}\\)\n",
    "\n",
    "**Posterior mean**:\n",
    "\n",
    "$$\n",
    "\\frac{\\alpha + k}{\\alpha + \\beta + n} = \\frac{\\alpha + \\beta}{\\alpha + \\beta + n} \\cdot \\frac{\\alpha}{\\alpha + \\beta} + \\frac{n}{\\alpha + \\beta + n} \\cdot \\frac{k}{n}\n",
    "$$\n",
    "\n",
    "This is a **weighted average** of prior mean and MLE!\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def beta_binomial_update(alpha_prior, beta_prior, k, n):\n",
    "    \"\"\"\n",
    "    Update Beta prior with Binomial data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha_prior, beta_prior : Prior Beta parameters\n",
    "    k : Number of successes observed\n",
    "    n : Number of trials\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    alpha_post, beta_post : Posterior Beta parameters\n",
    "    \"\"\"\n",
    "    alpha_post = alpha_prior + k\n",
    "    beta_post = beta_prior + (n - k)\n",
    "    return alpha_post, beta_post\n",
    "\n",
    "# Example\n",
    "print(\"Beta-Binomial Conjugacy Example\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prior\n",
    "alpha_prior, beta_prior = 2, 2\n",
    "print(f\"\\nPrior: Beta({alpha_prior}, {beta_prior})\")\n",
    "print(f\"Prior mean: {alpha_prior/(alpha_prior + beta_prior):.3f}\")\n",
    "\n",
    "# Data\n",
    "k, n = 15, 20  # 15 successes in 20 trials\n",
    "print(f\"\\nData: {k} successes in {n} trials\")\n",
    "print(f\"MLE: {k/n:.3f}\")\n",
    "\n",
    "# Posterior\n",
    "alpha_post, beta_post = beta_binomial_update(alpha_prior, beta_prior, k, n)\n",
    "print(f\"\\nPosterior: Beta({alpha_post}, {beta_post})\")\n",
    "print(f\"Posterior mean: {alpha_post/(alpha_post + beta_post):.3f}\")\n",
    "\n",
    "# Visualize\n",
    "p_range = np.linspace(0, 1, 1000)\n",
    "prior = stats.beta(alpha_prior, beta_prior)\n",
    "posterior = stats.beta(alpha_post, beta_post)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_range, prior.pdf(p_range), 'b--', linewidth=2, label='Prior', alpha=0.7)\n",
    "plt.plot(p_range, posterior.pdf(p_range), 'r-', linewidth=2.5, label='Posterior')\n",
    "plt.axvline(k/n, color='green', linestyle=':', linewidth=2, label=f'MLE = {k/n:.2f}')\n",
    "plt.axvline(alpha_post/(alpha_post + beta_post), color='red', linestyle=':', \n",
    "            linewidth=1.5, label=f'Posterior mean = {alpha_post/(alpha_post + beta_post):.2f}')\n",
    "plt.xlabel('p (probability)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Beta-Binomial Conjugacy', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('beta_binomial_conjugate.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta-Binomial Conjugacy Example\n",
    "======================================================================\n",
    "\n",
    "Prior: Beta(2, 2)\n",
    "Prior mean: 0.500\n",
    "\n",
    "Data: 15 successes in 20 trials\n",
    "MLE: 0.750\n",
    "\n",
    "Posterior: Beta(17, 7)\n",
    "Posterior mean: 0.708"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_65262f3f4099.png)\n",
    "\n",
    "\n",
    "## Gamma-Poisson (Detailed)\n",
    "\n",
    "### The Setup\n",
    "\n",
    "**Likelihood**: Poisson observations \\(x_1, \\ldots, x_n\\)\n",
    "\n",
    "$$\n",
    "p(x_1, \\ldots, x_n \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!} \\propto \\lambda^{\\sum x_i} e^{-n\\lambda}\n",
    "$$\n",
    "\n",
    "**Prior**: Gamma\n",
    "\n",
    "$$\n",
    "p(\\lambda) = \\text{Gamma}(\\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}\n",
    "$$\n",
    "\n",
    "### Posterior Derivation\n",
    "\n",
    "$$\n",
    "p(\\lambda \\mid x_1, \\ldots, x_n) \\propto p(x_1, \\ldots, x_n \\mid \\lambda) \\cdot p(\\lambda)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\propto \\lambda^{\\sum x_i} e^{-n\\lambda} \\cdot \\lambda^{\\alpha-1} e^{-\\beta \\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\lambda^{(\\alpha + \\sum x_i) - 1} e^{-(\\beta + n)\\lambda}\n",
    "$$\n",
    "\n",
    "This is the kernel of:\n",
    "\n",
    "$$\n",
    "p(\\lambda \\mid x_1, \\ldots, x_n) = \\text{Gamma}(\\alpha + \\sum x_i, \\beta + n)\n",
    "$$\n",
    "\n",
    "### Hyperparameter Interpretation\n",
    "\n",
    "- \\(\\alpha\\) = prior \"total count\"\n",
    "- \\(\\beta\\) = prior \"observation periods\"\n",
    "- Prior mean: \\(\\frac{\\alpha}{\\beta}\\)\n",
    "- Posterior mean: \\(\\frac{\\alpha + \\sum x_i}{\\beta + n}\\)\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameter\n",
    "true_lambda = 5.0\n",
    "\n",
    "# Generate Poisson data\n",
    "n = 30\n",
    "data = np.random.poisson(true_lambda, n)\n",
    "sum_x = np.sum(data)\n",
    "\n",
    "print(\"Gamma-Poisson Conjugacy Example\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTrue λ: {true_lambda}\")\n",
    "print(f\"Data: {n} observations, sum = {sum_x}\")\n",
    "print(f\"MLE: {sum_x/n:.3f}\")\n",
    "\n",
    "# Prior: Gamma(2, 1) - weakly informative\n",
    "alpha_prior = 2\n",
    "beta_prior = 1\n",
    "print(f\"\\nPrior: Gamma({alpha_prior}, {beta_prior})\")\n",
    "print(f\"Prior mean: {alpha_prior/beta_prior:.3f}\")\n",
    "\n",
    "# Posterior\n",
    "alpha_post = alpha_prior + sum_x\n",
    "beta_post = beta_prior + n\n",
    "print(f\"\\nPosterior: Gamma({alpha_post}, {beta_post})\")\n",
    "print(f\"Posterior mean: {alpha_post/beta_post:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "lambda_range = np.linspace(0, 10, 1000)\n",
    "prior = stats.gamma(alpha_prior, scale=1/beta_prior)\n",
    "posterior = stats.gamma(alpha_post, scale=1/beta_post)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_range, prior.pdf(lambda_range), 'b--', linewidth=2, \n",
    "         label='Prior', alpha=0.7)\n",
    "plt.plot(lambda_range, posterior.pdf(lambda_range), 'r-', linewidth=2.5, \n",
    "         label='Posterior')\n",
    "plt.axvline(true_lambda, color='black', linestyle='--', linewidth=1.5,\n",
    "            label=f'True λ = {true_lambda}', alpha=0.5)\n",
    "plt.axvline(sum_x/n, color='green', linestyle=':', linewidth=2,\n",
    "            label=f'MLE = {sum_x/n:.2f}')\n",
    "plt.axvline(alpha_post/beta_post, color='red', linestyle=':', linewidth=1.5,\n",
    "            label=f'Posterior mean = {alpha_post/beta_post:.2f}')\n",
    "plt.xlabel('λ (rate parameter)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Gamma-Poisson Conjugacy', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim(0, 10)\n",
    "plt.savefig('gamma_poisson_conjugate.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma-Poisson Conjugacy Example\n",
    "======================================================================\n",
    "\n",
    "True λ: 5.0\n",
    "Data: 30 observations, sum = 142\n",
    "MLE: 4.733\n",
    "\n",
    "Prior: Gamma(2, 1)\n",
    "Prior mean: 2.000\n",
    "\n",
    "Posterior: Gamma(144, 31)\n",
    "Posterior mean: 4.645"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_3b698bf38975.png)\n",
    "\n",
    "\n",
    "## Normal-Normal (Known Variance)\n",
    "\n",
    "### The Setup\n",
    "\n",
    "**Likelihood**: \\(x_1, \\ldots, x_n \\sim N(\\mu, \\sigma^2)\\) where \\(\\sigma^2\\) is known\n",
    "\n",
    "$$\n",
    "p(x_1, \\ldots, x_n \\mid \\mu) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "**Prior**: \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\)\n",
    "\n",
    "$$\n",
    "p(\\mu) \\propto \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2}\\right)\n",
    "$$\n",
    "\n",
    "### Posterior (Closed Form)\n",
    "\n",
    "The posterior is also Normal:\n",
    "\n",
    "$$\n",
    "p(\\mu \\mid x_1, \\ldots, x_n) = N(\\mu_n, \\sigma_n^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "**Posterior mean**:\n",
    "\n",
    "$$\n",
    "\\mu_n = \\frac{\\sigma^2}{\\sigma^2 + n\\sigma_0^2} \\mu_0 + \\frac{n\\sigma_0^2}{\\sigma^2 + n\\sigma_0^2} \\bar{x}\n",
    "$$\n",
    "\n",
    "**Posterior variance**:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma_n^2} = \\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "(Precision of posterior = precision of prior + precision of data)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- Posterior mean is weighted average of prior mean and sample mean\n",
    "- Weights depend on relative precisions (inverse variances)\n",
    "- As \\(n \\to \\infty\\), posterior mean \\(\\to \\bar{x}\\) (data dominates)\n",
    "- Posterior variance always smaller than prior variance (we learned!)\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameter\n",
    "true_mu = 5.0\n",
    "sigma = 2.0  # Known variance\n",
    "\n",
    "# Generate data\n",
    "n = 10\n",
    "data = np.random.normal(true_mu, sigma, n)\n",
    "xbar = np.mean(data)\n",
    "\n",
    "print(\"Normal-Normal Conjugacy Example\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTrue μ: {true_mu}\")\n",
    "print(f\"Known σ: {sigma}\")\n",
    "print(f\"Data: n = {n}, x̄ = {xbar:.3f}\")\n",
    "print(f\"MLE: {xbar:.3f}\")\n",
    "\n",
    "# Prior\n",
    "mu_0 = 0.0\n",
    "sigma_0 = 5.0\n",
    "print(f\"\\nPrior: N({mu_0}, {sigma_0**2})\")\n",
    "\n",
    "# Posterior parameters\n",
    "precision_prior = 1 / sigma_0**2\n",
    "precision_data = n / sigma**2\n",
    "precision_post = precision_prior + precision_data\n",
    "sigma_n = np.sqrt(1 / precision_post)\n",
    "\n",
    "mu_n = (precision_prior * mu_0 + precision_data * xbar) / precision_post\n",
    "\n",
    "print(f\"\\nPosterior: N({mu_n:.3f}, {sigma_n**2:.3f})\")\n",
    "print(f\"Posterior mean: {mu_n:.3f}\")\n",
    "print(f\"Posterior std: {sigma_n:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "mu_range = np.linspace(-5, 10, 1000)\n",
    "prior = stats.norm(mu_0, sigma_0)\n",
    "posterior = stats.norm(mu_n, sigma_n)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mu_range, prior.pdf(mu_range), 'b--', linewidth=2, \n",
    "         label=f'Prior: N({mu_0}, {sigma_0**2})', alpha=0.7)\n",
    "plt.plot(mu_range, posterior.pdf(mu_range), 'r-', linewidth=2.5,\n",
    "         label=f'Posterior: N({mu_n:.1f}, {sigma_n**2:.1f})')\n",
    "plt.axvline(true_mu, color='black', linestyle='--', linewidth=1.5,\n",
    "            label=f'True μ = {true_mu}', alpha=0.5)\n",
    "plt.axvline(xbar, color='green', linestyle=':', linewidth=2,\n",
    "            label=f'MLE (x̄) = {xbar:.2f}')\n",
    "plt.xlabel('μ', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Normal-Normal Conjugacy (Known Variance)', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('normal_normal_conjugate.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal-Normal Conjugacy Example\n",
    "======================================================================\n",
    "\n",
    "True μ: 5.0\n",
    "Known σ: 2.0\n",
    "Data: n = 10, x̄ = 5.896\n",
    "MLE: 5.896\n",
    "\n",
    "Prior: N(0.0, 25.0)\n",
    "\n",
    "Posterior: N(5.803, 0.394)\n",
    "Posterior mean: 5.803\n",
    "Posterior std: 0.627"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_202a17daec93.png)\n",
    "\n",
    "\n",
    "## Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "### Definition\n",
    "\n",
    "The **MAP estimate** is the mode of the posterior distribution:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} p(\\theta \\mid \\text{data})\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} p(\\text{data} \\mid \\theta) \\cdot p(\\theta)\n",
    "$$\n",
    "\n",
    "Taking logs:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\left[ \\log p(\\text{data} \\mid \\theta) + \\log p(\\theta) \\right]\n",
    "$$\n",
    "\n",
    "### MAP vs. MLE\n",
    "\n",
    "**MLE**:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} \\log p(\\text{data} \\mid \\theta)\n",
    "$$\n",
    "\n",
    "**MAP**:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\left[ \\log p(\\text{data} \\mid \\theta) + \\log p(\\theta) \\right]\n",
    "$$\n",
    "\n",
    "**Key insight**: MAP = MLE + log prior penalty\n",
    "\n",
    "### MAP as Regularized MLE\n",
    "\n",
    "The log prior acts as a **regularization term**!\n",
    "\n",
    "**Example**: Normal prior \\(\\theta \\sim N(0, \\sigma_p^2)\\)\n",
    "\n",
    "$$\n",
    "\\log p(\\theta) = -\\frac{\\theta^2}{2\\sigma_p^2} + \\text{const}\n",
    "$$\n",
    "\n",
    "So MAP becomes:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\left[ \\log p(\\text{data} \\mid \\theta) - \\frac{\\theta^2}{2\\sigma_p^2} \\right]\n",
    "$$\n",
    "\n",
    "This is equivalent to **L2 regularization (Ridge regression)**!\n",
    "\n",
    "### When MAP = Posterior Mean\n",
    "\n",
    "For **symmetric unimodal** posteriors:\n",
    "- Mode = Mean = Median\n",
    "- MAP estimate = Posterior mean\n",
    "\n",
    "Examples: Normal posterior, symmetric Beta\n",
    "\n",
    "### Python Example: Comparing Point Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data\n",
    "true_p = 0.7\n",
    "n = 20\n",
    "k = np.random.binomial(n, true_p)\n",
    "\n",
    "print(\"Comparing Point Estimates: MLE, MAP, Posterior Mean\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData: {k} heads in {n} flips\")\n",
    "print(f\"True p: {true_p}\")\n",
    "\n",
    "# Prior: Beta(2, 2)\n",
    "alpha_prior, beta_prior = 2, 2\n",
    "\n",
    "# Posterior: Beta(alpha + k, beta + n - k)\n",
    "alpha_post = alpha_prior + k\n",
    "beta_post = beta_prior + (n - k)\n",
    "\n",
    "print(f\"\\nPrior: Beta({alpha_prior}, {beta_prior})\")\n",
    "print(f\"Posterior: Beta({alpha_post}, {beta_post})\")\n",
    "\n",
    "# Point estimates\n",
    "p_mle = k / n\n",
    "p_map = (alpha_post - 1) / (alpha_post + beta_post - 2)  # Mode of Beta\n",
    "p_mean = alpha_post / (alpha_post + beta_post)  # Mean of Beta\n",
    "p_median = stats.beta(alpha_post, beta_post).median()\n",
    "\n",
    "print(f\"\\nPoint Estimates:\")\n",
    "print(f\"  MLE (no prior):      {p_mle:.4f}\")\n",
    "print(f\"  MAP (mode):          {p_map:.4f}\")\n",
    "print(f\"  Posterior mean:      {p_mean:.4f}\")\n",
    "print(f\"  Posterior median:    {p_median:.4f}\")\n",
    "print(f\"  True p:              {true_p:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "p_range = np.linspace(0, 1, 1000)\n",
    "posterior = stats.beta(alpha_post, beta_post)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(p_range, posterior.pdf(p_range), 'b-', linewidth=2.5, \n",
    "         label='Posterior')\n",
    "plt.axvline(true_p, color='black', linestyle='--', linewidth=2,\n",
    "            label=f'True p = {true_p:.2f}', alpha=0.7)\n",
    "plt.axvline(p_mle, color='green', linestyle=':', linewidth=2,\n",
    "            label=f'MLE = {p_mle:.3f}')\n",
    "plt.axvline(p_map, color='red', linestyle='-.', linewidth=2,\n",
    "            label=f'MAP = {p_map:.3f}')\n",
    "plt.axvline(p_mean, color='purple', linestyle=':', linewidth=2,\n",
    "            label=f'Post. mean = {p_mean:.3f}')\n",
    "plt.xlabel('p', fontsize=12)\n",
    "plt.ylabel('Posterior Density', fontsize=12)\n",
    "plt.title('Comparison of Point Estimates', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('point_estimates_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparing Point Estimates: MLE, MAP, Posterior Mean\n",
    "======================================================================\n",
    "\n",
    "Data: 15 heads in 20 flips\n",
    "True p: 0.7\n",
    "\n",
    "Prior: Beta(2, 2)\n",
    "Posterior: Beta(17, 7)\n",
    "\n",
    "Point Estimates:\n",
    "  MLE (no prior):      0.7500\n",
    "  MAP (mode):          0.7273\n",
    "  Posterior mean:      0.7083\n",
    "  Posterior median:    0.7142\n",
    "  True p:              0.7000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_d8b38fc4e640.png)\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Conjugate Priors\n",
    "\n",
    "| Distribution | Conjugate Prior | Update Rule |\n",
    "|--------------|-----------------|-------------|\n",
    "| Bernoulli/Binomial | Beta(α, β) | α → α+k, β → β+n-k |\n",
    "| Poisson | Gamma(α, β) | α → α+Σx, β → β+n |\n",
    "| Normal (μ unknown) | Normal(μ₀, σ₀²) | Precision adds |\n",
    "\n",
    "### Point Estimates\n",
    "\n",
    "**MLE**: \\(\\arg\\max p(\\text{data} \\mid \\theta)\\)\n",
    "\n",
    "**MAP**: \\(\\arg\\max p(\\theta \\mid \\text{data})\\) = MLE + prior penalty\n",
    "\n",
    "**Posterior Mean**: \\(\\mathbb{E}[\\theta \\mid \\text{data}]\\) = Optimal under squared loss\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "✅ Conjugacy → closed-form posteriors\n",
    "✅ MAP = regularized MLE\n",
    "✅ Hyperparameters = \"pseudo-data\"\n",
    "✅ As n → ∞, all estimates converge\n",
    "✅ Prior matters most with small data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}