{
 "cells": [
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "# 7.1 Significance and P-Values\n",
    "\n",
    "## The Logic of Hypothesis Testing\n",
    "\n",
    "### The Setup\n",
    "\n",
    "We have:\n",
    "1. **Data**: Observations from an experiment or sample\n",
    "2. **Question**: Is there a real effect, or just random variation?\n",
    "\n",
    "### The Framework\n",
    "\n",
    "**Null Hypothesis (H₀)**: The \"nothing is happening\" hypothesis\n",
    "- Examples: \"The drug has no effect\", \"The coin is fair\", \"Both designs convert equally\"\n",
    "\n",
    "**Alternative Hypothesis (H₁ or Hₐ)**: What we're trying to establish\n",
    "- Examples: \"The drug reduces blood pressure\", \"The coin is biased\", \"Design B converts better\"\n",
    "\n",
    "### The Logic\n",
    "\n",
    "1. **Assume H₀ is true**\n",
    "2. **Calculate**: How likely is our observed data (or more extreme) under H₀?\n",
    "3. **Decide**: If very unlikely, reject H₀ in favor of H₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "┌─────────────────────────────────────────────┐\n",
    "│  If H₀ is true                              │\n",
    "│  ↓                                          │\n",
    "│  How probable is our observed result?       │\n",
    "│  ↓                                          │\n",
    "│  If very improbable → Reject H₀             │\n",
    "│  If reasonably probable → Don't reject H₀   │\n",
    "└─────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "## P-Values: The Core Concept\n",
    "\n",
    "### Definition\n",
    "\n",
    "**P-value**: The probability of observing data as extreme as (or more extreme than) what we actually observed, **assuming H₀ is true**.\n",
    "\n",
    "$$\n",
    "\\text{p-value} = P(\\text{data as extreme or more} \\mid H_0 \\text{ is true})\n",
    "$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Small p-value** (typically < 0.05): Data is unlikely under H₀ → Evidence against H₀\n",
    "- **Large p-value**: Data is consistent with H₀ → Insufficient evidence against H₀\n",
    "\n",
    "### Common Misconception\n",
    "\n",
    "❌ **WRONG**: \"p-value is the probability that H₀ is true\"\n",
    "✅ **CORRECT**: \"p-value is the probability of our data (or more extreme) if H₀ were true\"\n",
    "\n",
    "## Significance Levels\n",
    "\n",
    "### The α Threshold\n",
    "\n",
    "**Significance level (α)**: The threshold below which we reject H₀.\n",
    "\n",
    "Common choices:\n",
    "- α = 0.05 (5%) - standard in many fields\n",
    "- α = 0.01 (1%) - more conservative\n",
    "- α = 0.10 (10%) - more liberal\n",
    "\n",
    "### Decision Rule\n",
    "\n",
    "- If **p-value < α**: Reject H₀ (result is \"statistically significant\")\n",
    "- If **p-value ≥ α**: Fail to reject H₀ (result is \"not significant\")\n",
    "\n",
    "## Types of Errors\n",
    "\n",
    "### The Truth Table\n",
    "\n",
    "|                | H₀ is True | H₀ is False |\n",
    "|----------------|------------|-------------|\n",
    "| **Reject H₀**  | Type I Error (α) | ✓ Correct |\n",
    "| **Don't Reject**| ✓ Correct | Type II Error (β) |\n",
    "\n",
    "### Type I Error (False Positive)\n",
    "\n",
    "- **Definition**: Rejecting H₀ when it's actually true\n",
    "- **Probability**: α (the significance level)\n",
    "- **Example**: Concluding a drug works when it doesn't\n",
    "\n",
    "### Type II Error (False Negative)\n",
    "\n",
    "- **Definition**: Failing to reject H₀ when it's actually false\n",
    "- **Probability**: β (depends on effect size, sample size, α)\n",
    "- **Example**: Missing a real drug effect\n",
    "\n",
    "### Power\n",
    "\n",
    "**Statistical Power** = 1 - β = Probability of correctly rejecting false H₀\n",
    "\n",
    "- High power (0.80 or more) is desirable\n",
    "- Power increases with: larger effects, larger samples, higher α\n",
    "\n",
    "## Example 1: Coin Fairness Test\n",
    "\n",
    "### Problem\n",
    "\n",
    "You flip a coin 100 times and get 60 heads. Is the coin fair?\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "n = 100\n",
    "observed_heads = 60\n",
    "\n",
    "# Hypotheses\n",
    "print(\"H₀: The coin is fair (p = 0.5)\")\n",
    "print(\"H₁: The coin is not fair (p ≠ 0.5)\")\n",
    "print()\n",
    "\n",
    "# Under H₀, number of heads ~ Binomial(n=100, p=0.5)\n",
    "# For large n, approximate with normal distribution\n",
    "p_null = 0.5\n",
    "mean_null = n * p_null\n",
    "std_null = np.sqrt(n * p_null * (1 - p_null))\n",
    "\n",
    "print(f\"Under H₀: Number of heads ~ N({mean_null}, {std_null:.2f}²)\")\n",
    "print()\n",
    "\n",
    "# Z-score of observed result\n",
    "z_score = (observed_heads - mean_null) / std_null\n",
    "print(f\"Observed: {observed_heads} heads\")\n",
    "print(f\"Z-score: {z_score:.3f}\")\n",
    "print()\n",
    "\n",
    "# P-value (two-tailed test)\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "print(f\"P-value (two-tailed): {p_value:.4f}\")\n",
    "print()\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(f\"Decision: p-value ({p_value:.4f}) < α ({alpha})\")\n",
    "    print(\"Reject H₀. Evidence suggests the coin is biased.\")\n",
    "else:\n",
    "    print(f\"Decision: p-value ({p_value:.4f}) ≥ α ({alpha})\")\n",
    "    print(\"Fail to reject H₀. Insufficient evidence of bias.\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Binomial distribution under H₀\n",
    "x = np.arange(0, 101)\n",
    "binom_probs = stats.binom.pmf(x, n, p_null)\n",
    "\n",
    "ax1.bar(x, binom_probs, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "ax1.axvline(observed_heads, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Observed = {observed_heads}')\n",
    "ax1.axvline(mean_null, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Expected = {mean_null}')\n",
    "ax1.set_xlabel('Number of Heads', fontsize=11)\n",
    "ax1.set_ylabel('Probability', fontsize=11)\n",
    "ax1.set_title('Binomial Distribution under H₀ (p=0.5)', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Standard normal showing p-value\n",
    "z_range = np.linspace(-4, 4, 1000)\n",
    "norm_pdf = stats.norm.pdf(z_range)\n",
    "\n",
    "ax2.plot(z_range, norm_pdf, 'b-', linewidth=2)\n",
    "ax2.fill_between(z_range[z_range <= -abs(z_score)], 0, \n",
    "                  stats.norm.pdf(z_range[z_range <= -abs(z_score)]),\n",
    "                  alpha=0.3, color='red', label='p-value region')\n",
    "ax2.fill_between(z_range[z_range >= abs(z_score)], 0,\n",
    "                  stats.norm.pdf(z_range[z_range >= abs(z_score)]),\n",
    "                  alpha=0.3, color='red')\n",
    "ax2.axvline(z_score, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Observed z = {z_score:.2f}')\n",
    "ax2.axvline(-z_score, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Z-score', fontsize=11)\n",
    "ax2.set_ylabel('Density', fontsize=11)\n",
    "ax2.set_title(f'Standard Normal (p-value = {p_value:.4f})', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('coin_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H₀: The coin is fair (p = 0.5)\n",
    "H₁: The coin is not fair (p ≠ 0.5)\n",
    "\n",
    "Under H₀: Number of heads ~ N(50.0, 5.00²)\n",
    "\n",
    "Observed: 60 heads\n",
    "Z-score: 2.000\n",
    "\n",
    "P-value (two-tailed): 0.0455\n",
    "\n",
    "Decision: p-value (0.0455) < α (0.05)\n",
    "Reject H₀. Evidence suggests the coin is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_1f236d962352.png)\n",
    "\n",
    "\n",
    "**Output**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H₀: The coin is fair (p = 0.5)\n",
    "H₁: The coin is not fair (p ≠ 0.5)\n",
    "\n",
    "Under H₀: Number of heads ~ N(50.0, 5.00²)\n",
    "\n",
    "Observed: 60 heads\n",
    "Z-score: 2.000\n",
    "\n",
    "P-value (two-tailed): 0.0455\n",
    "\n",
    "Decision: p-value (0.0455) < α (0.05)\n",
    "Reject H₀. Evidence suggests the coin is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "## One-Tailed vs Two-Tailed Tests\n",
    "\n",
    "### Two-Tailed Test\n",
    "\n",
    "**H₁**: Parameter ≠ value (different in either direction)\n",
    "\n",
    "Example: \"The coin is biased\" (could be biased toward heads OR tails)\n",
    "\n",
    "P-value includes both tails of the distribution.\n",
    "\n",
    "### One-Tailed Test\n",
    "\n",
    "**H₁**: Parameter > value OR Parameter < value (specific direction)\n",
    "\n",
    "Example: \"The coin is biased toward heads\" (only testing one direction)\n",
    "\n",
    "P-value includes only one tail.\n",
    "\n",
    "### Choosing\n",
    "\n",
    "- **Two-tailed**: Default choice, more conservative\n",
    "- **One-tailed**: Use only when you have a strong directional hypothesis **before** seeing data\n",
    "\n",
    "⚠️ **Warning**: Don't switch to one-tailed after seeing your data - this is p-hacking!\n",
    "\n",
    "## Example 2: One-Sample Mean Test\n",
    "\n",
    "### Problem\n",
    "\n",
    "A manufacturer claims light bulbs last 1000 hours on average. You test 25 bulbs: mean = 950 hours, std = 100 hours. Test the claim at α = 0.05.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data\n",
    "n = 25\n",
    "sample_mean = 950\n",
    "sample_std = 100\n",
    "claim_mean = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"Hypothesis Test: One-Sample t-test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"H₀: μ = {claim_mean} (manufacturer's claim is true)\")\n",
    "print(f\"H₁: μ ≠ {claim_mean} (manufacturer's claim is false)\")\n",
    "print(f\"Significance level: α = {alpha}\")\n",
    "print()\n",
    "\n",
    "# Test statistic\n",
    "se = sample_std / np.sqrt(n)\n",
    "t_stat = (sample_mean - claim_mean) / se\n",
    "df = n - 1\n",
    "\n",
    "print(f\"Sample mean: {sample_mean}\")\n",
    "print(f\"Sample std: {sample_std}\")\n",
    "print(f\"Standard error: {se:.2f}\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"Degrees of freedom: {df}\")\n",
    "print()\n",
    "\n",
    "# P-value (two-tailed)\n",
    "p_value = 2 * stats.t.cdf(t_stat, df)  # t_stat is negative\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print()\n",
    "\n",
    "# Decision\n",
    "if p_value < alpha:\n",
    "    print(f\"Decision: p-value ({p_value:.4f}) < α ({alpha})\")\n",
    "    print(\"Reject H₀. The manufacturer's claim is questionable.\")\n",
    "else:\n",
    "    print(f\"Decision: p-value ({p_value:.4f}) ≥ α ({alpha})\")\n",
    "    print(\"Fail to reject H₀. Insufficient evidence against the claim.\")\n",
    "\n",
    "# Using scipy's built-in function\n",
    "t_stat_scipy, p_value_scipy = stats.ttest_1samp(\n",
    "    np.random.normal(sample_mean, sample_std, n), claim_mean\n",
    ")\n",
    "print(f\"\\nVerification using scipy.stats.ttest_1samp:\")\n",
    "print(f\"t-statistic: {t_stat:.3f}, p-value: {p_value:.4f}\")\n",
    "\n",
    "# Confidence interval\n",
    "t_crit = stats.t.ppf(1 - alpha/2, df)\n",
    "ci_lower = sample_mean - t_crit * se\n",
    "ci_upper = sample_mean + t_crit * se\n",
    "\n",
    "print(f\"\\n95% Confidence Interval: [{ci_lower:.1f}, {ci_upper:.1f}]\")\n",
    "print(f\"Claimed value {claim_mean} is {'NOT' if claim_mean < ci_lower or claim_mean > ci_upper else ''} in the CI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hypothesis Test: One-Sample t-test\n",
    "==================================================\n",
    "H₀: μ = 1000 (manufacturer's claim is true)\n",
    "H₁: μ ≠ 1000 (manufacturer's claim is false)\n",
    "Significance level: α = 0.05\n",
    "\n",
    "Sample mean: 950\n",
    "Sample std: 100\n",
    "Standard error: 20.00\n",
    "t-statistic: -2.500\n",
    "Degrees of freedom: 24\n",
    "\n",
    "P-value: 0.0197\n",
    "\n",
    "Decision: p-value (0.0197) < α (0.05)\n",
    "Reject H₀. The manufacturer's claim is questionable.\n",
    "\n",
    "Verification using scipy.stats.ttest_1samp:\n",
    "t-statistic: -2.500, p-value: 0.0197\n",
    "\n",
    "95% Confidence Interval: [908.7, 991.3]\n",
    "Claimed value 1000 is NOT in the CI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hypothesis Test: One-Sample t-test\n",
    "==================================================\n",
    "H₀: μ = 1000 (manufacturer's claim is true)\n",
    "H₁: μ ≠ 1000 (manufacturer's claim is false)\n",
    "Significance level: α = 0.05\n",
    "\n",
    "Sample mean: 950\n",
    "Sample std: 100\n",
    "Standard error: 20.00\n",
    "t-statistic: -2.500\n",
    "Degrees of freedom: 24\n",
    "\n",
    "P-value: 0.0197\n",
    "\n",
    "Decision: p-value (0.0197) < α (0.05)\n",
    "Reject H₀. The manufacturer's claim is questionable.\n",
    "\n",
    "95% Confidence Interval: [908.6, 991.4]\n",
    "Claimed value 1000 is NOT in the CI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "## The Relationship Between CIs and Hypothesis Tests\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "For a two-tailed test at significance level α:\n",
    "\n",
    "> **Reject H₀: μ = μ₀** ⟺ **μ₀ is outside the (1-α)×100% confidence interval**\n",
    "\n",
    "This provides an alternative way to test hypotheses using confidence intervals!\n",
    "\n",
    "## Power Analysis\n",
    "\n",
    "### What is Power?\n",
    "\n",
    "Power = Probability of detecting an effect when it exists\n",
    "\n",
    "### Factors Affecting Power\n",
    "\n",
    "1. **Effect size**: Larger effects → higher power\n",
    "2. **Sample size**: Larger n → higher power  \n",
    "3. **Significance level**: Larger α → higher power (but more Type I errors)\n",
    "4. **Variance**: Smaller σ → higher power\n",
    "\n",
    "### Example: Power Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_power(n, effect_size, alpha=0.05, sigma=1):\n",
    "    \"\"\"\n",
    "    Calculate power for one-sample t-test.\n",
    "    \n",
    "    effect_size: difference between true mean and null mean (in units of data)\n",
    "    \"\"\"\n",
    "    # Non-centrality parameter\n",
    "    ncp = effect_size / (sigma / np.sqrt(n))\n",
    "    \n",
    "    # Critical value\n",
    "    df = n - 1\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df)\n",
    "    \n",
    "    # Power (using non-central t-distribution)\n",
    "    power = 1 - stats.nct.cdf(t_crit, df, ncp) + stats.nct.cdf(-t_crit, df, ncp)\n",
    "    \n",
    "    return power\n",
    "\n",
    "# Example: Detect mean difference of 0.5σ\n",
    "effect_sizes = [0.2, 0.5, 0.8]  # Small, medium, large (Cohen's d)\n",
    "sample_sizes = np.arange(10, 201, 10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for es in effect_sizes:\n",
    "    powers = [calculate_power(n, es) for n in sample_sizes]\n",
    "    ax.plot(sample_sizes, powers, linewidth=2, marker='o', \n",
    "            label=f'Effect size = {es} ({\"\n",
    "small\" if es==0.2 else \"medium\" if es==0.5 else \"large\"})')\n",
    "\n",
    "ax.axhline(0.8, color='red', linestyle='--', linewidth=2, \n",
    "           label='Desired power = 0.80')\n",
    "ax.set_xlabel('Sample Size (n)', fontsize=12)\n",
    "ax.set_ylabel('Power', fontsize=12)\n",
    "ax.set_title('Statistical Power vs. Sample Size\\n(α = 0.05, two-tailed test)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('power_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample Size Needed for 80% Power:\")\n",
    "print(\"=\"*50)\n",
    "for es in effect_sizes:\n",
    "    for n in sample_sizes:\n",
    "        if calculate_power(n, es) >= 0.8:\n",
    "            print(f\"Effect size {es}: n ≥ {n}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error: SyntaxError: unterminated string literal (detected at line 32) (<string>, line 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### The Hypothesis Testing Recipe\n",
    "\n",
    "1. **State hypotheses**: H₀ and H₁\n",
    "2. **Choose α**: Significance level (usually 0.05)\n",
    "3. **Collect data**: Sample and compute test statistic\n",
    "4. **Calculate p-value**: Probability under H₀\n",
    "5. **Make decision**: Reject H₀ if p-value < α\n",
    "6. **Interpret**: In context of the problem\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**One-sample t-test**:\n",
    "$$\n",
    "t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1}\n",
    "$$\n",
    "\n",
    "**Z-test** (σ known):\n",
    "$$\n",
    "z = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n",
    "$$\n",
    "\n",
    "### Important Points\n",
    "\n",
    "✅ p-value measures evidence against H₀  \n",
    "✅ Small p-value → reject H₀  \n",
    "✅ \"Not significant\" ≠ \"no effect\"  \n",
    "✅ Statistical significance ≠ practical importance  \n",
    "✅ Always report effect sizes, not just p-values  \n",
    "\n",
    "❌ p-value ≠ P(H₀ is true)  \n",
    "❌ Don't change hypotheses after seeing data  \n",
    "❌ Don't confuse significance with importance  \n",
    "❌ Don't p-hack (we'll discuss this later!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}