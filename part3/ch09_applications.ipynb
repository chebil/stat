{
 "cells": [
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "# 9.5 Applications of Parameter Estimation\n",
    "\n",
    "## Application 1: A/B Testing with Bayesian Inference\n",
    "\n",
    "### Problem\n",
    "\n",
    "You're a data scientist at a company testing two website designs:\n",
    "- **Design A** (control): Current design\n",
    "- **Design B** (treatment): New design\n",
    "\n",
    "**Goal**: Which design has a higher click-through rate (CTR)?\n",
    "\n",
    "### Frequentist Approach (Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate data\n",
    "n_A = 1000  # Visitors to A\n",
    "n_B = 1000  # Visitors to B\n",
    "\n",
    "# True CTRs (unknown in practice)\n",
    "true_p_A = 0.10\n",
    "true_p_B = 0.12\n",
    "\n",
    "# Generate clicks\n",
    "clicks_A = np.random.binomial(n_A, true_p_A)\n",
    "clicks_B = np.random.binomial(n_B, true_p_B)\n",
    "\n",
    "print(\"A/B Testing: Frequentist vs. Bayesian\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Design A: {clicks_A}/{n_A} clicks (CTR = {clicks_A/n_A:.3f})\")\n",
    "print(f\"Design B: {clicks_B}/{n_B} clicks (CTR = {clicks_B/n_B:.3f})\")\n",
    "print()\n",
    "\n",
    "# Frequentist test: Two-proportion z-test\n",
    "p_A_hat = clicks_A / n_A\n",
    "p_B_hat = clicks_B / n_B\n",
    "p_pooled = (clicks_A + clicks_B) / (n_A + n_B)\n",
    "\n",
    "se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_A + 1/n_B))\n",
    "z = (p_B_hat - p_A_hat) / se\n",
    "p_value = 1 - stats.norm.cdf(z)  # One-sided\n",
    "\n",
    "print(\"FREQUENTIST APPROACH\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"p̂_A = {p_A_hat:.4f}\")\n",
    "print(f\"p̂_B = {p_B_hat:.4f}\")\n",
    "print(f\"Difference: {p_B_hat - p_A_hat:.4f}\")\n",
    "print(f\"Z-statistic: {z:.3f}\")\n",
    "print(f\"P-value (one-sided): {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Decision: Reject H₀, B is better than A (p < 0.05)\")\n",
    "else:\n",
    "    print(\"Decision: Insufficient evidence that B is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A/B Testing: Frequentist vs. Bayesian\n",
    "======================================================================\n",
    "Design A: 96/1000 clicks (CTR = 0.096)\n",
    "Design B: 122/1000 clicks (CTR = 0.122)\n",
    "\n",
    "FREQUENTIST APPROACH\n",
    "----------------------------------------------------------------------\n",
    "p̂_A = 0.0960\n",
    "p̂_B = 0.1220\n",
    "Difference: 0.0260\n",
    "Z-statistic: 1.866\n",
    "P-value (one-sided): 0.0311\n",
    "Decision: Reject H₀, B is better than A (p < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "### Bayesian Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bayesian A/B test\n",
    "# Prior: Uniform (Beta(1,1)) for both\n",
    "alpha_A_prior = 1\n",
    "beta_A_prior = 1\n",
    "alpha_B_prior = 1\n",
    "beta_B_prior = 1\n",
    "\n",
    "# Posterior: Beta(alpha + clicks, beta + (n - clicks))\n",
    "alpha_A_post = alpha_A_prior + clicks_A\n",
    "beta_A_post = beta_A_prior + (n_A - clicks_A)\n",
    "\n",
    "alpha_B_post = alpha_B_prior + clicks_B\n",
    "beta_B_post = beta_B_prior + (n_B - clicks_B)\n",
    "\n",
    "print(\"\\nBAYESIAN APPROACH\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Posterior A: Beta({alpha_A_post}, {beta_A_post})\")\n",
    "print(f\"  Posterior mean: {alpha_A_post/(alpha_A_post+beta_A_post):.4f}\")\n",
    "print(f\"\\nPosterior B: Beta({alpha_B_post}, {beta_B_post})\")\n",
    "print(f\"  Posterior mean: {alpha_B_post/(alpha_B_post+beta_B_post):.4f}\")\n",
    "\n",
    "# Monte Carlo: P(p_B > p_A)\n",
    "n_samples = 100000\n",
    "p_A_samples = np.random.beta(alpha_A_post, beta_A_post, n_samples)\n",
    "p_B_samples = np.random.beta(alpha_B_post, beta_B_post, n_samples)\n",
    "\n",
    "prob_B_better = np.mean(p_B_samples > p_A_samples)\n",
    "\n",
    "print(f\"\\nP(p_B > p_A | data) = {prob_B_better:.4f}\")\n",
    "print(f\"\\nInterpretation: There is a {prob_B_better*100:.1f}% probability that B is better than A\")\n",
    "\n",
    "# Expected lift\n",
    "expected_lift = np.mean((p_B_samples - p_A_samples) / p_A_samples) * 100\n",
    "print(f\"Expected lift: {expected_lift:.1f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Posterior distributions\n",
    "p_values = np.linspace(0, 0.20, 1000)\n",
    "posterior_A = stats.beta.pdf(p_values, alpha_A_post, beta_A_post)\n",
    "posterior_B = stats.beta.pdf(p_values, alpha_B_post, beta_B_post)\n",
    "\n",
    "ax1.plot(p_values, posterior_A, 'b-', linewidth=2, label='Design A')\n",
    "ax1.plot(p_values, posterior_B, 'r-', linewidth=2, label='Design B')\n",
    "ax1.fill_between(p_values, 0, posterior_A, alpha=0.3, color='blue')\n",
    "ax1.fill_between(p_values, 0, posterior_B, alpha=0.3, color='red')\n",
    "ax1.axvline(true_p_A, color='blue', linestyle='--', alpha=0.7, label=f'True p_A={true_p_A}')\n",
    "ax1.axvline(true_p_B, color='red', linestyle='--', alpha=0.7, label=f'True p_B={true_p_B}')\n",
    "ax1.set_xlabel('Click-Through Rate', fontsize=11)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Posterior Distributions', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Distribution of difference\n",
    "diff_samples = p_B_samples - p_A_samples\n",
    "ax2.hist(diff_samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='No difference')\n",
    "ax2.axvline(np.mean(diff_samples), color='green', linestyle='-', linewidth=2,\n",
    "           label=f'Mean diff = {np.mean(diff_samples):.4f}')\n",
    "ax2.set_xlabel('Difference (p_B - p_A)', fontsize=11)\n",
    "ax2.set_ylabel('Density', fontsize=11)\n",
    "ax2.set_title('Posterior of Difference', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Probability B > A\n",
    "categories = ['A is better', 'B is better']\n",
    "probs = [1 - prob_B_better, prob_B_better]\n",
    "colors = ['#ff6b6b', '#51cf66']\n",
    "\n",
    "ax3.bar(categories, probs, color=colors, edgecolor='black', linewidth=2)\n",
    "for i, (cat, prob) in enumerate(zip(categories, probs)):\n",
    "    ax3.text(i, prob/2, f'{prob*100:.1f}%', ha='center', va='center',\n",
    "            fontsize=16, fontweight='bold', color='white')\n",
    "ax3.set_ylabel('Probability', fontsize=11)\n",
    "ax3.set_title('Decision', fontsize=12)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ab_testing_bayesian.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRECOMMENDATION:\")\n",
    "if prob_B_better > 0.95:\n",
    "    print(f\"  ✅ Deploy Design B (>95% certain it's better)\")\n",
    "elif prob_B_better > 0.90:\n",
    "    print(f\"  ⚠️ Consider deploying B (>90% certain)\")\n",
    "else:\n",
    "    print(f\"  ❌ Keep testing or stick with A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAYESIAN APPROACH\n",
    "----------------------------------------------------------------------\n",
    "Posterior A: Beta(97, 905)\n",
    "  Posterior mean: 0.0968\n",
    "\n",
    "Posterior B: Beta(123, 879)\n",
    "  Posterior mean: 0.1228\n",
    "\n",
    "P(p_B > p_A | data) = 0.9684\n",
    "\n",
    "Interpretation: There is a 96.8% probability that B is better than A\n",
    "Expected lift: 28.0%\n",
    "\n",
    "RECOMMENDATION:\n",
    "  ✅ Deploy Design B (>95% certain it's better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_bbc7b528f897.png)\n",
    "\n",
    "\n",
    "## Application 2: Estimating Coin Fairness\n",
    "\n",
    "### Problem\n",
    "\n",
    "You suspect a coin might be biased. How many flips do you need to be confident?\n",
    "\n",
    "### Sequential Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True coin bias (unknown to experimenter)\n",
    "true_p = 0.6  # Biased toward heads\n",
    "\n",
    "# Sequential experiment\n",
    "max_flips = 200\n",
    "flips = np.random.binomial(1, true_p, max_flips)\n",
    "\n",
    "# Track posterior evolution\n",
    "alpha = 1  # Start with uniform prior\n",
    "beta = 1\n",
    "\n",
    "posteriors = []\n",
    "ci_lowers = []\n",
    "ci_uppers = []\n",
    "means = []\n",
    "\n",
    "for n in range(1, max_flips + 1):\n",
    "    # Update posterior\n",
    "    if flips[n-1] == 1:  # Heads\n",
    "        alpha += 1\n",
    "    else:  # Tails\n",
    "        beta += 1\n",
    "    \n",
    "    # Store statistics\n",
    "    mean = alpha / (alpha + beta)\n",
    "    ci_lower = stats.beta.ppf(0.025, alpha, beta)\n",
    "    ci_upper = stats.beta.ppf(0.975, alpha, beta)\n",
    "    \n",
    "    means.append(mean)\n",
    "    ci_lowers.append(ci_lower)\n",
    "    ci_uppers.append(ci_upper)\n",
    "    \n",
    "    # Check if we're confident it's NOT fair (p ≠ 0.5)\n",
    "    prob_fair = stats.beta.cdf(0.55, alpha, beta) - stats.beta.cdf(0.45, alpha, beta)\n",
    "    \n",
    "    if n in [10, 50, 100, 200] or (prob_fair < 0.05 and n > 10):\n",
    "        print(f\"After {n:3d} flips: p̂={mean:.3f}, 95% CI=[{ci_lower:.3f}, {ci_upper:.3f}], P(fair)={prob_fair:.3f}\")\n",
    "\n",
    "print(\"\\nSequential Bayesian Testing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Posterior mean with credible interval\n",
    "n_range = range(1, max_flips + 1)\n",
    "ax1.plot(n_range, means, 'b-', linewidth=2, label='Posterior mean')\n",
    "ax1.fill_between(n_range, ci_lowers, ci_uppers, alpha=0.3, color='blue',\n",
    "                  label='95% Credible Interval')\n",
    "ax1.axhline(0.5, color='gray', linestyle='--', linewidth=2, label='Fair coin (p=0.5)')\n",
    "ax1.axhline(true_p, color='red', linestyle='--', linewidth=2, label=f'True p={true_p}')\n",
    "ax1.set_xlabel('Number of Flips', fontsize=11)\n",
    "ax1.set_ylabel('Estimated p (probability of heads)', fontsize=11)\n",
    "ax1.set_title('Sequential Bayesian Estimation', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_ylim(0.3, 0.8)\n",
    "\n",
    "# Posterior distributions at key points\n",
    "p_vals = np.linspace(0, 1, 1000)\n",
    "for n in [10, 50, 100, 200]:\n",
    "    cumsum_heads = np.sum(flips[:n])\n",
    "    a = 1 + cumsum_heads\n",
    "    b = 1 + (n - cumsum_heads)\n",
    "    posterior = stats.beta.pdf(p_vals, a, b)\n",
    "    ax2.plot(p_vals, posterior, linewidth=2, label=f'n={n}')\n",
    "\n",
    "ax2.axvline(0.5, color='gray', linestyle='--', linewidth=2, label='Fair coin')\n",
    "ax2.axvline(true_p, color='red', linestyle='--', linewidth=2, label=f'True p={true_p}')\n",
    "ax2.set_xlabel('p (probability of heads)', fontsize=11)\n",
    "ax2.set_ylabel('Density', fontsize=11)\n",
    "ax2.set_title('Evolution of Posterior Distribution', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequential_coin_testing.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After  10 flips: p̂=0.500, 95% CI=[0.234, 0.766], P(fair)=0.266\n",
    "After  30 flips: p̂=0.688, 95% CI=[0.520, 0.833], P(fair)=0.049\n",
    "After  33 flips: p̂=0.686, 95% CI=[0.525, 0.826], P(fair)=0.045\n",
    "After  49 flips: p̂=0.667, 95% CI=[0.533, 0.788], P(fair)=0.042\n",
    "After  50 flips: p̂=0.673, 95% CI=[0.541, 0.792], P(fair)=0.033\n",
    "After  51 flips: p̂=0.660, 95% CI=[0.529, 0.780], P(fair)=0.048\n",
    "After  61 flips: p̂=0.651, 95% CI=[0.530, 0.763], P(fair)=0.049\n",
    "After  62 flips: p̂=0.656, 95% CI=[0.537, 0.767], P(fair)=0.040\n",
    "After  64 flips: p̂=0.652, 95% CI=[0.534, 0.761], P(fair)=0.045\n",
    "After  65 flips: p̂=0.657, 95% CI=[0.540, 0.765], P(fair)=0.036\n",
    "After  66 flips: p̂=0.662, 95% CI=[0.546, 0.768], P(fair)=0.029\n",
    "After  67 flips: p̂=0.667, 95% CI=[0.552, 0.772], P(fair)=0.023\n",
    "After  68 flips: p̂=0.657, 95% CI=[0.543, 0.763], P(fair)=0.032\n",
    "After  69 flips: p̂=0.662, 95% CI=[0.549, 0.767], P(fair)=0.026\n",
    "After  70 flips: p̂=0.653, 95% CI=[0.540, 0.758], P(fair)=0.036\n",
    "After  71 flips: p̂=0.644, 95% CI=[0.531, 0.749], P(fair)=0.050\n",
    "After  72 flips: p̂=0.649, 95% CI=[0.537, 0.753], P(fair)=0.041\n",
    "After  73 flips: p̂=0.653, 95% CI=[0.543, 0.756], P(fair)=0.033\n",
    "After  74 flips: p̂=0.645, 95% CI=[0.535, 0.748], P(fair)=0.045\n",
    "After  86 flips: p̂=0.636, 95% CI=[0.534, 0.733], P(fair)=0.048\n",
    "After 100 flips: p̂=0.627, 95% CI=[0.532, 0.718], P(fair)=0.055\n",
    "After 101 flips: p̂=0.631, 95% CI=[0.536, 0.721], P(fair)=0.046\n",
    "After 103 flips: p̂=0.629, 95% CI=[0.534, 0.718], P(fair)=0.050\n",
    "After 104 flips: p̂=0.632, 95% CI=[0.539, 0.721], P(fair)=0.042\n",
    "After 106 flips: p̂=0.630, 95% CI=[0.537, 0.718], P(fair)=0.045\n",
    "After 107 flips: p̂=0.633, 95% CI=[0.541, 0.721], P(fair)=0.038\n",
    "After 108 flips: p̂=0.627, 95% CI=[0.535, 0.715], P(fair)=0.049\n",
    "After 109 flips: p̂=0.631, 95% CI=[0.539, 0.718], P(fair)=0.041\n",
    "After 110 flips: p̂=0.634, 95% CI=[0.543, 0.720], P(fair)=0.035\n",
    "After 111 flips: p̂=0.637, 95% CI=[0.547, 0.723], P(fair)=0.029\n",
    "After 112 flips: p̂=0.640, 95% CI=[0.550, 0.726], P(fair)=0.024\n",
    "After 113 flips: p̂=0.635, 95% CI=[0.545, 0.720], P(fair)=0.032\n",
    "After 114 flips: p̂=0.629, 95% CI=[0.540, 0.715], P(fair)=0.041\n",
    "After 133 flips: p̂=0.622, 95% CI=[0.539, 0.702], P(fair)=0.044\n",
    "After 134 flips: p̂=0.625, 95% CI=[0.542, 0.704], P(fair)=0.037\n",
    "After 135 flips: p̂=0.620, 95% CI=[0.538, 0.700], P(fair)=0.047\n",
    "After 136 flips: p̂=0.623, 95% CI=[0.541, 0.702], P(fair)=0.040\n",
    "After 137 flips: p̂=0.626, 95% CI=[0.544, 0.704], P(fair)=0.034\n",
    "After 138 flips: p̂=0.621, 95% CI=[0.540, 0.700], P(fair)=0.043\n",
    "After 139 flips: p̂=0.624, 95% CI=[0.543, 0.702], P(fair)=0.037\n",
    "After 140 flips: p̂=0.620, 95% CI=[0.539, 0.698], P(fair)=0.045\n",
    "After 142 flips: p̂=0.618, 95% CI=[0.538, 0.695], P(fair)=0.048\n",
    "After 143 flips: p̂=0.621, 95% CI=[0.541, 0.698], P(fair)=0.042\n",
    "After 144 flips: p̂=0.623, 95% CI=[0.543, 0.700], P(fair)=0.036\n",
    "After 145 flips: p̂=0.626, 95% CI=[0.546, 0.702], P(fair)=0.031\n",
    "After 146 flips: p̂=0.628, 95% CI=[0.549, 0.704], P(fair)=0.026\n",
    "After 147 flips: p̂=0.624, 95% CI=[0.545, 0.700], P(fair)=0.033\n",
    "After 148 flips: p̂=0.627, 95% CI=[0.548, 0.702], P(fair)=0.028\n",
    "After 149 flips: p̂=0.629, 95% CI=[0.551, 0.704], P(fair)=0.024\n",
    "After 150 flips: p̂=0.632, 95% CI=[0.554, 0.706], P(fair)=0.020\n",
    "After 151 flips: p̂=0.627, 95% CI=[0.550, 0.702], P(fair)=0.026\n",
    "After 152 flips: p̂=0.630, 95% CI=[0.552, 0.704], P(fair)=0.022\n",
    "After 153 flips: p̂=0.632, 95% CI=[0.555, 0.706], P(fair)=0.018\n",
    "After 154 flips: p̂=0.635, 95% CI=[0.558, 0.708], P(fair)=0.016\n",
    "After 155 flips: p̂=0.631, 95% CI=[0.554, 0.704], P(fair)=0.020\n",
    "After 156 flips: p̂=0.633, 95% CI=[0.557, 0.706], P(fair)=0.017\n",
    "After 157 flips: p̂=0.629, 95% CI=[0.553, 0.702], P(fair)=0.021\n",
    "After 158 flips: p̂=0.625, 95% CI=[0.549, 0.698], P(fair)=0.027\n",
    "After 159 flips: p̂=0.627, 95% CI=[0.551, 0.700], P(fair)=0.023\n",
    "After 160 flips: p̂=0.623, 95% CI=[0.548, 0.696], P(fair)=0.029\n",
    "After 161 flips: p̂=0.626, 95% CI=[0.550, 0.698], P(fair)=0.024\n",
    "After 162 flips: p̂=0.622, 95% CI=[0.547, 0.694], P(fair)=0.030\n",
    "After 163 flips: p̂=0.618, 95% CI=[0.543, 0.691], P(fair)=0.037\n",
    "After 164 flips: p̂=0.620, 95% CI=[0.546, 0.693], P(fair)=0.032\n",
    "After 165 flips: p̂=0.623, 95% CI=[0.548, 0.695], P(fair)=0.028\n",
    "After 166 flips: p̂=0.619, 95% CI=[0.545, 0.691], P(fair)=0.034\n",
    "After 167 flips: p̂=0.621, 95% CI=[0.547, 0.693], P(fair)=0.030\n",
    "After 168 flips: p̂=0.624, 95% CI=[0.550, 0.695], P(fair)=0.026\n",
    "After 169 flips: p̂=0.626, 95% CI=[0.552, 0.697], P(fair)=0.022\n",
    "After 170 flips: p̂=0.628, 95% CI=[0.555, 0.698], P(fair)=0.019\n",
    "After 171 flips: p̂=0.624, 95% CI=[0.551, 0.695], P(fair)=0.023\n",
    "After 172 flips: p̂=0.626, 95% CI=[0.553, 0.697], P(fair)=0.020\n",
    "After 173 flips: p̂=0.629, 95% CI=[0.556, 0.698], P(fair)=0.017\n",
    "After 174 flips: p̂=0.631, 95% CI=[0.558, 0.700], P(fair)=0.015\n",
    "After 175 flips: p̂=0.627, 95% CI=[0.555, 0.697], P(fair)=0.018\n",
    "After 176 flips: p̂=0.629, 95% CI=[0.557, 0.699], P(fair)=0.016\n",
    "After 177 flips: p̂=0.626, 95% CI=[0.554, 0.695], P(fair)=0.020\n",
    "After 178 flips: p̂=0.628, 95% CI=[0.556, 0.697], P(fair)=0.017\n",
    "After 179 flips: p̂=0.624, 95% CI=[0.553, 0.693], P(fair)=0.021\n",
    "After 180 flips: p̂=0.626, 95% CI=[0.555, 0.695], P(fair)=0.018\n",
    "After 181 flips: p̂=0.628, 95% CI=[0.557, 0.697], P(fair)=0.015\n",
    "After 182 flips: p̂=0.630, 95% CI=[0.560, 0.699], P(fair)=0.013\n",
    "After 183 flips: p̂=0.627, 95% CI=[0.556, 0.695], P(fair)=0.017\n",
    "After 184 flips: p̂=0.624, 95% CI=[0.553, 0.692], P(fair)=0.021\n",
    "After 185 flips: p̂=0.626, 95% CI=[0.555, 0.693], P(fair)=0.018\n",
    "After 186 flips: p̂=0.622, 95% CI=[0.552, 0.690], P(fair)=0.022\n",
    "After 187 flips: p̂=0.619, 95% CI=[0.549, 0.687], P(fair)=0.027\n",
    "After 188 flips: p̂=0.621, 95% CI=[0.551, 0.689], P(fair)=0.023\n",
    "After 189 flips: p̂=0.623, 95% CI=[0.553, 0.690], P(fair)=0.020\n",
    "After 190 flips: p̂=0.625, 95% CI=[0.556, 0.692], P(fair)=0.017\n",
    "After 191 flips: p̂=0.627, 95% CI=[0.558, 0.694], P(fair)=0.015\n",
    "After 192 flips: p̂=0.624, 95% CI=[0.555, 0.690], P(fair)=0.018\n",
    "After 193 flips: p̂=0.621, 95% CI=[0.551, 0.687], P(fair)=0.023\n",
    "After 194 flips: p̂=0.617, 95% CI=[0.548, 0.684], P(fair)=0.028\n",
    "After 195 flips: p̂=0.619, 95% CI=[0.551, 0.686], P(fair)=0.024\n",
    "After 196 flips: p̂=0.621, 95% CI=[0.553, 0.687], P(fair)=0.021\n",
    "After 197 flips: p̂=0.618, 95% CI=[0.550, 0.684], P(fair)=0.025\n",
    "After 198 flips: p̂=0.615, 95% CI=[0.547, 0.681], P(fair)=0.031\n",
    "After 199 flips: p̂=0.612, 95% CI=[0.544, 0.678], P(fair)=0.037\n",
    "After 200 flips: p̂=0.609, 95% CI=[0.541, 0.675], P(fair)=0.045\n",
    "\n",
    "Sequential Bayesian Testing\n",
    "======================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_f72ed70d3e38.png)\n",
    "\n",
    "\n",
    "## Application 3: Quality Control in Manufacturing\n",
    "\n",
    "### Problem\n",
    "\n",
    "A factory produces bolts. The diameter should be 10mm with tolerance ±0.5mm.\n",
    "\n",
    "**Goal**: Estimate the mean and variance of bolt diameters from a sample.\n",
    "\n",
    "### Bayesian Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True process parameters (unknown)\n",
    "true_mu = 10.1  # Slightly off target\n",
    "true_sigma = 0.3\n",
    "\n",
    "# Sample bolts\n",
    "n = 30\n",
    "measurements = np.random.normal(true_mu, true_sigma, n)\n",
    "\n",
    "print(\"Quality Control: Bayesian Inference\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target: 10.0 mm ± 0.5 mm\")\n",
    "print(f\"Sample size: {n}\")\n",
    "print(f\"Sample mean: {np.mean(measurements):.3f} mm\")\n",
    "print(f\"Sample std: {np.std(measurements, ddof=1):.3f} mm\")\n",
    "print()\n",
    "\n",
    "# Weakly informative prior (centered on target)\n",
    "mu_0 = 10.0  # Target diameter\n",
    "kappa_0 = 1  # Low confidence\n",
    "alpha = 3    # Weak prior for variance\n",
    "beta = 0.5 * (alpha - 1)  # Prior mean variance ~ 0.25\n",
    "\n",
    "# Data summaries\n",
    "xbar = np.mean(measurements)\n",
    "ss = np.sum((measurements - xbar)**2)\n",
    "\n",
    "# Posterior parameters\n",
    "kappa_n = kappa_0 + n\n",
    "mu_n = (kappa_0 * mu_0 + n * xbar) / kappa_n\n",
    "alpha_n = alpha + n/2\n",
    "beta_n = beta + 0.5*ss + (kappa_0*n*(xbar - mu_0)**2)/(2*kappa_n)\n",
    "\n",
    "print(f\"Prior: Normal-Gamma({mu_0}, {kappa_0}, {alpha}, {beta})\")\n",
    "print(f\"Posterior: Normal-Gamma({mu_n:.3f}, {kappa_n}, {alpha_n:.1f}, {beta_n:.3f})\")\n",
    "print()\n",
    "\n",
    "# Marginal posterior for mu\n",
    "df_mu = 2 * alpha_n\n",
    "scale_mu = np.sqrt(beta_n / (alpha_n * kappa_n))\n",
    "ci_mu_lower = stats.t.ppf(0.025, df_mu, loc=mu_n, scale=scale_mu)\n",
    "ci_mu_upper = stats.t.ppf(0.975, df_mu, loc=mu_n, scale=scale_mu)\n",
    "\n",
    "print(f\"Process Mean (μ):\")\n",
    "print(f\"  Estimate: {mu_n:.3f} mm\")\n",
    "print(f\"  95% CI: [{ci_mu_lower:.3f}, {ci_mu_upper:.3f}] mm\")\n",
    "\n",
    "# Check if within tolerance\n",
    "target = 10.0\n",
    "tolerance = 0.5\n",
    "prob_in_spec = stats.t.cdf(target + tolerance, df_mu, loc=mu_n, scale=scale_mu) - \\\n",
    "               stats.t.cdf(target - tolerance, df_mu, loc=mu_n, scale=scale_mu)\n",
    "\n",
    "print(f\"\\nP(process mean within spec) = {prob_in_spec:.3f}\")\n",
    "\n",
    "# Marginal posterior for sigma^2\n",
    "sigma2_mean = beta_n / (alpha_n - 1)\n",
    "print(f\"\\nProcess Variance (σ²):\")\n",
    "print(f\"  Estimate: {sigma2_mean:.3f}\")\n",
    "print(f\"  Std Dev: {np.sqrt(sigma2_mean):.3f} mm\")\n",
    "\n",
    "# Predictive: What proportion of future bolts will be in spec?\n",
    "df_pred = 2 * alpha_n\n",
    "scale_pred = np.sqrt(beta_n * (kappa_n + 1) / (alpha_n * kappa_n))\n",
    "\n",
    "prob_future_in_spec = stats.t.cdf(target + tolerance, df_pred, loc=mu_n, scale=scale_pred) - \\\n",
    "                      stats.t.cdf(target - tolerance, df_pred, loc=mu_n, scale=scale_pred)\n",
    "\n",
    "print(f\"\\nPredicted proportion of future bolts in spec: {prob_future_in_spec*100:.1f}%\")\n",
    "\n",
    "if prob_future_in_spec < 0.95:\n",
    "    print(\"  ⚠️ WARNING: Process may need adjustment!\")\n",
    "else:\n",
    "    print(\"  ✅ Process is within acceptable limits\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Measurements\n",
    "axes[0,0].hist(measurements, bins=15, density=True, alpha=0.7, edgecolor='black',\n",
    "              label='Observed')\n",
    "x = np.linspace(9, 11, 100)\n",
    "axes[0,0].axvline(target, color='green', linestyle='--', linewidth=2, label='Target')\n",
    "axes[0,0].axvspan(target-tolerance, target+tolerance, alpha=0.2, color='green',\n",
    "                  label='Tolerance')\n",
    "axes[0,0].set_xlabel('Diameter (mm)', fontsize=11)\n",
    "axes[0,0].set_ylabel('Density', fontsize=11)\n",
    "axes[0,0].set_title('Measured Diameters', fontsize=12)\n",
    "axes[0,0].legend(fontsize=9)\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Posterior for mu\n",
    "mu_vals = np.linspace(9.5, 10.5, 1000)\n",
    "posterior_mu = stats.t.pdf(mu_vals, df_mu, loc=mu_n, scale=scale_mu)\n",
    "axes[0,1].fill_between(mu_vals, 0, posterior_mu, alpha=0.3, color='blue')\n",
    "axes[0,1].plot(mu_vals, posterior_mu, 'b-', linewidth=2)\n",
    "axes[0,1].axvline(mu_n, color='blue', linestyle='--', linewidth=2,\n",
    "                 label=f'Post. mean={mu_n:.2f}')\n",
    "axes[0,1].axvline(target, color='green', linestyle='--', linewidth=2, label='Target')\n",
    "axes[0,1].axvspan(ci_mu_lower, ci_mu_upper, alpha=0.3, color='blue',\n",
    "                  label='95% CI')\n",
    "axes[0,1].set_xlabel('μ (mm)', fontsize=11)\n",
    "axes[0,1].set_ylabel('Density', fontsize=11)\n",
    "axes[0,1].set_title('Posterior for Process Mean', fontsize=12)\n",
    "axes[0,1].legend(fontsize=9)\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Posterior for sigma^2\n",
    "sigma2_vals = np.linspace(0.01, 0.5, 1000)\n",
    "posterior_sigma2 = stats.invgamma.pdf(sigma2_vals, alpha_n, scale=beta_n)\n",
    "axes[1,0].fill_between(sigma2_vals, 0, posterior_sigma2, alpha=0.3, color='red')\n",
    "axes[1,0].plot(sigma2_vals, posterior_sigma2, 'r-', linewidth=2)\n",
    "axes[1,0].axvline(sigma2_mean, color='red', linestyle='--', linewidth=2,\n",
    "                 label=f'Post. mean={sigma2_mean:.3f}')\n",
    "axes[1,0].set_xlabel('σ²', fontsize=11)\n",
    "axes[1,0].set_ylabel('Density', fontsize=11)\n",
    "axes[1,0].set_title('Posterior for Process Variance', fontsize=12)\n",
    "axes[1,0].legend(fontsize=9)\n",
    "axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "# Predictive distribution\n",
    "predictive_vals = np.linspace(9, 11, 1000)\n",
    "predictive = stats.t.pdf(predictive_vals, df_pred, loc=mu_n, scale=scale_pred)\n",
    "axes[1,1].fill_between(predictive_vals, 0, predictive, alpha=0.3, color='purple')\n",
    "axes[1,1].plot(predictive_vals, predictive, 'purple', linewidth=2, label='Predictive')\n",
    "axes[1,1].axvline(target, color='green', linestyle='--', linewidth=2, label='Target')\n",
    "axes[1,1].axvspan(target-tolerance, target+tolerance, alpha=0.2, color='green',\n",
    "                  label=f'Spec ({prob_future_in_spec*100:.1f}% in spec)')\n",
    "axes[1,1].set_xlabel('Diameter (mm)', fontsize=11)\n",
    "axes[1,1].set_ylabel('Density', fontsize=11)\n",
    "axes[1,1].set_title('Posterior Predictive for Future Bolts', fontsize=12)\n",
    "axes[1,1].legend(fontsize=9)\n",
    "axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('quality_control_bayesian.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quality Control: Bayesian Inference\n",
    "======================================================================\n",
    "Target: 10.0 mm ± 0.5 mm\n",
    "Sample size: 30\n",
    "Sample mean: 10.044 mm\n",
    "Sample std: 0.270 mm\n",
    "\n",
    "Prior: Normal-Gamma(10.0, 1, 3, 1.0)\n",
    "Posterior: Normal-Gamma(10.042, 31, 18.0, 2.058)\n",
    "\n",
    "Process Mean (μ):\n",
    "  Estimate: 10.042 mm\n",
    "  95% CI: [9.919, 10.165] mm\n",
    "\n",
    "P(process mean within spec) = 1.000\n",
    "\n",
    "Process Variance (σ²):\n",
    "  Estimate: 0.121\n",
    "  Std Dev: 0.348 mm\n",
    "\n",
    "Predicted proportion of future bolts in spec: 84.3%\n",
    "  ⚠️ WARNING: Process may need adjustment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_f741d52d3223.png)\n",
    "\n",
    "\n",
    "## Application 4: Email Spam Rate Estimation\n",
    "\n",
    "### Problem\n",
    "\n",
    "Estimate spam rate in email with limited data and update as new emails arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Email Spam Rate: Sequential Bayesian Updating\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# True spam rate (unknown)\n",
    "true_spam_rate = 0.35\n",
    "\n",
    "# Prior based on historical data: Beta(7, 13)\n",
    "# Corresponds to previous belief: ~35% spam, moderate confidence\n",
    "alpha_prior = 7\n",
    "beta_prior = 13\n",
    "\n",
    "print(f\"Prior belief: Beta({alpha_prior}, {beta_prior})\")\n",
    "print(f\"Prior mean: {alpha_prior/(alpha_prior+beta_prior):.3f}\")\n",
    "print()\n",
    "\n",
    "# Simulate checking emails in batches\n",
    "batches = [10, 20, 50, 100, 200]\n",
    "alpha = alpha_prior\n",
    "beta = beta_prior\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "p_vals = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Plot prior\n",
    "prior = stats.beta.pdf(p_vals, alpha_prior, beta_prior)\n",
    "axes[0].plot(p_vals, prior, 'b-', linewidth=2)\n",
    "axes[0].axvline(alpha_prior/(alpha_prior+beta_prior), color='blue',\n",
    "               linestyle='--', linewidth=2,\n",
    "               label=f'Prior mean={alpha_prior/(alpha_prior+beta_prior):.2f}')\n",
    "axes[0].axvline(true_spam_rate, color='green', linestyle='--', linewidth=2,\n",
    "               label=f'True rate={true_spam_rate}')\n",
    "axes[0].set_xlabel('Spam Rate', fontsize=10)\n",
    "axes[0].set_ylabel('Density', fontsize=10)\n",
    "axes[0].set_title('Prior', fontsize=11)\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Process batches\n",
    "total_emails = 0\n",
    "for idx, batch_size in enumerate(batches):\n",
    "    # Simulate new emails\n",
    "    new_spam = np.random.binomial(batch_size, true_spam_rate)\n",
    "    \n",
    "    # Update posterior\n",
    "    alpha += new_spam\n",
    "    beta += (batch_size - new_spam)\n",
    "    total_emails += batch_size\n",
    "    \n",
    "    # Posterior\n",
    "    posterior = stats.beta.pdf(p_vals, alpha, beta)\n",
    "    post_mean = alpha / (alpha + beta)\n",
    "    ci_lower = stats.beta.ppf(0.025, alpha, beta)\n",
    "    ci_upper = stats.beta.ppf(0.975, alpha, beta)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx + 1]\n",
    "    ax.fill_between(p_vals, 0, posterior, alpha=0.3, color='red')\n",
    "    ax.plot(p_vals, posterior, 'r-', linewidth=2)\n",
    "    ax.axvline(post_mean, color='red', linestyle='--', linewidth=2,\n",
    "              label=f'Post. mean={post_mean:.3f}')\n",
    "    ax.axvline(true_spam_rate, color='green', linestyle='--', linewidth=2,\n",
    "              label=f'True={true_spam_rate}')\n",
    "    ax.fill_between(p_vals[(p_vals >= ci_lower) & (p_vals <= ci_upper)],\n",
    "                     0,\n",
    "                     posterior[(p_vals >= ci_lower) & (p_vals <= ci_upper)],\n",
    "                     alpha=0.5, color='darkred')\n",
    "    ax.set_xlabel('Spam Rate', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'After {total_emails} emails\\n({new_spam}/{batch_size} spam in last batch)',\n",
    "                fontsize=11)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlim(0, 0.8)\n",
    "    \n",
    "    print(f\"After {total_emails:3d} emails: Beta({alpha:3d}, {beta:3d}), \"\n",
    "          f\"mean={post_mean:.3f}, 95% CI=[{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('spam_rate_sequential.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Email Spam Rate: Sequential Bayesian Updating\n",
    "======================================================================\n",
    "Prior belief: Beta(7, 13)\n",
    "Prior mean: 0.350\n",
    "\n",
    "After  10 emails: Beta( 10,  20), mean=0.333, 95% CI=[0.179, 0.508]\n",
    "After  30 emails: Beta( 21,  29), mean=0.420, 95% CI=[0.288, 0.558]\n",
    "After  80 emails: Beta( 41,  59), mean=0.410, 95% CI=[0.316, 0.507]\n",
    "After 180 emails: Beta( 79, 121), mean=0.395, 95% CI=[0.328, 0.463]\n",
    "After 380 emails: Beta(160, 240), mean=0.400, 95% CI=[0.353, 0.448]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_a1e4688f3562.png)\n",
    "\n",
    "\n",
    "## Application 5: Machine Learning - Hyperparameter Tuning\n",
    "\n",
    "### Problem\n",
    "\n",
    "Estimate the optimal learning rate for a neural network using Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def objective_function(learning_rate):\n",
    "    \"\"\"\n",
    "    Simulated model performance (unknown in practice).\n",
    "    True optimum around 0.01\n",
    "    \"\"\"\n",
    "    optimal_lr = 0.01\n",
    "    # Gaussian-like performance with noise\n",
    "    noise = np.random.normal(0, 0.02)\n",
    "    performance = 1.0 - 20 * (learning_rate - optimal_lr)**2 + noise\n",
    "    return np.clip(performance, 0, 1)\n",
    "\n",
    "print(\"Bayesian Hyperparameter Optimization\")\n",
    "print(\"=\"*70)\n",
    "print(\"Goal: Find optimal learning rate for neural network\")\n",
    "print()\n",
    "\n",
    "# Observations\n",
    "lr_tested = []\n",
    "performance = []\n",
    "\n",
    "# Initial random samples\n",
    "for _ in range(5):\n",
    "    lr = np.random.uniform(0.001, 0.1)\n",
    "    perf = objective_function(lr)\n",
    "    lr_tested.append(lr)\n",
    "    performance.append(perf)\n",
    "    print(f\"Try LR={lr:.4f}: Performance={perf:.3f}\")\n",
    "\n",
    "print(\"\\nSwitch to Bayesian optimization...\")\n",
    "print()\n",
    "\n",
    "# Fit Gaussian Process (simplified: use Normal approximation)\n",
    "# In practice, use GPyOpt or similar\n",
    "for iteration in range(10):\n",
    "    # Simple acquisition: expected improvement\n",
    "    # Here we use a simplified version\n",
    "    \n",
    "    # Try learning rate that maximizes expected performance\n",
    "    # based on current data\n",
    "    lr_candidates = np.linspace(0.001, 0.1, 100)\n",
    "    \n",
    "    # Simple model: higher performance regions more likely\n",
    "    weights = np.array(performance)\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Sample from weighted distribution of successful regions\n",
    "    best_idx = np.argmax(performance)\n",
    "    best_lr = lr_tested[best_idx]\n",
    "    \n",
    "    # Explore around best + some randomness\n",
    "    lr_new = best_lr + np.random.normal(0, 0.01)\n",
    "    lr_new = np.clip(lr_new, 0.001, 0.1)\n",
    "    \n",
    "    perf_new = objective_function(lr_new)\n",
    "    lr_tested.append(lr_new)\n",
    "    performance.append(perf_new)\n",
    "    \n",
    "    if perf_new > np.max(performance[:-1]):\n",
    "        print(f\"Iteration {iteration+1}: LR={lr_new:.4f}, Performance={perf_new:.3f} ⭐ NEW BEST\")\n",
    "    else:\n",
    "        print(f\"Iteration {iteration+1}: LR={lr_new:.4f}, Performance={perf_new:.3f}\")\n",
    "\n",
    "# Results\n",
    "best_idx = np.argmax(performance)\n",
    "print(f\"\\nBest learning rate found: {lr_tested[best_idx]:.4f}\")\n",
    "print(f\"Best performance: {performance[best_idx]:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True function and tested points\n",
    "lr_range = np.linspace(0.001, 0.1, 1000)\n",
    "true_performance = [1.0 - 20*(lr - 0.01)**2 for lr in lr_range]\n",
    "\n",
    "ax1.plot(lr_range, true_performance, 'g--', linewidth=2, alpha=0.5,\n",
    "        label='True (unknown)')\n",
    "ax1.scatter(lr_tested[:5], performance[:5], s=100, c='blue',\n",
    "           marker='o', label='Random search', zorder=5)\n",
    "ax1.scatter(lr_tested[5:], performance[5:], s=100, c='red',\n",
    "           marker='^', label='Bayesian optimization', zorder=5)\n",
    "ax1.scatter(lr_tested[best_idx], performance[best_idx], s=300, c='gold',\n",
    "           marker='*', edgecolor='black', linewidth=2,\n",
    "           label='Best found', zorder=10)\n",
    "ax1.axvline(0.01, color='green', linestyle=':', linewidth=2,\n",
    "           label='True optimum')\n",
    "ax1.set_xlabel('Learning Rate', fontsize=11)\n",
    "ax1.set_ylabel('Model Performance', fontsize=11)\n",
    "ax1.set_title('Hyperparameter Optimization', fontsize=12)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Performance over iterations\n",
    "max_so_far = []\n",
    "current_max = 0\n",
    "for perf in performance:\n",
    "    current_max = max(current_max, perf)\n",
    "    max_so_far.append(current_max)\n",
    "\n",
    "ax2.plot(range(1, len(performance)+1), max_so_far, 'b-o', linewidth=2,\n",
    "        markersize=8)\n",
    "ax2.axhline(max(true_performance), color='green', linestyle='--',\n",
    "           linewidth=2, label='True maximum')\n",
    "ax2.axvline(5.5, color='red', linestyle=':', linewidth=2,\n",
    "           label='Start Bayesian opt.')\n",
    "ax2.set_xlabel('Iteration', fontsize=11)\n",
    "ax2.set_ylabel('Best Performance So Far', fontsize=11)\n",
    "ax2.set_title('Optimization Progress', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hyperparameter_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayesian Hyperparameter Optimization\n",
    "======================================================================\n",
    "Goal: Find optimal learning rate for neural network\n",
    "\n",
    "Try LR=0.0381: Performance=0.962\n",
    "Try LR=0.0164: Performance=1.000\n",
    "Try LR=0.0068: Performance=1.000\n",
    "Try LR=0.0711: Performance=0.946\n",
    "Try LR=0.0030: Performance=0.987\n",
    "\n",
    "Switch to Bayesian optimization...\n",
    "\n",
    "Iteration 1: LR=0.0112, Performance=0.989\n",
    "Iteration 2: LR=0.0072, Performance=0.948\n",
    "Iteration 3: LR=0.0259, Performance=1.000\n",
    "Iteration 4: LR=0.0012, Performance=0.990\n",
    "Iteration 5: LR=0.0090, Performance=0.986\n",
    "Iteration 6: LR=0.0010, Performance=0.986\n",
    "Iteration 7: LR=0.0224, Performance=1.000\n",
    "Iteration 8: LR=0.0204, Performance=1.000\n",
    "Iteration 9: LR=0.0113, Performance=0.988\n",
    "Iteration 10: LR=0.0259, Performance=1.000\n",
    "\n",
    "Best learning rate found: 0.0164\n",
    "Best performance: 1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_8af478398449.png)\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Applications Covered\n",
    "\n",
    "1. **A/B Testing**: Bayesian approach gives probability statements\n",
    "2. **Sequential Testing**: Update beliefs as data arrives\n",
    "3. **Quality Control**: Combine prior knowledge with measurements\n",
    "4. **Spam Detection**: Online learning from streaming data\n",
    "5. **Hyperparameter Tuning**: Efficient exploration of parameter space\n",
    "\n",
    "### Why Bayesian Methods Excel\n",
    "\n",
    "✅ **Small Data**: Prior knowledge regularizes estimates  \n",
    "✅ **Sequential Data**: Natural online updating  \n",
    "✅ **Decision Making**: Direct probability statements  \n",
    "✅ **Uncertainty Quantification**: Full posterior distributions  \n",
    "✅ **Prior Knowledge**: Incorporate domain expertise  \n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "**When to use Bayesian inference**:\n",
    "- Small to moderate datasets\n",
    "- Strong prior knowledge available\n",
    "- Need probability statements (not just p-values)\n",
    "- Sequential/online learning scenarios\n",
    "- Cost of being wrong is high\n",
    "\n",
    "**When MLE might be better**:\n",
    "- Very large datasets (prior becomes irrelevant)\n",
    "- No prior knowledge (but can use uniform prior)\n",
    "- Computational constraints (MLE usually faster)\n",
    "- Peer community expects frequentist methods\n",
    "\n",
    "### Modern Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For complex Bayesian models\n",
    "import pymc  # Probabilistic programming\n",
    "import stan  # Another popular framework\n",
    "import arviz  # Visualization and diagnostics\n",
    "\n",
    "# For Bayesian ML\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import gpytorch  # Deep GPs\n",
    "import gpyopt  # Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**\n",
    "`Error: ModuleNotFoundError: No module named 'pymc'`\n",
    "\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Parameter estimation is fundamental to:\n",
    "- **Statistics**: Understanding populations from samples\n",
    "- **Machine Learning**: Training models from data\n",
    "- **Science**: Estimating physical constants\n",
    "- **Engineering**: System identification and control\n",
    "\n",
    "Both **MLE** and **Bayesian** approaches are valuable:\n",
    "- MLE: Simple, efficient, widely accepted\n",
    "- Bayesian: Flexible, principled, quantifies uncertainty\n",
    "\n",
    "Choose based on your problem, data size, and available prior knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}