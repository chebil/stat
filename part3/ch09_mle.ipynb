{
 "cells": [
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "# 9.1 Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "**Maximum Likelihood Estimation**: Find the parameter value that **maximizes the probability** of observing the data we actually observed.\n",
    "\n",
    "## The Likelihood Function\n",
    "\n",
    "### Definition\n",
    "\n",
    "Given data \\(x_1, x_2, \\ldots, x_n\\) and a model with parameter θ, the **likelihood function** is:\n",
    "\n",
    "$$\n",
    "L(\\theta \\mid x_1, \\ldots, x_n) = P(x_1, \\ldots, x_n \\mid \\theta)\n",
    "$$\n",
    "\n",
    "### For Independent Data\n",
    "\n",
    "If observations are **independent and identically distributed (i.i.d.)**:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "where \\(f(x \\mid \\theta)\\) is the probability (PMF) or density (PDF).\n",
    "\n",
    "### Log-Likelihood\n",
    "\n",
    "In practice, we work with **log-likelihood** (easier to maximize):\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "## The MLE\n",
    "\n",
    "**Maximum Likelihood Estimate** \\(\\hat{\\theta}_{\\text{MLE}}\\) is the value that maximizes \\(L(\\theta)\\) or \\(\\ell(\\theta)\\):\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} \\ell(\\theta)\n",
    "$$\n",
    "\n",
    "### Finding the MLE\n",
    "\n",
    "1. Write down the likelihood \\(L(\\theta)\\)\n",
    "2. Take the log: \\(\\ell(\\theta) = \\log L(\\theta)\\)\n",
    "3. Take the derivative: \\(\\frac{d\\ell}{d\\theta}\\)\n",
    "4. Set equal to zero and solve: \\(\\frac{d\\ell}{d\\theta} = 0\\)\n",
    "5. Verify it's a maximum (check second derivative < 0)\n",
    "\n",
    "## Example 1: Bernoulli Distribution\n",
    "\n",
    "### Problem\n",
    "\n",
    "Flip a coin n times, observe k heads. Estimate p (probability of heads).\n",
    "\n",
    "### Solution\n",
    "\n",
    "**Model**: Each flip \\(X_i \\sim \\text{Bernoulli}(p)\\)\n",
    "\n",
    "**Likelihood**:\n",
    "$$\n",
    "L(p) = \\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i} = p^k(1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "where k = number of heads.\n",
    "\n",
    "**Log-likelihood**:\n",
    "$$\n",
    "\\ell(p) = k\\log p + (n-k)\\log(1-p)\n",
    "$$\n",
    "\n",
    "**Derivative**:\n",
    "$$\n",
    "\\frac{d\\ell}{dp} = \\frac{k}{p} - \\frac{n-k}{1-p}\n",
    "$$\n",
    "\n",
    "**Set to zero**:\n",
    "$$\n",
    "\\frac{k}{p} = \\frac{n-k}{1-p} \\implies k(1-p) = p(n-k) \\implies k = np\n",
    "$$\n",
    "\n",
    "**MLE**:\n",
    "$$\n",
    "\\hat{p}_{\\text{MLE}} = \\frac{k}{n}\n",
    "$$\n",
    "\n",
    "The MLE is just the sample proportion!\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data: 100 flips with true p = 0.6\n",
    "true_p = 0.6\n",
    "n = 100\n",
    "flips = np.random.binomial(1, true_p, n)\n",
    "k = np.sum(flips)\n",
    "\n",
    "print(\"Bernoulli MLE Example\")\n",
    "print(\"=\"*70)\n",
    "print(f\"True p: {true_p}\")\n",
    "print(f\"Sample: {n} flips, {k} heads\")\n",
    "print()\n",
    "\n",
    "# MLE\n",
    "p_mle = k / n\n",
    "print(f\"MLE estimate: p̂ = {p_mle:.3f}\")\n",
    "print()\n",
    "\n",
    "# Log-likelihood function\n",
    "def log_likelihood_bernoulli(p, data):\n",
    "    k = np.sum(data)\n",
    "    n = len(data)\n",
    "    if p <= 0 or p >= 1:\n",
    "        return -np.inf\n",
    "    return k * np.log(p) + (n - k) * np.log(1 - p)\n",
    "\n",
    "# Plot likelihood function\n",
    "p_values = np.linspace(0.01, 0.99, 1000)\n",
    "log_likes = [log_likelihood_bernoulli(p, flips) for p in p_values]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Log-likelihood\n",
    "ax1.plot(p_values, log_likes, 'b-', linewidth=2)\n",
    "ax1.axvline(p_mle, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'MLE = {p_mle:.3f}')\n",
    "ax1.axvline(true_p, color='green', linestyle='--', linewidth=2,\n",
    "            label=f'True p = {true_p}')\n",
    "ax1.set_xlabel('p', fontsize=12)\n",
    "ax1.set_ylabel('Log-Likelihood', fontsize=12)\n",
    "ax1.set_title('Log-Likelihood Function', fontsize=13)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Likelihood (not log)\n",
    "likes = np.exp(log_likes - np.max(log_likes))  # Normalize for plotting\n",
    "ax2.plot(p_values, likes, 'b-', linewidth=2)\n",
    "ax2.axvline(p_mle, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'MLE = {p_mle:.3f}')\n",
    "ax2.axvline(true_p, color='green', linestyle='--', linewidth=2,\n",
    "            label=f'True p = {true_p}')\n",
    "ax2.set_xlabel('p', fontsize=12)\n",
    "ax2.set_ylabel('Likelihood (normalized)', fontsize=12)\n",
    "ax2.set_title('Likelihood Function', fontsize=13)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bernoulli_mle.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli MLE Example\n",
    "======================================================================\n",
    "True p: 0.6\n",
    "Sample: 100 flips, 63 heads\n",
    "\n",
    "MLE estimate: p̂ = 0.630"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_84a931ba1a29.png)\n",
    "\n",
    "\n",
    "## Example 2: Normal Distribution\n",
    "\n",
    "### Problem\n",
    "\n",
    "Observe \\(x_1, \\ldots, x_n \\sim N(\\mu, \\sigma^2)\\). Estimate μ and σ².\n",
    "\n",
    "### Solution\n",
    "\n",
    "**PDF**:\n",
    "$$\n",
    "f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**Log-likelihood**:\n",
    "$$\n",
    "\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "**Derivative w.r.t. μ**:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial\\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "**MLE for μ**:\n",
    "$$\n",
    "\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\bar{x}\n",
    "$$\n",
    "\n",
    "**Derivative w.r.t. σ²**:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial\\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "**MLE for σ²**:\n",
    "$$\n",
    "\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "Note: This uses \\(n\\) in denominator, not \\(n-1\\) (slightly biased for small n).\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "true_mu = 10\n",
    "true_sigma = 2\n",
    "n = 50\n",
    "data = np.random.normal(true_mu, true_sigma, n)\n",
    "\n",
    "print(\"Normal Distribution MLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"True μ: {true_mu}, True σ: {true_sigma}\")\n",
    "print(f\"Sample size: {n}\")\n",
    "print()\n",
    "\n",
    "# Analytical MLE\n",
    "mu_mle = np.mean(data)\n",
    "sigma2_mle = np.mean((data - mu_mle)**2)\n",
    "sigma_mle = np.sqrt(sigma2_mle)\n",
    "\n",
    "print(\"Analytical MLE:\")\n",
    "print(f\"  μ̂ = {mu_mle:.3f}\")\n",
    "print(f\"  σ̂ = {sigma_mle:.3f}\")\n",
    "print()\n",
    "\n",
    "# Numerical MLE (for illustration)\n",
    "def neg_log_likelihood_normal(params, data):\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    return n/2 * np.log(2*np.pi) + n/2 * np.log(sigma**2) + \\\n",
    "           np.sum((data - mu)**2) / (2 * sigma**2)\n",
    "\n",
    "# Optimize\n",
    "initial_guess = [0, 1]\n",
    "result = minimize(neg_log_likelihood_normal, initial_guess, args=(data,),\n",
    "                  method='Nelder-Mead')\n",
    "\n",
    "mu_mle_num, sigma_mle_num = result.x\n",
    "\n",
    "print(\"Numerical MLE (verification):\")\n",
    "print(f\"  μ̂ = {mu_mle_num:.3f}\")\n",
    "print(f\"  σ̂ = {sigma_mle_num:.3f}\")\n",
    "print()\n",
    "\n",
    "# Compare with true values\n",
    "print(\"Comparison with true values:\")\n",
    "print(f\"  μ: error = {abs(mu_mle - true_mu):.3f}\")\n",
    "print(f\"  σ: error = {abs(sigma_mle - true_sigma):.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Data histogram with fitted distribution\n",
    "ax1.hist(data, bins=15, density=True, alpha=0.7, edgecolor='black',\n",
    "         label='Data')\n",
    "x = np.linspace(data.min(), data.max(), 100)\n",
    "ax1.plot(x, stats.norm.pdf(x, mu_mle, sigma_mle), 'r-', linewidth=2,\n",
    "         label=f'MLE: N({mu_mle:.1f}, {sigma_mle:.1f}²)')\n",
    "ax1.plot(x, stats.norm.pdf(x, true_mu, true_sigma), 'g--', linewidth=2,\n",
    "         label=f'True: N({true_mu}, {true_sigma}²)')\n",
    "ax1.set_xlabel('Value', fontsize=11)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Data and Fitted Distribution', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Likelihood surface (contour plot)\n",
    "mu_range = np.linspace(8, 12, 100)\n",
    "sigma_range = np.linspace(1, 3, 100)\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "LL = np.zeros_like(MU)\n",
    "\n",
    "for i in range(len(mu_range)):\n",
    "    for j in range(len(sigma_range)):\n",
    "        LL[j, i] = -neg_log_likelihood_normal([MU[j,i], SIGMA[j,i]], data)\n",
    "\n",
    "contour = ax2.contour(MU, SIGMA, LL, levels=20, cmap='viridis')\n",
    "ax2.plot(mu_mle, sigma_mle, 'r*', markersize=20, label='MLE')\n",
    "ax2.plot(true_mu, true_sigma, 'go', markersize=12, label='True values')\n",
    "ax2.set_xlabel('μ', fontsize=12)\n",
    "ax2.set_ylabel('σ', fontsize=12)\n",
    "ax2.set_title('Log-Likelihood Surface', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "plt.colorbar(contour, ax=ax2, label='Log-Likelihood')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('normal_mle.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal Distribution MLE\n",
    "======================================================================\n",
    "True μ: 10, True σ: 2\n",
    "Sample size: 50\n",
    "\n",
    "Analytical MLE:\n",
    "  μ̂ = 9.549\n",
    "  σ̂ = 1.849\n",
    "\n",
    "Numerical MLE (verification):\n",
    "  μ̂ = 9.549\n",
    "  σ̂ = 1.849\n",
    "\n",
    "Comparison with true values:\n",
    "  μ: error = 0.451\n",
    "  σ: error = 0.151"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_194f7e84594e.png)\n",
    "\n",
    "\n",
    "## Example 3: Poisson Distribution\n",
    "\n",
    "### Problem\n",
    "\n",
    "Count data \\(x_1, \\ldots, x_n \\sim \\text{Poisson}(\\lambda)\\). Estimate λ.\n",
    "\n",
    "### Solution\n",
    "\n",
    "**PMF**: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\n",
    "\n",
    "**Log-likelihood**:\n",
    "$$\n",
    "\\ell(\\lambda) = \\sum_{i=1}^{n}\\left[x_i\\log\\lambda - \\lambda - \\log(x_i!)\\right]\n",
    "$$\n",
    "\n",
    "**Derivative**:\n",
    "$$\n",
    "\\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda}\\sum_{i=1}^{n}x_i - n = 0\n",
    "$$\n",
    "\n",
    "**MLE**:\n",
    "$$\n",
    "\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\bar{x}\n",
    "$$\n",
    "\n",
    "The MLE is the sample mean!\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate Poisson data (e.g., number of customers per hour)\n",
    "true_lambda = 5\n",
    "n = 100\n",
    "counts = np.random.poisson(true_lambda, n)\n",
    "\n",
    "print(\"Poisson MLE Example\")\n",
    "print(\"=\"*70)\n",
    "print(f\"True λ: {true_lambda}\")\n",
    "print(f\"Sample: {n} observations\")\n",
    "print(f\"Sample mean: {np.mean(counts):.3f}\")\n",
    "print(f\"Sample variance: {np.var(counts, ddof=1):.3f}\")\n",
    "print()\n",
    "\n",
    "# MLE\n",
    "lambda_mle = np.mean(counts)\n",
    "print(f\"MLE estimate: λ̂ = {lambda_mle:.3f}\")\n",
    "print()\n",
    "\n",
    "# Log-likelihood function\n",
    "def log_likelihood_poisson(lam, data):\n",
    "    if lam <= 0:\n",
    "        return -np.inf\n",
    "    return np.sum(data * np.log(lam) - lam - np.log(stats.factorial(data)))\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Data distribution\n",
    "max_count = counts.max()\n",
    "ax1.hist(counts, bins=range(max_count+2), density=True, alpha=0.7,\n",
    "         edgecolor='black', align='left', label='Observed')\n",
    "x = np.arange(0, max_count+1)\n",
    "ax1.plot(x, stats.poisson.pmf(x, lambda_mle), 'ro-', linewidth=2,\n",
    "         markersize=8, label=f'Fitted Poisson(λ̂={lambda_mle:.1f})')\n",
    "ax1.plot(x, stats.poisson.pmf(x, true_lambda), 'g^--', linewidth=2,\n",
    "         markersize=6, label=f'True Poisson(λ={true_lambda})')\n",
    "ax1.set_xlabel('Count', fontsize=11)\n",
    "ax1.set_ylabel('Probability', fontsize=11)\n",
    "ax1.set_title('Observed vs. Fitted Distribution', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Likelihood function\n",
    "lambda_values = np.linspace(3, 7, 1000)\n",
    "log_likes = [log_likelihood_poisson(lam, counts) for lam in lambda_values]\n",
    "\n",
    "ax2.plot(lambda_values, log_likes, 'b-', linewidth=2)\n",
    "ax2.axvline(lambda_mle, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'MLE = {lambda_mle:.2f}')\n",
    "ax2.axvline(true_lambda, color='green', linestyle='--', linewidth=2,\n",
    "            label=f'True λ = {true_lambda}')\n",
    "ax2.set_xlabel('λ', fontsize=12)\n",
    "ax2.set_ylabel('Log-Likelihood', fontsize=12)\n",
    "ax2.set_title('Log-Likelihood Function', fontsize=12)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('poisson_mle.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**\n",
    "`Error: AttributeError: module 'scipy.stats' has no attribute 'factorial'`\n",
    "\n",
    "\n",
    "## Properties of MLEs\n",
    "\n",
    "### 1. Consistency\n",
    "\n",
    "As \\(n \\to \\infty\\), \\(\\hat{\\theta}_{\\text{MLE}} \\to \\theta_{\\text{true}}\\) (in probability)\n",
    "\n",
    "### 2. Asymptotic Normality\n",
    "\n",
    "For large n:\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MLE}} \\sim N\\left(\\theta, \\frac{1}{nI(\\theta)}\\right)\n",
    "$$\n",
    "\n",
    "where \\(I(\\theta)\\) is the **Fisher Information**.\n",
    "\n",
    "### 3. Efficiency\n",
    "\n",
    "MLEs achieve the **Cramér-Rao lower bound** - no unbiased estimator has smaller variance.\n",
    "\n",
    "### 4. Invariance\n",
    "\n",
    "If \\(\\hat{\\theta}\\) is the MLE of θ, then \\(g(\\hat{\\theta})\\) is the MLE of \\(g(\\theta)\\) for any function g.\n",
    "\n",
    "## Confidence Intervals from MLE\n",
    "\n",
    "### Using Asymptotic Normality\n",
    "\n",
    "For large n:\n",
    "$$\n",
    "\\hat{\\theta} \\pm z_{\\alpha/2} \\cdot \\text{SE}(\\hat{\\theta})\n",
    "$$\n",
    "\n",
    "where SE is estimated from Fisher Information or observed information.\n",
    "\n",
    "### Profile Likelihood\n",
    "\n",
    "More accurate for small samples:\n",
    "$$\n",
    "\\{\\theta: 2(\\ell(\\hat{\\theta}) - \\ell(\\theta)) \\leq \\chi^2_{1,\\alpha}\\}\n",
    "$$\n",
    "\n",
    "## Numerical MLE\n",
    "\n",
    "For complex models without closed-form solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def neg_log_likelihood(params, data):\n",
    "    # Define your model here\n",
    "    # Return negative log-likelihood\n",
    "    pass\n",
    "\n",
    "# Find MLE\n",
    "initial_guess = [...]  # Starting values\n",
    "result = minimize(neg_log_likelihood, initial_guess, args=(data,),\n",
    "                  method='BFGS')\n",
    "\n",
    "mle_params = result.x\n",
    "print(f\"MLE: {mle_params}\")\n",
    "print(f\"Negative log-likelihood: {result.fun}\")\n",
    "print(f\"Success: {result.success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error: TypeError: float() argument must be a string or a real number, not 'ellipsis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### MLE Recipe\n",
    "\n",
    "1. **Write likelihood**: \\(L(\\theta) = \\prod f(x_i \\mid \\theta)\\)\n",
    "2. **Take log**: \\(\\ell(\\theta) = \\sum \\log f(x_i \\mid \\theta)\\)\n",
    "3. **Differentiate**: \\(\\frac{d\\ell}{d\\theta} = 0\\)\n",
    "4. **Solve**: Find \\(\\hat{\\theta}_{\\text{MLE}}\\)\n",
    "5. **Verify**: Check second derivative or use numerical optimization\n",
    "\n",
    "### Common MLEs\n",
    "\n",
    "| Distribution | Parameter | MLE |\n",
    "|-------------|-----------|-----|\n",
    "| Bernoulli(p) | p | k/n (sample proportion) |\n",
    "| Normal(μ, σ²) | μ | \\(\\bar{x}\\) (sample mean) |\n",
    "| Normal(μ, σ²) | σ² | \\(\\frac{1}{n}\\sum(x_i-\\bar{x})^2\\) |\n",
    "| Poisson(λ) | λ | \\(\\bar{x}\\) (sample mean) |\n",
    "| Exponential(λ) | λ | \\(1/\\bar{x}\\) |\n",
    "\n",
    "### Advantages of MLE\n",
    "\n",
    "✅ Optimal properties (consistency, efficiency)  \n",
    "✅ General method (works for any model)  \n",
    "✅ No prior distribution needed  \n",
    "✅ Widely used and understood  \n",
    "\n",
    "### Limitations\n",
    "\n",
    "❌ Can be unstable with small samples  \n",
    "❌ May not exist or be unique  \n",
    "❌ Doesn't incorporate prior knowledge  \n",
    "❌ Requires optimization for complex models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}