{
 "cells": [
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "# 9.2 Bayesian Inference\n",
    "\n",
    "## The Bayesian Philosophy\n",
    "\n",
    "**Bayesian inference** treats parameters as random variables with probability distributions, unlike MLE which treats them as fixed unknown values.\n",
    "\n",
    "### Key Difference from MLE\n",
    "\n",
    "- **MLE (Frequentist)**: Parameter θ is fixed but unknown; estimate it from data\n",
    "- **Bayesian**: Parameter θ is a random variable; update beliefs about θ given data\n",
    "\n",
    "## Bayes' Theorem for Parameters\n",
    "\n",
    "### The Fundamental Formula\n",
    "\n",
    "Given data \\(x_1, \\ldots, x_n\\) and parameter θ:\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid x_1, \\ldots, x_n) = \\frac{p(x_1, \\ldots, x_n \\mid \\theta) \\cdot p(\\theta)}{p(x_1, \\ldots, x_n)}\n",
    "$$\n",
    "\n",
    "**Components:**\n",
    "\n",
    "- **Posterior**: \\(p(\\theta \\mid x_1, \\ldots, x_n)\\) - Updated belief about θ after seeing data\n",
    "- **Likelihood**: \\(p(x_1, \\ldots, x_n \\mid \\theta)\\) - Probability of data given θ\n",
    "- **Prior**: \\(p(\\theta)\\) - Initial belief about θ before seeing data\n",
    "- **Evidence**: \\(p(x_1, \\ldots, x_n)\\) - Marginal probability of data (normalizing constant)\n",
    "\n",
    "### Simplified Form\n",
    "\n",
    "Since the evidence is just a normalizing constant:\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid \\text{data}) \\propto p(\\text{data} \\mid \\theta) \\cdot p(\\theta)\n",
    "$$\n",
    "\n",
    "Or more intuitively:\n",
    "\n",
    "$$\n",
    "\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n",
    "$$\n",
    "\n",
    "## Example 1: Coin Flipping (Beta-Binomial)\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "Estimate probability \\(p\\) of heads from \\(n\\) coin flips with \\(k\\) heads observed.\n",
    "\n",
    "### Bayesian Solution\n",
    "\n",
    "**Prior**: Beta distribution (common choice for probabilities)\n",
    "\n",
    "$$\n",
    "p(p) = \\text{Beta}(\\alpha, \\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha, \\beta)}\n",
    "$$\n",
    "\n",
    "where \\(B(\\alpha, \\beta)\\) is the Beta function.\n",
    "\n",
    "**Likelihood**: Binomial\n",
    "\n",
    "$$\n",
    "p(k \\mid p, n) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "**Posterior**: Also Beta! (conjugate prior)\n",
    "\n",
    "$$\n",
    "p(p \\mid k, n) = \\text{Beta}(\\alpha + k, \\beta + n - k)\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "- \\(\\alpha\\) = prior \"pseudo-counts\" of heads\n",
    "- \\(\\beta\\) = prior \"pseudo-counts\" of tails\n",
    "- Data adds \\(k\\) heads and \\(n-k\\) tails to these counts\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "true_p = 0.7\n",
    "n_flips = 20\n",
    "flips = np.random.binomial(1, true_p, n_flips)\n",
    "k = np.sum(flips)\n",
    "\n",
    "print(\"Bayesian Coin Flipping Example\")\n",
    "print(\"=\"*70)\n",
    "print(f\"True p: {true_p}\")\n",
    "print(f\"Data: {k} heads in {n_flips} flips\")\n",
    "print()\n",
    "\n",
    "# Different priors\n",
    "priors = [\n",
    "    (1, 1, \"Uniform (uninformative)\"),\n",
    "    (2, 2, \"Weak prior towards 0.5\"),\n",
    "    (10, 10, \"Strong prior towards 0.5\"),\n",
    "    (5, 2, \"Prior favoring heads\")\n",
    "]\n",
    "\n",
    "p_range = np.linspace(0, 1, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (alpha, beta, label) in enumerate(priors):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prior distribution\n",
    "    prior_dist = stats.beta(alpha, beta)\n",
    "    prior_pdf = prior_dist.pdf(p_range)\n",
    "    \n",
    "    # Posterior distribution\n",
    "    posterior_alpha = alpha + k\n",
    "    posterior_beta = beta + (n_flips - k)\n",
    "    posterior_dist = stats.beta(posterior_alpha, posterior_beta)\n",
    "    posterior_pdf = posterior_dist.pdf(p_range)\n",
    "    \n",
    "    # Likelihood (normalized for plotting)\n",
    "    likelihood = p_range**k * (1-p_range)**(n_flips-k)\n",
    "    likelihood = likelihood / np.max(likelihood) * np.max(posterior_pdf)\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(p_range, prior_pdf, 'b--', linewidth=2, label='Prior', alpha=0.7)\n",
    "    ax.plot(p_range, likelihood, 'g:', linewidth=2, label='Likelihood (scaled)', alpha=0.7)\n",
    "    ax.plot(p_range, posterior_pdf, 'r-', linewidth=2.5, label='Posterior')\n",
    "    ax.axvline(true_p, color='black', linestyle='--', linewidth=1.5, \n",
    "               label=f'True p = {true_p}', alpha=0.5)\n",
    "    \n",
    "    # MLE for comparison\n",
    "    p_mle = k / n_flips\n",
    "    ax.axvline(p_mle, color='orange', linestyle='--', linewidth=1.5,\n",
    "               label=f'MLE = {p_mle:.2f}', alpha=0.7)\n",
    "    \n",
    "    # Posterior mean\n",
    "    post_mean = posterior_alpha / (posterior_alpha + posterior_beta)\n",
    "    ax.axvline(post_mean, color='red', linestyle=':', linewidth=1.5,\n",
    "               label=f'Posterior mean = {post_mean:.2f}', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('p (probability of heads)', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'{label}\\nBeta({alpha}, {beta}) → Beta({posterior_alpha}, {posterior_beta})',\n",
    "                 fontsize=12)\n",
    "    ax.legend(fontsize=9, loc='upper left')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bayesian_coin_priors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPosterior Statistics:\")\n",
    "for alpha, beta, label in priors:\n",
    "    post_alpha = alpha + k\n",
    "    post_beta = beta + (n_flips - k)\n",
    "    post_mean = post_alpha / (post_alpha + post_beta)\n",
    "    post_mode = (post_alpha - 1) / (post_alpha + post_beta - 2)\n",
    "    post_var = (post_alpha * post_beta) / ((post_alpha + post_beta)**2 * (post_alpha + post_beta + 1))\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Posterior: Beta({post_alpha}, {post_beta})\")\n",
    "    print(f\"  Mean: {post_mean:.3f}\")\n",
    "    print(f\"  Mode: {post_mode:.3f}\")\n",
    "    print(f\"  Std: {np.sqrt(post_var):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayesian Coin Flipping Example\n",
    "======================================================================\n",
    "True p: 0.7\n",
    "Data: 14 heads in 20 flips\n",
    "\n",
    "\n",
    "Posterior Statistics:\n",
    "\n",
    "Uniform (uninformative):\n",
    "  Posterior: Beta(15, 7)\n",
    "  Mean: 0.682\n",
    "  Mode: 0.700\n",
    "  Std: 0.097\n",
    "\n",
    "Weak prior towards 0.5:\n",
    "  Posterior: Beta(16, 8)\n",
    "  Mean: 0.667\n",
    "  Mode: 0.682\n",
    "  Std: 0.094\n",
    "\n",
    "Strong prior towards 0.5:\n",
    "  Posterior: Beta(24, 16)\n",
    "  Mean: 0.600\n",
    "  Mode: 0.605\n",
    "  Std: 0.077\n",
    "\n",
    "Prior favoring heads:\n",
    "  Posterior: Beta(19, 8)\n",
    "  Mean: 0.704\n",
    "  Mode: 0.720\n",
    "  Std: 0.086"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_9e2ec49c1db7.png)\n",
    "\n",
    "\n",
    "## Credible Intervals\n",
    "\n",
    "### Definition\n",
    "\n",
    "A **95% credible interval** \\([a, b]\\) satisfies:\n",
    "\n",
    "$$\n",
    "P(a \\leq \\theta \\leq b \\mid \\text{data}) = 0.95\n",
    "$$\n",
    "\n",
    "**Interpretation**: \"There is a 95% probability that θ lies in \\([a, b]\\) given the data we observed.\"\n",
    "\n",
    "### Credible Intervals vs. Confidence Intervals\n",
    "\n",
    "**Credible Interval (Bayesian)**:\n",
    "- Direct probability statement about the parameter\n",
    "- \"θ has 95% probability of being in this interval\"\n",
    "\n",
    "**Confidence Interval (Frequentist)**:\n",
    "- Statement about the procedure, not the parameter\n",
    "- \"If we repeat this procedure many times, 95% of intervals will contain the true θ\"\n",
    "\n",
    "### Computing Credible Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Continuing from previous example\n",
    "alpha, beta = 1, 1  # Uniform prior\n",
    "k, n_flips = 14, 20  # Observed data\n",
    "\n",
    "# Posterior\n",
    "post_alpha = alpha + k\n",
    "post_beta = beta + (n_flips - k)\n",
    "posterior = stats.beta(post_alpha, post_beta)\n",
    "\n",
    "print(\"Credible Intervals\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Equal-tailed 95% credible interval\n",
    "lower = posterior.ppf(0.025)\n",
    "upper = posterior.ppf(0.975)\n",
    "print(f\"\\n95% Equal-tailed credible interval: [{lower:.3f}, {upper:.3f}]\")\n",
    "\n",
    "# Highest Posterior Density (HPD) interval\n",
    "# For unimodal symmetric distributions, similar to equal-tailed\n",
    "print(f\"\\nInterpretation: Given the data, there is a 95% probability\")\n",
    "print(f\"that the true probability of heads is between {lower:.3f} and {upper:.3f}\")\n",
    "\n",
    "# Compare with frequentist confidence interval\n",
    "p_mle = k / n_flips\n",
    "se = np.sqrt(p_mle * (1 - p_mle) / n_flips)\n",
    "ci_lower = p_mle - 1.96 * se\n",
    "ci_upper = p_mle + 1.96 * se\n",
    "\n",
    "print(f\"\\n95% Frequentist confidence interval: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "print(f\"\\nNote: These are often numerically similar but have different interpretations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credible Intervals\n",
    "======================================================================\n",
    "\n",
    "95% Equal-tailed credible interval: [0.478, 0.854]\n",
    "\n",
    "Interpretation: Given the data, there is a 95% probability\n",
    "that the true probability of heads is between 0.478 and 0.854\n",
    "\n",
    "95% Frequentist confidence interval: [0.499, 0.901]\n",
    "\n",
    "Note: These are often numerically similar but have different interpretations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "## Prior Selection\n",
    "\n",
    "### Types of Priors\n",
    "\n",
    "**1. Informative Prior**: Incorporates specific prior knowledge\n",
    "\n",
    "$$\n",
    "p = \\text{Beta}(20, 5) \\quad \\text{(strong belief that } p \\approx 0.8\\text{)}\n",
    "$$\n",
    "\n",
    "**2. Weakly Informative Prior**: General shape but not too strong\n",
    "\n",
    "$$\n",
    "p = \\text{Beta}(2, 2) \\quad \\text{(weak preference for } p \\approx 0.5\\text{)}\n",
    "$$\n",
    "\n",
    "**3. Non-informative (Uniform) Prior**: No prior knowledge\n",
    "\n",
    "$$\n",
    "p = \\text{Beta}(1, 1) = \\text{Uniform}(0, 1)\n",
    "$$\n",
    "\n",
    "### Choosing Priors\n",
    "\n",
    "**Guidelines**:\n",
    "1. Use domain knowledge when available\n",
    "2. Check sensitivity: Try different priors, see if conclusions change\n",
    "3. With large data, prior becomes less important\n",
    "4. Document your choice and reasoning\n",
    "\n",
    "## Sequential Updating\n",
    "\n",
    "### Bayesian Learning\n",
    "\n",
    "One beautiful property: **today's posterior = tomorrow's prior**\n",
    "\n",
    "$$\n",
    "\\text{Prior}_1 \\xrightarrow{\\text{Data}_1} \\text{Posterior}_1 = \\text{Prior}_2 \\xrightarrow{\\text{Data}_2} \\text{Posterior}_2\n",
    "$$\n",
    "\n",
    "### Example: Sequential Coin Flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameter\n",
    "true_p = 0.6\n",
    "\n",
    "# Start with uniform prior\n",
    "alpha, beta = 1, 1\n",
    "\n",
    "# Generate data in batches\n",
    "batch_sizes = [5, 10, 20, 50]\n",
    "p_range = np.linspace(0, 1, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, n_new in enumerate(batch_sizes):\n",
    "    # Generate new data\n",
    "    new_flips = np.random.binomial(1, true_p, n_new)\n",
    "    k_new = np.sum(new_flips)\n",
    "    \n",
    "    # Update posterior\n",
    "    alpha += k_new\n",
    "    beta += (n_new - k_new)\n",
    "    \n",
    "    # Current posterior\n",
    "    posterior = stats.beta(alpha, beta)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.plot(p_range, posterior.pdf(p_range), 'b-', linewidth=2.5, \n",
    "            label=f'Posterior: Beta({alpha}, {beta})')\n",
    "    ax.axvline(true_p, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'True p = {true_p}')\n",
    "    \n",
    "    post_mean = alpha / (alpha + beta)\n",
    "    ax.axvline(post_mean, color='green', linestyle=':', linewidth=2,\n",
    "               label=f'Posterior mean = {post_mean:.3f}')\n",
    "    \n",
    "    ax.fill_between(p_range, posterior.pdf(p_range), alpha=0.3)\n",
    "    \n",
    "    total_flips = alpha + beta - 2  # Subtract initial prior counts\n",
    "    ax.set_xlabel('p', fontsize=11)\n",
    "    ax.set_ylabel('Posterior Density', fontsize=11)\n",
    "    ax.set_title(f'After {total_flips} total flips ({k_new} heads in last {n_new})',\n",
    "                 fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequential_updating.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSequential Bayesian Updating\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final posterior after all data: Beta({alpha}, {beta})\")\n",
    "print(f\"Posterior mean: {alpha / (alpha + beta):.3f}\")\n",
    "print(f\"True p: {true_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequential Bayesian Updating\n",
    "======================================================================\n",
    "Final posterior after all data: Beta(55, 32)\n",
    "Posterior mean: 0.632\n",
    "True p: 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "![Plot](images/output_dadd51eb0e0f.png)\n",
    "\n",
    "\n",
    "## Posterior Predictive Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "Predict **new data** \\(\\tilde{x}\\) given observed data:\n",
    "\n",
    "$$\n",
    "p(\\tilde{x} \\mid x_1, \\ldots, x_n) = \\int p(\\tilde{x} \\mid \\theta) \\cdot p(\\theta \\mid x_1, \\ldots, x_n) \\, d\\theta\n",
    "$$\n",
    "\n",
    "**Interpretation**: Average predictions over all possible parameter values, weighted by their posterior probability.\n",
    "\n",
    "### Example: Predicting Next Coin Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Observed data: 7 heads in 10 flips\n",
    "alpha_prior, beta_prior = 1, 1\n",
    "k, n = 7, 10\n",
    "\n",
    "# Posterior\n",
    "alpha_post = alpha_prior + k\n",
    "beta_post = beta_prior + (n - k)\n",
    "\n",
    "print(\"Posterior Predictive Distribution\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nObserved: {k} heads in {n} flips\")\n",
    "print(f\"Posterior: Beta({alpha_post}, {beta_post})\")\n",
    "print()\n",
    "\n",
    "# Probability that next flip is heads\n",
    "# For Beta-Binomial, this has a closed form:\n",
    "prob_heads = alpha_post / (alpha_post + beta_post)\n",
    "\n",
    "print(f\"Predicted probability of heads on next flip: {prob_heads:.3f}\")\n",
    "print(f\"\\n(Compare to MLE: {k/n:.3f})\")\n",
    "print(f\"\\nNote: Bayesian prediction accounts for uncertainty in p!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Posterior Predictive Distribution\n",
    "======================================================================\n",
    "\n",
    "Observed: 7 heads in 10 flips\n",
    "Posterior: Beta(8, 4)\n",
    "\n",
    "Predicted probability of heads on next flip: 0.667\n",
    "\n",
    "(Compare to MLE: 0.700)\n",
    "\n",
    "Note: Bayesian prediction accounts for uncertainty in p!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": null,
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Bayes' Theorem**:\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid \\text{data}) = \\frac{p(\\text{data} \\mid \\theta) \\cdot p(\\theta)}{p(\\text{data})}\n",
    "$$\n",
    "\n",
    "**Posterior ∝ Likelihood × Prior**\n",
    "\n",
    "### Advantages of Bayesian Inference\n",
    "\n",
    "✅ Incorporates prior knowledge\n",
    "✅ Provides full distribution over parameters\n",
    "✅ Natural sequential updating\n",
    "✅ Direct probability statements\n",
    "✅ Automatic regularization (via prior)\n",
    "\n",
    "### Challenges\n",
    "\n",
    "❌ Requires prior specification\n",
    "❌ Computational complexity (often needs MCMC)\n",
    "❌ Sensitive to prior with small data\n",
    "❌ More conceptually complex\n",
    "\n",
    "### When to Use Bayesian vs. MLE\n",
    "\n",
    "**Use Bayesian when**:\n",
    "- You have good prior information\n",
    "- You want uncertainty quantification\n",
    "- Small sample sizes\n",
    "- Sequential learning is important\n",
    "\n",
    "**Use MLE when**:\n",
    "- Large sample sizes\n",
    "- No prior information\n",
    "- Computational simplicity needed\n",
    "- Standard errors are sufficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}