{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/chebil/stat/blob/main/part1/ch02_correlation.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Correlation\n",
    "\n",
    "Correlation quantifies the strength and direction of the **linear relationship** between two variables. It's one of the most important tools for understanding how variables are related.\n",
    "\n",
    "## Why Correlation Matters\n",
    "\n",
    "Scatter plots show us **visual** patterns. Correlation gives us **numbers** to:\n",
    "- Quantify relationship strength\n",
    "- Compare different relationships objectively\n",
    "- Make predictions\n",
    "- Test hypotheses statistically\n",
    "\n",
    "## 2.2.1 The Correlation Coefficient\n",
    "\n",
    "The **Pearson correlation coefficient** (denoted $r$) measures the strength and direction of the linear relationship between two variables.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For standardized data (z-scores) $\\{\\tilde{x}\\}$ and $\\{\\tilde{y}\\}$:\n",
    "\n",
    "$$r = \\text{corr}(\\{x, y\\}) = \\text{mean}(\\{\\tilde{x}\\tilde{y}\\}) = \\frac{1}{N}\\sum_{i=1}^N \\tilde{x}_i \\cdot \\tilde{y}_i$$\n",
    "\n",
    "where:\n",
    "$$\\tilde{x}_i = \\frac{x_i - \\bar{x}}{\\sigma_x}, \\quad \\tilde{y}_i = \\frac{y_i - \\bar{y}}{\\sigma_y}$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "**Range:** $-1 \\leq r \\leq 1$\n",
    "\n",
    "**Sign tells direction:**\n",
    "- **$r > 0$**: Positive correlation â†— (as x increases, y tends to increase)\n",
    "- **$r < 0$**: Negative correlation â†˜ (as x increases, y tends to decrease)  \n",
    "- **$r = 0$**: No linear correlation â—\n",
    "\n",
    "**Magnitude tells strength:**\n",
    "- **$|r| = 1$**: Perfect linear relationship\n",
    "- **$|r| â‰¥ 0.7$**: Strong correlation\n",
    "- **$0.3 â‰¤ |r| < 0.7$**: Moderate correlation\n",
    "- **$|r| < 0.3$**: Weak correlation\n",
    "\n",
    "*Note: These are guidelines - interpretation depends on context!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Correlation: Three Methods\n",
    "\n",
    "### Worked Example: Height and Weight\n",
    "\n",
    "Consider a dataset of 6 people with heights (cm) and weights (kg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "heights = np.array([160, 165, 170, 175, 180, 185])\n",
    "weights = np.array([55, 62, 68, 75, 81, 88])\n",
    "\n",
    "# Display as table\n",
    "df_hw = pd.DataFrame({'Height (cm)': heights, 'Weight (kg)': weights})\n",
    "df_hw.index.name = 'Person'\n",
    "df_hw.index += 1\n",
    "print(df_hw)\n",
    "print(f\"\\nMean height: {np.mean(heights):.1f} cm\")\n",
    "print(f\"Mean weight: {np.mean(weights):.1f} kg\")\n",
    "print(f\"Std height: {np.std(heights):.2f} cm\")\n",
    "print(f\"Std weight: {np.std(weights):.2f} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: Using NumPy\n",
    "r_numpy = np.corrcoef(heights, weights)[0, 1]\n",
    "\n",
    "# METHOD 2: Using SciPy (also gives p-value)\n",
    "r_scipy, p_value = stats.pearsonr(heights, weights)\n",
    "\n",
    "# METHOD 3: Manual calculation (educational!)\n",
    "mean_x = np.mean(heights)\n",
    "mean_y = np.mean(weights)\n",
    "std_x = np.std(heights)\n",
    "std_y = np.std(weights)\n",
    "\n",
    "# Step 1: Standardize\n",
    "z_x = (heights - mean_x) / std_x\n",
    "z_y = (weights - mean_y) / std_y\n",
    "\n",
    "# Step 2: Multiply and average\n",
    "r_manual = np.mean(z_x * z_y)\n",
    "\n",
    "# Display results\n",
    "print(\"=== CORRELATION COEFFICIENT ===")\n",
    "print(f\"\nMethod 1 (NumPy):   r = {r_numpy:.4f}\")\n",
    "print(f\"Method 2 (SciPy):   r = {r_scipy:.4f}\")\n",
    "print(f\"Method 3 (Manual):  r = {r_manual:.4f}\")\n",
    "print(f\"\nP-value: {p_value:.6f}\")\n",
    "print(f\"\nðŸ“Š Interpretation: VERY STRONG POSITIVE correlation\")\n",
    "print(\"\nWhat this means:\")\n",
    "print(\"  â€¢ r â‰ˆ 1 indicates nearly perfect linear relationship\")\n",
    "print(\"  â€¢ Taller people almost always weigh more\")\n",
    "print(\"  â€¢ We can make accurate predictions\")\n",
    "\n",
    "# Visualize the manual calculation\n",
    "print(\"\\n=== MANUAL CALCULATION BREAKDOWN ===")\n",
    "print(\"\nStandardized values:\")\n",
    "calc_df = pd.DataFrame({\n",
    "    'x': heights,\n",
    "    'y': weights,\n",
    "    'z_x': z_x,\n",
    "    'z_y': z_y,\n",
    "    'z_x * z_y': z_x * z_y\n",
    "})\n",
    "print(calc_df.round(3))\n",
    "print(f\"\\nSum of (z_x * z_y): {np.sum(z_x * z_y):.4f}\")\n",
    "print(f\"Divide by N = {len(heights)}: {np.sum(z_x * z_y) / len(heights):.4f}\")\n",
    "print(f\"\nFinal correlation: r = {r_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Correlation\n",
    "\n",
    "Understanding these properties is crucial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate correlation properties\n",
    "x_orig = np.array([1, 2, 3, 4, 5])\n",
    "y_orig = np.array([2, 4, 5, 7, 9])\n",
    "r_orig = np.corrcoef(x_orig, y_orig)[0, 1]\n",
    "\n",
    "print(\"=== CORRELATION PROPERTIES ===")\n",
    "print(f\"\nOriginal data correlation: r = {r_orig:.4f}\")\n",
    "\n",
    "# Property 1: Symmetry\n",
    "r_xy = np.corrcoef(x_orig, y_orig)[0, 1]\n",
    "r_yx = np.corrcoef(y_orig, x_orig)[0, 1]\n",
    "print(f\"\n1. SYMMETRY: corr(x,y) = corr(y,x)\")\n",
    "print(f\"   corr(x,y) = {r_xy:.4f}\")\n",
    "print(f\"   corr(y,x) = {r_yx:.4f}\")\n",
    "print(f\"   âœ“ Same value!\")\n",
    "\n",
    "# Property 2: Translation invariance\n",
    "x_translated = x_orig + 100\n",
    "y_translated = y_orig + 50\n",
    "r_translated = np.corrcoef(x_translated, y_translated)[0, 1]\n",
    "print(f\"\n2. TRANSLATION INVARIANCE: Adding constants doesn't change r\")\n",
    "print(f\"   Original: r = {r_orig:.4f}\")\n",
    "print(f\"   After adding (100, 50): r = {r_translated:.4f}\")\n",
    "print(f\"   âœ“ Same value!\")\n",
    "\n",
    "# Property 3: Scale invariance (magnitude)\n",
    "x_scaled = x_orig * 10\n",
    "y_scaled = y_orig * 5\n",
    "r_scaled = np.corrcoef(x_scaled, y_scaled)[0, 1]\n",
    "print(f\"\n3. SCALE INVARIANCE: Scaling preserves correlation magnitude\")\n",
    "print(f\"   Original: r = {r_orig:.4f}\")\n",
    "print(f\"   After scaling (10x, 5x): r = {r_scaled:.4f}\")\n",
    "print(f\"   âœ“ Same value!\")\n",
    "\n",
    "# Property 4: Negative scaling reverses sign\n",
    "x_neg = x_orig * (-1)\n",
    "r_neg = np.corrcoef(x_neg, y_orig)[0, 1]\n",
    "print(f\"\n4. NEGATIVE SCALING: Reverses sign but not magnitude\")\n",
    "print(f\"   Original: r = {r_orig:.4f}\")\n",
    "print(f\"   After x â†’ -x: r = {r_neg:.4f}\")\n",
    "print(f\"   âœ“ Sign flipped!\")\n",
    "\n",
    "# Property 5: Bounded\n",
    "print(f\"\n5. BOUNDED: -1 â‰¤ r â‰¤ 1 (ALWAYS!)\")\n",
    "print(f\"   This is a mathematical guarantee.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Using Correlation to Predict\n",
    "\n",
    "Correlation enables **linear prediction**. Given a new x value, we can predict y!\n",
    "\n",
    "### Prediction Formula\n",
    "\n",
    "To predict $\\hat{y}$ from a new value $x_0$:\n",
    "\n",
    "**In original coordinates:**\n",
    "$$\\hat{y} = \\bar{y} + r \\cdot \\frac{\\sigma_y}{\\sigma_x} \\cdot (x_0 - \\bar{x})$$\n",
    "\n",
    "**In standard coordinates:**\n",
    "$$\\tilde{y} = r \\cdot \\tilde{x}$$\n",
    "\n",
    "### Rule of Thumb ðŸ’¡\n",
    "\n",
    "**If x is k standard deviations from its mean,**\n",
    "**then y is predicted to be râ€¢k standard deviations from its mean.**\n",
    "\n",
    "Or even simpler:\n",
    "\n",
    "**When x goes up by 1 standard deviation,**\n",
    "**y goes up by r standard deviations.**\n",
    "\n",
    "### Worked Example: Predicting Weight from Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the height-weight data\n",
    "heights = np.array([160, 165, 170, 175, 180, 185])\n",
    "weights = np.array([55, 62, 68, 75, 81, 88])\n",
    "\n",
    "# Statistics\n",
    "h_mean = np.mean(heights)\n",
    "h_std = np.std(heights)\n",
    "w_mean = np.mean(weights)\n",
    "w_std = np.std(weights)\n",
    "r = np.corrcoef(heights, weights)[0, 1]\n",
    "\n",
    "print(\"=== PREDICTION EXAMPLE ===")\n",
    "print(f\"\nDataset statistics:\")\n",
    "print(f\"  Mean height: {h_mean:.1f} cm\")\n",
    "print(f\"  Std height:  {h_std:.2f} cm\")\n",
    "print(f\"  Mean weight: {w_mean:.1f} kg\")\n",
    "print(f\"  Std weight:  {w_std:.2f} kg\")\n",
    "print(f\"  Correlation: r = {r:.4f}\")\n",
    "\n",
    "# Predict weight for height = 172 cm\n",
    "h_new = 172\n",
    "\n",
    "# Method 1: Using formula\n",
    "w_pred = w_mean + r * (w_std / h_std) * (h_new - h_mean)\n",
    "\n",
    "# Method 2: Using rule of thumb\n",
    "# How many std deviations from mean?\n",
    "h_std_dev = (h_new - h_mean) / h_std\n",
    "# Weight prediction in std deviations\n",
    "w_std_dev = r * h_std_dev\n",
    "# Convert back to kg\n",
    "w_pred2 = w_mean + w_std_dev * w_std\n",
    "\n",
    "print(f\"\n=== PREDICTION FOR HEIGHT = {h_new} CM ===")\n",
    "print(f\"\nMethod 1 (Formula):\")\n",
    "print(f\"  Å· = {w_mean:.1f} + {r:.4f} Ã— ({w_std:.2f}/{h_std:.2f}) Ã— ({h_new} - {h_mean:.1f})\")\n",
    "print(f\"  Å· = {w_pred:.2f} kg\")\n",
    "\n",
    "print(f\"\nMethod 2 (Rule of Thumb):\")\n",
    "print(f\"  Height is {h_std_dev:.3f} std devs from mean\")\n",
    "print(f\"  Weight predicted to be {w_std_dev:.3f} std devs from mean\")\n",
    "print(f\"  Å· = {w_mean:.1f} + {w_std_dev:.3f} Ã— {w_std:.2f} = {w_pred2:.2f} kg\")\n",
    "\n",
    "print(f\"\nâœ… Both methods agree: Predicted weight = {w_pred:.2f} kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_correlation(x_new, x_data, y_data, verbose=True):\n",
    "    \"\"\"\n",
    "    Predict y from x using correlation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_new : float or array\n",
    "        New x value(s) to predict y for\n",
    "    x_data : array\n",
    "        Historical x data\n",
    "    y_data : array\n",
    "        Historical y data\n",
    "    verbose : bool\n",
    "        Print detailed steps\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred : float or array\n",
    "        Predicted y value(s)\n",
    "    \"\"\"\n",
    "    # Calculate statistics\n",
    "    r = np.corrcoef(x_data, y_data)[0, 1]\n",
    "    x_mean = np.mean(x_data)\n",
    "    y_mean = np.mean(y_data)\n",
    "    x_std = np.std(x_data)\n",
    "    y_std = np.std(y_data)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = y_mean + r * (y_std / x_std) * (x_new - x_mean)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Correlation: r = {r:.4f}\")\n",
    "        print(f\"Prediction: Å· = {y_pred:.2f}\")\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Test with multiple heights\n",
    "print(\"=== MULTIPLE PREDICTIONS ===")\n",
    "test_heights = [160, 170, 180, 190, 200]\n",
    "\n",
    "print(f\"\nHeight (cm) â†’ Predicted Weight (kg)\")\n",
    "print(\"-\" * 40)\n",
    "for h in test_heights:\n",
    "    w = predict_from_correlation(h, heights, weights, verbose=False)\n",
    "    print(f\"   {h:3d} cm    â†’    {w:5.1f} kg\")\n",
    "\n",
    "print(\"\nðŸ“Š Notice the linear pattern in predictions!\")\n",
    "print(\"Each 10 cm increase in height predicts same weight increase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Error\n",
    "\n",
    "The **root mean square error** (RMSE) of predictions is:\n",
    "\n",
    "$$\\text{RMSE} = \\sigma_y \\sqrt{1 - r^2}$$\n",
    "\n",
    "**Key insights:**\n",
    "- If $|r| = 1$ (perfect correlation), RMSE = 0 (perfect predictions)\n",
    "- If $|r| = 0$ (no correlation), RMSE = $\\sigma_y$ (predictions no better than guessing the mean)\n",
    "- Higher $|r|$ â†’ lower RMSE â†’ better predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction error\n",
    "rmse = w_std * np.sqrt(1 - r**2)\n",
    "\n",
    "print(\"=== PREDICTION ACCURACY ===")\n",
    "print(f\"\nCorrelation: r = {r:.4f}\")\n",
    "print(f\"R-squared: rÂ² = {r**2:.4f}\")\n",
    "print(f\"\nRoot Mean Square Error:\")\n",
    "print(f\"  RMSE = {w_std:.2f} Ã— âˆš(1 - {r**2:.4f})\")\n",
    "print(f\"  RMSE = {rmse:.2f} kg\")\n",
    "\n",
    "print(f\"\nðŸŽ¯ Interpretation:\")\n",
    "print(f\"  â€¢ Typical prediction error: Â±{rmse:.2f} kg\")\n",
    "print(f\"  â€¢ rÂ² = {r**2:.1%} of weight variation explained by height\")\n",
    "print(f\"\nðŸ’¡ Very high correlation = very accurate predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Different Correlation Values\n",
    "\n",
    "Let's see what different correlation values look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with different correlations\n",
    "def generate_correlated_data(r_target, n=100):\n",
    "    \"\"\"Generate data with specified correlation.\"\"\"\n",
    "    x = np.random.randn(n)\n",
    "    y = r_target * x + np.sqrt(1 - r_target**2) * np.random.randn(n)\n",
    "    return x, y\n",
    "\n",
    "# Create grid of plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "correlations = [1.0, 0.8, 0.5, 0.0, -0.5, -0.8]\n",
    "\n",
    "for idx, r_val in enumerate(correlations):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Generate data\n",
    "    x, y = generate_correlated_data(r_val)\n",
    "    \n",
    "    # Verify correlation\n",
    "    r_actual = np.corrcoef(x, y)[0, 1]\n",
    "    \n",
    "    # Color based on correlation\n",
    "    if r_actual > 0:\n",
    "        color = 'darkgreen'\n",
    "    elif r_actual < 0:\n",
    "        color = 'darkred'\n",
    "    else:\n",
    "        color = 'gray'\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(x, y, alpha=0.6, s=30, color=color)\n",
    "    ax.set_title(f'r = {r_actual:.2f}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('X (standardized)')\n",
    "    ax.set_ylabel('Y (standardized)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5, alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Add best-fit line\n",
    "    x_line = np.array([ax.get_xlim()[0], ax.get_xlim()[1]])\n",
    "    y_line = r_actual * x_line\n",
    "    ax.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.5, label='Best fit')\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Different Correlation Values', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“š Pattern Guide:\")\n",
    "print(\"\nr = +1.0: Perfect positive (all points on upward line)\")\n",
    "print(\"r = +0.8: Strong positive (tight upward cloud)\")\n",
    "print(\"r = +0.5: Moderate positive (loose upward cloud)\")\n",
    "print(\"r =  0.0: No correlation (random circular cloud)\")\n",
    "print(\"r = -0.5: Moderate negative (loose downward cloud)\")\n",
    "print(\"r = -0.8: Strong negative (tight downward cloud)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Confusion Caused by Correlation\n",
    "\n",
    "### âš ï¸ Critical Warning: Correlation â‰  Causation!\n",
    "\n",
    "**Just because two variables are correlated does NOT mean one causes the other!**\n",
    "\n",
    "Three possible explanations for correlation:\n",
    "\n",
    "1. **X causes Y**: Pressing accelerator â†’ car speeds up\n",
    "2. **Y causes X**: (Sometimes) Happiness â†’ success\n",
    "3. **Hidden variable Z causes both**: Temperature â†’ ice cream sales AND drownings\n",
    "\n",
    "### Example 1: Spurious Correlation - Ice Cream and Drowning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Famous spurious correlation example\n",
    "months = np.arange(1, 13)\n",
    "# Summer months have high values\n",
    "summer_effect = np.sin((months - 3) * np.pi / 6)\n",
    "\n",
    "ice_cream = 100 + 80 * summer_effect + np.random.normal(0, 5, 12)\n",
    "drownings = 20 + 30 * summer_effect + np.random.normal(0, 2, 12)\n",
    "\n",
    "r_spurious = np.corrcoef(ice_cream, drownings)[0, 1]\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Time series\n",
    "month_names = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "ax1.plot(months, ice_cream, 'o-', linewidth=2, label='Ice Cream Sales', color='coral')\n",
    "ax1.plot(months, drownings, 's-', linewidth=2, label='Drownings', color='steelblue')\n",
    "ax1.set_xlabel('Month', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Ice Cream Sales vs Drownings Over Time', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(months[::2])\n",
    "ax1.set_xticklabels([month_names[i-1] for i in months[::2]])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "ax2.scatter(ice_cream, drownings, s=100, alpha=0.6, color='orange', edgecolors='black')\n",
    "ax2.set_xlabel('Ice Cream Sales', fontsize=12)\n",
    "ax2.set_ylabel('Drownings', fontsize=12)\n",
    "ax2.set_title(f'Scatter Plot (r = {r_spurious:.3f})', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ WARNING: SPURIOUS CORRELATION!\")\n",
    "print(f\"\nCorrelation: r = {r_spurious:.3f} (Strong positive!)\")\n",
    "print(\"\nBUT WAIT! Does ice cream cause drowning? NO!\")\n",
    "\n",
    "print(\"\nðŸ” The real explanation:\")\n",
    "print(\"\n  Hidden variable: SUMMER (warm weather)\")\n",
    "print(\"  â””â”€ Warm weather â†’ More ice cream sales\")\n",
    "print(\"  â””â”€ Warm weather â†’ More swimming â†’ More drownings\")\n",
    "\n",
    "print(\"\nðŸ’¡ Lesson: Always think critically about WHY variables correlate!\")\n",
    "print(\"\nQuestions to ask:\")\n",
    "print(\"  1. Could X directly cause Y?\")\n",
    "print(\"  2. Could Y directly cause X?\")\n",
    "print(\"  3. Could a third variable cause BOTH?\")\n",
    "print(\"  4. Is this just coincidence (random chance)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Confounding Variable - Shoe Size and Reading Ability\n",
    "\n",
    "A classic example from statistics textbooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate children's data\n",
    "n_children = 50\n",
    "ages = np.random.uniform(5, 15, n_children)\n",
    "\n",
    "# Both increase with age!\n",
    "shoe_sizes = 10 + 1.5 * ages + np.random.normal(0, 1, n_children)\n",
    "reading_scores = 20 + 5 * ages + np.random.normal(0, 5, n_children)\n",
    "\n",
    "r_shoes_reading = np.corrcoef(shoe_sizes, reading_scores)[0, 1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(shoe_sizes, reading_scores, c=ages, cmap='viridis', \n",
    "                     s=100, alpha=0.6, edgecolors='black')\n",
    "plt.colorbar(scatter, label='Age (years)')\n",
    "plt.xlabel('Shoe Size', fontsize=12)\n",
    "plt.ylabel('Reading Score', fontsize=12)\n",
    "plt.title(f'Shoe Size vs Reading Ability (r = {r_shoes_reading:.3f})', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ MISLEADING CORRELATION!\")\n",
    "print(f\"\nCorrelation: r = {r_shoes_reading:.3f}\")\n",
    "print(\"\nDoes bigger feet make you read better? NO!\")\n",
    "print(\"\nðŸ” The Confounding Variable: AGE\")\n",
    "print(\"\n  Age â†’ Bigger feet (growth)\")\n",
    "print(\"  Age â†’ Better reading (practice)\")\n",
    "\n",
    "print(\"\nColor of points shows age - notice younger (purple) in\")\n",
    "print(\"bottom-left, older (yellow) in top-right!\")\n",
    "\n",
    "print(\"\nðŸ’¡ Key insight: Control for age, correlation disappears!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Real-World Case Study: Sterile Males in Wild Horse Herds\n",
    "\n",
    "Let's apply everything we've learned to a real conservation problem!\n",
    "\n",
    "### The Problem\n",
    "\n",
    "**Question:** Do sterilized males reduce foal births in wild horse herds?\n",
    "\n",
    "**Hypothesis:** More sterile males â†’ fewer foals\n",
    "\n",
    "**Data:** Observations over 3 years of:\n",
    "- Number of adults\n",
    "- Number of sterile males\n",
    "- Number of foals\n",
    "\n",
    "Let's see what the data shows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate wild horse herd data (based on textbook example)\n",
    "days = np.array([0, 6, 39, 40, 66, 67, 335, 336, 360, 361, 374, 375, 404, 696, 700, 710, 738, 742, 772])\n",
    "\n",
    "# Herd starts large, gradually shrinks\n",
    "# Create realistic declining pattern\n",
    "adults = np.array([62, 60, 58, 59, 56, 58, 47, 46, 44, 45, 42, 43, 40, 35, 34, 33, 32, 31, 30])\n",
    "sterile = np.array([5, 5, 7, 7, 8, 8, 9, 9, 9, 9, 8, 9, 8, 7, 7, 6, 5, 5, 4])\n",
    "foals = np.array([12, 11, 14, 13, 15, 14, 6, 6, 7, 6, 4, 3, 2, 3, 1, 2, 1, 1, 0])\n",
    "\n",
    "# Create DataFrame\n",
    "horse_data = pd.DataFrame({\n",
    "    'Day': days,\n",
    "    'Adults': adults,\n",
    "    'Sterile_Males': sterile,\n",
    "    'Foals': foals\n",
    "})\n",
    "\n",
    "print(\"=== WILD HORSE HERD DATA ===")\n",
    "print(\"\nFirst few observations:\")\n",
    "print(horse_data.head(10))\n",
    "\n",
    "print(\"\nLast few observations:\")\n",
    "print(horse_data.tail(5))\n",
    "\n",
    "print(\"\nðŸŽ Dataset summary:\")\n",
    "print(f\"  â€¢ {len(days)} observations over {days[-1]} days (~2 years)\")\n",
    "print(f\"  â€¢ Adults: {adults.min()} to {adults.max()}\")\n",
    "print(f\"  â€¢ Sterile males: {sterile.min()} to {sterile.max()}\")\n",
    "print(f\"  â€¢ Foals: {foals.min()} to {foals.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Analysis: Time Series Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all variables over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(days, adults, 'o-', linewidth=2, markersize=6, label='Adults', color='steelblue')\n",
    "plt.plot(days, sterile, 's-', linewidth=2, markersize=6, label='Sterile Males', color='orange')\n",
    "plt.plot(days, foals, '^-', linewidth=2, markersize=6, label='Foals', color='green')\n",
    "\n",
    "plt.xlabel('Days Since Start', fontsize=12)\n",
    "plt.ylabel('Number of Horses', fontsize=12)\n",
    "plt.title('Wild Horse Herd Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“‰ Observation from time series:\")\n",
    "print(\"  â€¢ ALL variables are DECREASING over time\")\n",
    "print(\"  â€¢ Herd size shrinking gradually\")\n",
    "print(\"  â€¢ Foals decreasing as herd shrinks\")\n",
    "print(\"\nâ“ Question: Are sterile males reducing foals?\")\n",
    "print(\"Let's check correlations...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Analysis: Naive Correlation (WRONG!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate naive correlations\n",
    "r_sterile_foals = np.corrcoef(sterile, foals)[0, 1]\n",
    "r_sterile_adults = np.corrcoef(sterile, adults)[0, 1]\n",
    "r_adults_foals = np.corrcoef(adults, foals)[0, 1]\n",
    "\n",
    "print(\"=== NAIVE CORRELATION ANALYSIS ===")\n",
    "print(f\"\nSterile Males vs Foals:  r = {r_sterile_foals:+.3f}\")\n",
    "print(f\"Sterile Males vs Adults: r = {r_sterile_adults:+.3f}\")\n",
    "print(f\"Adults vs Foals:         r = {r_adults_foals:+.3f}\")\n",
    "\n",
    "print(\"\nðŸ¤¯ SURPRISING RESULT!\")\n",
    "print(\"\nMore sterile males â†’ MORE foals? That makes no sense!\")\n",
    "print(\"More sterile males â†’ MORE adults? How?\")\n",
    "\n",
    "print(\"\nâ“ What's going on here? Let's investigate...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots (naive analysis)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Sterile vs Foals\n",
    "ax1.scatter(sterile, foals, s=100, alpha=0.6, color='coral', edgecolors='black')\n",
    "ax1.set_xlabel('Number of Sterile Males', fontsize=12)\n",
    "ax1.set_ylabel('Number of Foals', fontsize=12)\n",
    "ax1.set_title(f'Sterile Males vs Foals\n(r = {r_sterile_foals:.3f} - Positive!)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sterile vs Adults\n",
    "ax2.scatter(adults, sterile, s=100, alpha=0.6, color='steelblue', edgecolors='black')\n",
    "ax2.set_xlabel('Number of Adults', fontsize=12)\n",
    "ax2.set_ylabel('Number of Sterile Males', fontsize=12)\n",
    "ax2.set_title(f'Adults vs Sterile Males\n(r = {r_sterile_adults:.3f} - Positive!)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\nâ— MISLEADING ANALYSIS\")\n",
    "print(\"\nThese plots suggest more sterile males = more foals\")\n",
    "print(\"But horses can't detect which males are sterile!\")\n",
    "print(\"\nSomething else must be going on...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Insight: Time as a Lurking Variable\n",
    "\n",
    "Let's plot the scatter again, but **label points with the day number**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize for better visualization\n",
    "sterile_std = (sterile - np.mean(sterile)) / np.std(sterile)\n",
    "foals_std = (foals - np.mean(foals)) / np.std(foals)\n",
    "adults_std = (adults - np.mean(adults)) / np.std(adults)\n",
    "\n",
    "# Create revealing scatter plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot 1: Sterile vs Foals (with day numbers)\n",
    "for i in range(len(days)):\n",
    "    ax1.text(sterile_std[i], foals_std[i], str(days[i]), \n",
    "            ha='center', va='center', fontsize=9,\n",
    "            bbox=dict(boxstyle='circle', facecolor='yellow' if days[i] < 100 else 'lightblue', alpha=0.7))\n",
    "ax1.set_xlabel('Sterile Males (standardized)', fontsize=12)\n",
    "ax1.set_ylabel('Foals (standardized)', fontsize=12)\n",
    "ax1.set_title(f'Sterile Males vs Foals\n(Points labeled with DAY #)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "ax1.axvline(x=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "# Plot 2: Adults vs Sterile (with day numbers)\n",
    "for i in range(len(days)):\n",
    "    ax2.text(adults_std[i], sterile_std[i], str(days[i]),\n",
    "            ha='center', va='center', fontsize=9,\n",
    "            bbox=dict(boxstyle='circle', facecolor='yellow' if days[i] < 100 else 'lightblue', alpha=0.7))\n",
    "ax2.set_xlabel('Adults (standardized)', fontsize=12)\n",
    "ax2.set_ylabel('Sterile Males (standardized)', fontsize=12)\n",
    "ax2.set_title(f'Adults vs Sterile Males\n(Points labeled with DAY #)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "ax2.axvline(x=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ” THE REVELATION!\")\n",
    "print(\"\nLook at the day numbers:\")\n",
    "print(\"  â€¢ Yellow (early days): Upper right (many adults, many foals)\")\n",
    "print(\"  â€¢ Blue (later days): Lower left (few adults, few foals)\")\n",
    "print(\"\nðŸ’¡ The ENTIRE HERD is shrinking over time!\")\n",
    "print(\"\nThe positive correlation is because:\")\n",
    "print(\"  â€¢ When herd is large â†’ more adults AND more sterile males AND more foals\")\n",
    "print(\"  â€¢ When herd is small â†’ fewer adults AND fewer sterile males AND fewer foals\")\n",
    "print(\"\nâš ï¸ The correlation is driven by TIME, not causation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Analysis: Correlation with Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with TIME\n",
    "r_adults_time = np.corrcoef(days, adults)[0, 1]\n",
    "r_sterile_time = np.corrcoef(days, sterile)[0, 1]\n",
    "r_foals_time = np.corrcoef(days, foals)[0, 1]\n",
    "\n",
    "print(\"=== CORRELATION WITH TIME ===")\n",
    "print(f\"\nAdults vs Time:        r = {r_adults_time:.3f}\")\n",
    "print(f\"Sterile Males vs Time: r = {r_sterile_time:.3f}\")\n",
    "print(f\"Foals vs Time:         r = {r_foals_time:.3f}\")\n",
    "\n",
    "print(\"\nðŸ’¡ All negative! The herd is shrinking over time.\")\n",
    "\n",
    "# Using rule of thumb for interpretation\n",
    "print(\"\n=== RULE OF THUMB INTERPRETATION ===")\n",
    "\n",
    "# Calculate rate of decline\n",
    "time_std = np.std(days)\n",
    "adults_std = np.std(adults)\n",
    "foals_std = np.std(foals)\n",
    "\n",
    "print(f\"\nFor every 1 std dev increase in time ({time_std:.0f} days):\")\n",
    "print(f\"  â€¢ Adults decrease by {r_adults_time:.3f} Ã— {adults_std:.1f} = {r_adults_time * adults_std:.1f} horses\")\n",
    "print(f\"  â€¢ Foals decrease by {r_foals_time:.3f} Ã— {foals_std:.1f} = {r_foals_time * foals_std:.1f} foals\")\n",
    "\n",
    "print(\"\nâœ… CONCLUSION: The herd is naturally declining.\")\n",
    "print(\"\nThe original positive correlation between sterile males\")\n",
    "print(\"and foals was SPURIOUS - caused by the time variable!\")\n",
    "\n",
    "print(\"\nðŸŽ¯ KEY LESSON:\")\n",
    "print(\"  To understand a simple dataset, plot it MULTIPLE WAYS!\")\n",
    "print(\"  1. Plot each variable vs time\")\n",
    "print(\"  2. Create scatter plots\")\n",
    "print(\"  3. Calculate correlations\")\n",
    "print(\"  4. Think about confounding variables!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Correlation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: Study Hours vs Test Errors\n",
    "\n",
    "Expect negative correlation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_hours = np.array([8, 7, 6, 5, 4, 3, 2, 1])\n",
    "errors = np.array([2, 3, 4, 5, 7, 9, 12, 15])\n",
    "\n",
    "r_study = np.corrcoef(study_hours, errors)[0, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(study_hours, errors, s=150, alpha=0.7, color='purple', edgecolors='black')\n",
    "\n",
    "# Add labels for each point\n",
    "for i in range(len(study_hours)):\n",
    "    plt.text(study_hours[i], errors[i], f'S{i+1}', ha='center', va='center', \n",
    "            fontweight='bold', color='white')\n",
    "\n",
    "plt.xlabel('Study Hours', fontsize=12)\n",
    "plt.ylabel('Number of Errors on Test', fontsize=12)\n",
    "plt.title(f'Study Hours vs Errors (r = {r_study:.3f})', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\nCorrelation: r = {r_study:.3f}\")\n",
    "print(\"\nâœ“ Strong NEGATIVE correlation\")\n",
    "print(\"\nðŸ“š Interpretation:\")\n",
    "print(\"  More study â†’ fewer errors\")\n",
    "print(\"  Less study â†’ more errors\")\n",
    "print(\"\nThis makes sense intuitively AND statistically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Pitfalls\n",
    "\n",
    "### Pitfall 1: Non-Linear Relationships\n",
    "\n",
    "Correlation only measures **LINEAR** relationships!\n",
    "\n",
    "Non-linear patterns can have low correlation despite strong relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate different relationship types\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "\n",
    "y_linear = x_range + np.random.normal(0, 0.5, 100)\n",
    "y_quadratic = x_range**2 + np.random.normal(0, 2, 100)\n",
    "y_exponential = np.exp(x_range/3) + np.random.normal(0, 1, 100)\n",
    "y_sinusoidal = 3 * np.sin(x_range) + np.random.normal(0, 0.3, 100)\n",
    "\n",
    "# Calculate correlations\n",
    "r_linear = np.corrcoef(x_range, y_linear)[0, 1]\n",
    "r_quadratic = np.corrcoef(x_range, y_quadratic)[0, 1]\n",
    "r_exponential = np.corrcoef(x_range, y_exponential)[0, 1]\n",
    "r_sinusoidal = np.corrcoef(x_range, y_sinusoidal)[0, 1]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Linear\n",
    "axes[0,0].scatter(x_range, y_linear, alpha=0.5, s=20, color='green')\n",
    "axes[0,0].set_title(f'Linear: y = x\nr = {r_linear:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_xlabel('X')\n",
    "axes[0,0].set_ylabel('Y')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].text(0, 4, 'âœ… High r!\nStrong linear\nrelationship', ha='center',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "# Quadratic\n",
    "axes[0,1].scatter(x_range, y_quadratic, alpha=0.5, s=20, color='blue')\n",
    "axes[0,1].set_title(f'Quadratic: y = xÂ²\nr = {r_quadratic:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_xlabel('X')\n",
    "axes[0,1].set_ylabel('Y')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].text(0, 20, 'âš ï¸ Low r!\nBut PERFECT\nrelationship!', ha='center',\n",
    "              bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "\n",
    "# Exponential\n",
    "axes[1,0].scatter(x_range, y_exponential, alpha=0.5, s=20, color='orange')\n",
    "axes[1,0].set_title(f'Exponential: y = e^(x/3)\nr = {r_exponential:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_xlabel('X')\n",
    "axes[1,0].set_ylabel('Y')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].text(0, 4, 'âš ï¸ r misleading!\nNon-linear\nrelationship', ha='center',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "# Sinusoidal\n",
    "axes[1,1].scatter(x_range, y_sinusoidal, alpha=0.5, s=20, color='red')\n",
    "axes[1,1].set_title(f'Sinusoidal: y = 3sin(x)\nr = {r_sinusoidal:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_xlabel('X')\n",
    "axes[1,1].set_ylabel('Y')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].text(0, 0, 'âš ï¸ r â‰ˆ 0!\nBut clear\npattern exists!', ha='center',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Correlation Only Measures LINEAR Relationships!', \n",
    "            fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ CRITICAL LESSON:\")\n",
    "print(\"\n  ALWAYS plot your data first!\n")\n",
    "print(\"  Correlation can miss:\")\n",
    "print(\"    â€¢ Quadratic relationships (U-shaped)\")\n",
    "print(\"    â€¢ Exponential relationships\")\n",
    "print(\"    â€¢ Periodic relationships\")\n",
    "print(\"    â€¢ Other non-linear patterns\")\n",
    "print(\"\n  Don't rely on correlation alone - USE SCATTER PLOTS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfall 2: Outliers\n",
    "\n",
    "A single outlier can drastically change correlation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean data\n",
    "x_clean = np.array([1, 2, 3, 4, 5])\n",
    "y_clean = np.array([2, 4, 6, 8, 10])\n",
    "r_clean = np.corrcoef(x_clean, y_clean)[0, 1]\n",
    "\n",
    "# Add outlier\n",
    "x_outlier = np.append(x_clean, 6)\n",
    "y_outlier = np.append(y_clean, 1)\n",
    "r_outlier = np.corrcoef(x_outlier, y_outlier)[0, 1]\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Without outlier\n",
    "ax1.scatter(x_clean, y_clean, s=150, color='green', alpha=0.7, edgecolors='black', linewidths=2)\n",
    "ax1.set_title(f'Without Outlier\nr = {r_clean:.3f}', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('X', fontsize=12)\n",
    "ax1.set_ylabel('Y', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 7)\n",
    "ax1.set_ylim(0, 11)\n",
    "\n",
    "# With outlier\n",
    "ax2.scatter(x_clean, y_clean, s=150, color='blue', alpha=0.7, edgecolors='black', linewidths=2, label='Clean data')\n",
    "ax2.scatter(x_outlier[-1], y_outlier[-1], s=300, color='red', marker='*', \n",
    "           edgecolors='darkred', linewidths=2, label='OUTLIER', zorder=5)\n",
    "ax2.set_title(f'With Outlier\nr = {r_outlier:.3f}', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('X', fontsize=12)\n",
    "ax2.set_ylabel('Y', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 7)\n",
    "ax2.set_ylim(0, 11)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸš¨ OUTLIER IMPACT!\")\n",
    "print(f\"\nOriginal correlation: r = {r_clean:.3f} (perfect!)\")\n",
    "print(f\"With 1 outlier:       r = {r_outlier:.3f} (weak!)\")\n",
    "print(f\"\nChange: {r_clean - r_outlier:.3f} from just ONE point!\")\n",
    "\n",
    "print(\"\nâœ”ï¸ Always check for outliers:\")\n",
    "print(\"  1. Look at scatter plots\")\n",
    "print(\"  2. Compute correlation with/without outliers\")\n",
    "print(\"  3. Investigate why outliers exist\")\n",
    "print(\"  4. Consider robust methods (Spearman correlation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfall 3: The Scaling Problem\n",
    "\n",
    "Axis scaling can make relationships look stronger or weaker than they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data, different scales\n",
    "x_demo = np.random.normal(50, 10, 50)\n",
    "y_demo = 0.8 * x_demo + np.random.normal(0, 5, 50)\n",
    "r_demo = np.corrcoef(x_demo, y_demo)[0, 1]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Compressed y-axis - looks weak\n",
    "ax1.scatter(x_demo, y_demo, alpha=0.6, s=50, color='steelblue')\n",
    "ax1.set_ylim(0, 200)  # Wide range\n",
    "ax1.set_title(f'Compressed Y-axis\n(looks weaker)', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Proper scale\n",
    "ax2.scatter(x_demo, y_demo, alpha=0.6, s=50, color='green')\n",
    "ax2.set_title(f'Proper Scale\n(r = {r_demo:.3f})', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Standardized - always clear!\n",
    "x_std_demo = (x_demo - np.mean(x_demo)) / np.std(x_demo)\n",
    "y_std_demo = (y_demo - np.mean(y_demo)) / np.std(y_demo)\n",
    "ax3.scatter(x_std_demo, y_std_demo, alpha=0.6, s=50, color='purple')\n",
    "ax3.set_title(f'Standardized\n(always clear)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('X (std)')\n",
    "ax3.set_ylabel('Y (std)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "ax3.axvline(x=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Same Data (r = {r_demo:.3f}), Different Scales!', \n",
    "            fontsize=15, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸš¨ SCALING MATTERS!\")\n",
    "print(\"\nThe SAME data can look different with different axis scales.\")\n",
    "print(\"\nâœ… Solution: Use standardized coordinates!\")\n",
    "print(\"   â€¢ Removes unit effects\")\n",
    "print(\"   â€¢ Centers at (0,0)\")\n",
    "print(\"   â€¢ Makes patterns crystal clear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix\n",
    "\n",
    "For datasets with **multiple variables**, compute all pairwise correlations at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic multi-variable dataset\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "age = np.random.uniform(20, 60, n)\n",
    "height = np.random.normal(170, 10, n)\n",
    "weight = 0.6 * height + 0.1 * age + np.random.normal(0, 5, n)\n",
    "shoe_size = 0.15 * height + np.random.normal(-10, 2, n)\n",
    "income = 1000 * age + 500 * height + np.random.normal(0, 10000, n)\n",
    "\n",
    "# Create DataFrame\n",
    "data_multi = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Height': height,\n",
    "    'Weight': weight,\n",
    "    'Shoe_Size': shoe_size,\n",
    "    'Income': income\n",
    "})\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = data_multi.corr()\n",
    "\n",
    "print(\"=== CORRELATION MATRIX ===")\n",
    "print(\"\n", corr_matrix.round(3))\n",
    "\n",
    "# Find strongest correlations (excluding diagonal)\n",
    "mask = np.triu(np.ones_like(corr_matrix), k=1).astype(bool)\n",
    "corr_values = corr_matrix.where(mask).stack()\n",
    "corr_sorted = corr_values.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n=== STRONGEST CORRELATIONS ===")\n",
    "print(\"\nTop 5 (by magnitude):\")\n",
    "for i, (pair, val) in enumerate(corr_sorted.head(5).items(), 1):\n",
    "    original_val = corr_matrix.loc[pair[0], pair[1]]\n",
    "    print(f\"  {i}. {pair[0]:10s} - {pair[1]:10s}: r = {original_val:+.3f}\")\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=2,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"})\n",
    "plt.title('Correlation Matrix Heat Map', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Heat Map Legend:\")\n",
    "print(\"  ðŸ”´ Red: Strong positive correlation\")\n",
    "print(\"  ðŸ”µ Blue: Strong negative correlation\")\n",
    "print(\"  âšª White: No correlation\")\n",
    "print(\"\nðŸ’¡ Heat maps make it easy to spot patterns in multi-variable data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Correlation Measures\n",
    "\n",
    "### Spearman's Rank Correlation\n",
    "\n",
    "Uses **ranks** instead of values:\n",
    "- Robust to outliers\n",
    "- Works for monotonic (not necessarily linear) relationships\n",
    "- Range still -1 to +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Data with extreme outlier\n",
    "x_extreme = np.array([1, 2, 3, 4, 5, 1000])  # Huge outlier!\n",
    "y_extreme = np.array([2, 4, 6, 8, 10, 12])\n",
    "\n",
    "r_pearson = stats.pearsonr(x_extreme, y_extreme)[0]\n",
    "r_spearman = spearmanr(x_extreme, y_extreme)[0]\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot with outlier marked\n",
    "ax1.scatter(x_extreme[:-1], y_extreme[:-1], s=150, color='green', \n",
    "           alpha=0.7, edgecolors='black', linewidths=2, label='Normal data')\n",
    "ax1.scatter(x_extreme[-1], y_extreme[-1], s=400, color='red', marker='*',\n",
    "           edgecolors='darkred', linewidths=3, label='OUTLIER', zorder=5)\n",
    "ax1.set_xlabel('X', fontsize=12)\n",
    "ax1.set_ylabel('Y', fontsize=12)\n",
    "ax1.set_title('Data with Extreme Outlier', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart comparing correlations\n",
    "methods = ['Pearson\n(sensitive)', 'Spearman\n(robust)']\n",
    "values = [r_pearson, r_spearman]\n",
    "colors = ['coral' if v < 0.9 else 'green' for v in values]\n",
    "\n",
    "bars = ax2.bar(methods, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "ax2.set_title('Pearson vs Spearman', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03, \n",
    "            f'{val:.3f}', ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== ROBUSTNESS COMPARISON ===")\n",
    "print(f\"\nData: x = [1, 2, 3, 4, 5, 1000]\")\n",
    "print(f\"      y = [2, 4, 6, 8, 10, 12]\")\n",
    "print(f\"\nPearson correlation:  r = {r_pearson:.3f} (affected by outlier)\")\n",
    "print(f\"Spearman correlation: Ï = {r_spearman:.3f} (robust!)\")\n",
    "\n",
    "print(\"\nðŸ’¡ Why Spearman is robust:\")\n",
    "print(\"\n  Spearman uses RANKS, not values:\")\n",
    "print(\"    x values:    [1,    2,    3,    4,    5,    1000]\")\n",
    "print(\"    x ranks:     [1,    2,    3,    4,    5,    6   ]\")\n",
    "print(\"                                          â†‘\")\n",
    "print(\"                           Outlier becomes rank 6, not 1000!\")\n",
    "print(\"\n  â†’ Extreme values become just \"highest rank\"\")\n",
    "print(\"  â†’ Outliers have less impact!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary: When to Use What\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "Does your data have outliers?\n",
    "â”‚\n",
    "â”œâ”€â”€ NO â†’ Use Pearson correlation\n",
    "â”‚\n",
    "â””â”€â”€ YES â†’ Is the relationship linear?\n",
    "    â”‚\n",
    "    â”œâ”€â”€ YES, but with outliers â†’ Use Spearman\n",
    "    â”‚\n",
    "    â””â”€â”€ NO, non-linear â†’ Use Spearman or transform data\n",
    "```\n",
    "\n",
    "### Quick Reference Table\n",
    "\n",
    "| Method | Best For | Robust to Outliers? | Measures |\n",
    "|--------|----------|---------------------|----------|\n",
    "| **Pearson** | Linear relationships | âŒ No | Linear correlation |\n",
    "| **Spearman** | Monotonic relationships | âœ”ï¸ Yes | Rank correlation |\n",
    "| **Kendall** | Small samples, many ties | âœ”ï¸ Yes | Concordance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Practice Problems\n",
    "\n",
    "Work through these to master correlation!\n",
    "\n",
    "### Problem 1: Weight and Adiposity\n",
    "\n",
    "Given: $r = 0.9$, mean weight = 150 lb, $\\sigma_{weight} = 30$ lb  \n",
    "Adiposity: mean = 0.8, $\\sigma_{adiposity} = 0.1$\n",
    "\n",
    "**Tasks:**\n",
    "1. Predict adiposity for weight = 170 lb\n",
    "2. Predict weight for adiposity = 0.75\n",
    "3. What is the RMSE of predictions?\n",
    "\n",
    "### Problem 2: Income and IQ\n",
    "\n",
    "Given: $r = 0.30$, mean income = \\$60,000, $\\sigma_{income} = \\$20,000$  \n",
    "IQ: mean = 100, $\\sigma_{IQ} = 15$\n",
    "\n",
    "**Tasks:**\n",
    "1. Predict IQ for income = \\$70,000\n",
    "2. How reliable is this prediction?\n",
    "3. If family income increases, does the child's IQ increase?\n",
    "\n",
    "### Problem 3: Identify the Issue\n",
    "\n",
    "For each scenario, identify why correlation might be misleading:\n",
    "\n",
    "1. Hospital size (beds) vs mortality rate: r = +0.65\n",
    "   - *Hint: Larger hospitals handle sicker patients*\n",
    "\n",
    "2. Chocolate consumption per capita vs Nobel prizes per capita (by country): r = +0.79\n",
    "   - *Hint: Think about wealth*\n",
    "\n",
    "3. Number of firefighters vs fire damage: r = +0.80\n",
    "   - *Hint: What determines both?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions to Practice Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PROBLEM 1 SOLUTIONS ===")\n",
    "\n",
    "# Given\n",
    "r = 0.9\n",
    "weight_mean, weight_std = 150, 30\n",
    "adip_mean, adip_std = 0.8, 0.1\n",
    "\n",
    "# Task 1: Predict adiposity for 170 lb\n",
    "weight_new = 170\n",
    "adip_pred = adip_mean + r * (adip_std / weight_std) * (weight_new - weight_mean)\n",
    "print(f\"\n1. For weight = 170 lb:\")\n",
    "print(f\"   Predicted adiposity = {adip_pred:.3f}\")\n",
    "\n",
    "# Task 2: Predict weight for adiposity 0.75\n",
    "adip_new = 0.75\n",
    "weight_pred = weight_mean + r * (weight_std / adip_std) * (adip_new - adip_mean)\n",
    "print(f\"\n2. For adiposity = 0.75:\")\n",
    "print(f\"   Predicted weight = {weight_pred:.1f} lb\")\n",
    "\n",
    "# Task 3: RMSE\n",
    "rmse = weight_std * np.sqrt(1 - r**2)\n",
    "print(f\"\n3. Prediction accuracy:\")\n",
    "print(f\"   RMSE = {rmse:.1f} lb\")\n",
    "print(f\"   rÂ² = {r**2:.2f} ({r**2*100:.0f}% of variance explained)\")\n",
    "print(f\"   â†’ Pretty accurate predictions!\")\n",
    "\n",
    "print(\"\n=== PROBLEM 2 SOLUTIONS ===")\n",
    "\n",
    "# Given\n",
    "r2 = 0.30\n",
    "income_mean, income_std = 60000, 20000\n",
    "iq_mean, iq_std = 100, 15\n",
    "\n",
    "# Task 1\n",
    "income_new = 70000\n",
    "iq_pred = iq_mean + r2 * (iq_std / income_std) * (income_new - income_mean)\n",
    "print(f\"\n1. For income = $70,000:\")\n",
    "print(f\"   Predicted IQ = {iq_pred:.1f}\")\n",
    "\n",
    "# Task 2\n",
    "rmse2 = iq_std * np.sqrt(1 - r2**2)\n",
    "print(f\"\n2. Prediction reliability:\")\n",
    "print(f\"   RMSE = {rmse2:.1f} IQ points\")\n",
    "print(f\"   rÂ² = {r2**2:.2f} ({r2**2*100:.0f}% of variance explained)\")\n",
    "print(f\"   âš ï¸  Weak correlation = unreliable predictions!\")\n",
    "\n",
    "# Task 3\n",
    "print(f\"\n3. Correlation DOES NOT predict what happens when you change a variable!\")\n",
    "print(f\"   â€¢ Correlation shows association, not causation\")\n",
    "print(f\"   â€¢ Many confounding variables: parents' education, genetics, etc.\")\n",
    "print(f\"   â€¢ Giving someone money won't increase their child's IQ!\")\n",
    "\n",
    "print(\"\n=== PROBLEM 3 SOLUTIONS ===")\n",
    "print(\"\n1. Hospital size vs mortality:\")\n",
    "print(\"   âš ï¸ Confound: Larger hospitals treat sicker patients (selection bias)\")\n",
    "\n",
    "print(\"2. Chocolate vs Nobel prizes:\")\n",
    "print(\"   âš ï¸ Confound: National wealth affects both (richer countries buy more chocolate AND invest in science)\")\n",
    "\n",
    "print(\"3. Firefighters vs damage:\")\n",
    "print(\"   âš ï¸ Confound: Fire SEVERITY determines both (bigger fires need more firefighters AND cause more damage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### The Essential Rules\n",
    "\n",
    "1. **Correlation measures LINEAR relationships**\n",
    "   - Range: -1 to +1 (always!)\n",
    "   - Sign indicates direction\n",
    "   - Magnitude indicates strength\n",
    "\n",
    "2. **Correlation enables prediction**\n",
    "   - Formula: $\\hat{y} = \\bar{y} + r \\cdot \\frac{\\sigma_y}{\\sigma_x}(x - \\bar{x})$\n",
    "   - Rule of thumb: 1 std dev in x â†’ r std devs in y\n",
    "   - Accuracy: RMSE = $\\sigma_y\\sqrt{1-r^2}$\n",
    "\n",
    "3. **CORRELATION â‰  CAUSATION** (never forget!)\n",
    "   - Three possible explanations:\n",
    "     1. X causes Y\n",
    "     2. Y causes X\n",
    "     3. Hidden variable Z causes both\n",
    "\n",
    "4. **Always visualize first!**\n",
    "   - Scatter plots reveal patterns\n",
    "   - Correlation is just a number\n",
    "   - Non-linear patterns need different methods\n",
    "\n",
    "5. **Watch for confounding variables**\n",
    "   - Time is often a lurking variable\n",
    "   - Both variables might be caused by a third\n",
    "   - Control for confounds in analysis\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "âŒ Assuming high correlation means causation\n",
    "âŒ Using correlation for non-linear relationships\n",
    "âŒ Ignoring outliers\n",
    "âŒ Not plotting data first\n",
    "âŒ Forgetting about confounding variables\n",
    "âŒ Using inappropriate axis scales\n",
    "\n",
    "âœ… Always plot first (scatter plots!)\n",
    "âœ… Check for outliers\n",
    "âœ… Standardize when comparing\n",
    "âœ… Consider Spearman for robustness\n",
    "âœ… Think critically about WHY variables correlate\n",
    "âœ… Report both r and p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Further Reading\n",
    "\n",
    "### Recommended Resources\n",
    "\n",
    "1. **Anscombe's Quartet** - Four datasets with identical correlations but very different patterns!\n",
    "2. **Spurious Correlations Website** - tylervigen.com\n",
    "3. **Statistics Done Wrong** by Alex Reinhart - Chapter on correlation\n",
    "\n",
    "### Advanced Topics (for later)\n",
    "\n",
    "- Multiple correlation and regression\n",
    "- Partial correlation (controlling for confounds)\n",
    "- Distance correlation (for non-linear relationships)\n",
    "- Maximal information coefficient\n",
    "\n",
    "## ðŸ”œ What's Next?\n",
    "\n",
    "You now understand:\n",
    "âœ“ How to visualize 2D data\n",
    "âœ“ How to measure relationships\n",
    "âœ“ How to make predictions\n",
    "âœ“ Common pitfalls and how to avoid them\n",
    "\n",
    "**Next chapter:** We'll learn about **probability** - the foundation for making inferences from data!\n",
    "\n",
    "Continue to [Chapter 3: Probability](../part2/chapter03.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’» Programming Exercises\n",
    "\n",
    "### Exercise 1: Real Dataset Analysis\n",
    "\n",
    "Use the height-weight dataset and:\n",
    "1. Load data from a CSV file\n",
    "2. Create scatter plot\n",
    "3. Identify and remove outliers\n",
    "4. Calculate correlation\n",
    "5. Make predictions\n",
    "\n",
    "### Exercise 2: Global Temperature Analysis\n",
    "\n",
    "Download global temperature data (1880-2012) and:\n",
    "1. Plot temperature vs year\n",
    "2. Calculate correlation\n",
    "3. Predict temperatures for 2020, 2030, 2040\n",
    "4. Calculate prediction error\n",
    "\n",
    "### Exercise 3: Correlation Matrix\n",
    "\n",
    "Create a correlation matrix for a multi-variable dataset:\n",
    "1. Use Iris dataset or similar\n",
    "2. Calculate all pairwise correlations\n",
    "3. Create heat map visualization\n",
    "4. Identify strongest/weakest correlations\n",
    "5. Create scatter plots for top 3 correlations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
